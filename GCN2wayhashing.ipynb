{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c20581d-f5c4-47a1-8b6e-9289655b0ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-nas/Gabor_CNN_PyTorch/demo/net_factory.py:866: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is '':\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from utils import accuracy, AverageMeter, save_checkpoint, visualize_graph, get_parameters_size\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from net_factory import get_network_fn\n",
    "\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# del test_loader\n",
    "import tqdm\n",
    "# test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6983cbb4-bc52-4e7d-9ebf-a700dd4ba4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GCN MNIST Training')\n",
    "\n",
    "parser.add_argument('--epochs', default=50, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--pretrained', default='', type=str, metavar='PATH',\n",
    "                    help='path to pretrained checkpoint (default: none)')\n",
    "parser.add_argument('--gpu', default=0, type=int,\n",
    "                    metavar='N', help='GPU device ID (default: -1)')\n",
    "parser.add_argument('--dataset_dir', default='../../MNIST', type=str, metavar='PATH',\n",
    "                    help='path to dataset (default: ../MNIST)')\n",
    "parser.add_argument('--comment', default='', type=str, metavar='INFO',\n",
    "                    help='Extra description for tensorboard')\n",
    "parser.add_argument('--model', default='gcn', type=str, metavar='NETWORK',\n",
    "                    help='Network to train')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "use_cuda = (args.gpu >= 0) and torch.cuda.is_available()\n",
    "best_prec1 = 0\n",
    "writer = SummaryWriter(comment='_'+args.model+'_'+args.comment)\n",
    "iteration = 0\n",
    "\n",
    "# # from loaddataset import load_data\n",
    "# from loaddataset import load_data\n",
    "\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n",
    "# test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cdaa3e-43a8-4a65-b846-dbc1f66065e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-nas/Gabor_CNN_PyTorch/gcn/layers/GConv.py:67: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  for i in range(x.size(1)):\n",
      "/root/autodl-nas/Gabor_CNN_PyTorch/demo/net_factory.py:194: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return x + self.pe[:, :, :x.size(2), :x.size(3)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model\n",
    "model = get_network_fn(name='GCN2wayHashing')#GCNCNN\n",
    "# print(model)\n",
    "\n",
    "# Try to visulize the model\n",
    "try:\n",
    "\tvisualize_graph(model, writer, input_size=(1, 5, 128, 128))\n",
    "except:\n",
    "\tprint('\\nNetwork Visualization Failed! But the training procedure continue.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce89666-4993-4a4a-9625-722d15d45121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.01 million float parameters\n",
      "=> loading checkpoint 'GCN2wayhashing_model_best.pth.tar'\n",
      "0.0023333333333333335\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total parameters of the model\n",
    "print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "args.pretrained = 'GCN2wayhashing_model_best.pth.tar'\n",
    "if os.path.isfile(args.pretrained):\n",
    "    print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "    checkpoint = torch.load(args.pretrained,map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n",
    "print(checkpoint['best_prec1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29785785-5148-4172-abf0-46105050db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSQLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(CSQLoss, self).__init__()\n",
    "        self.is_single_label = config[\"dataset\"] not in {\"nuswide_21\", \"nuswide_21_m\", \"coco\"}\n",
    "        self.hash_targets = self.get_hash_targets(config[\"n_class\"], bit).to(config[\"device\"])\n",
    "        self.multi_label_random_center = torch.randint(2, (bit,)).float().to(config[\"device\"])\n",
    "        self.criterion = torch.nn.BCELoss().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        u = u.tanh()\n",
    "        hash_center = self.label2center(y)\n",
    "        center_loss = self.criterion(0.5 * (u + 1), 0.5 * (hash_center + 1))\n",
    "\n",
    "        Q_loss = (u.abs() - 1).pow(2).mean()\n",
    "        return center_loss + config[\"lambda\"] * Q_loss\n",
    "\n",
    "    def label2center(self, y):\n",
    "        if self.is_single_label:\n",
    "            hash_center = self.hash_targets[y.argmax(axis=1)]\n",
    "        else:\n",
    "            # to get sign no need to use mean, use sum here\n",
    "            center_sum = y @ self.hash_targets\n",
    "            random_center = self.multi_label_random_center.repeat(center_sum.shape[0], 1)\n",
    "            center_sum[center_sum == 0] = random_center[center_sum == 0]\n",
    "            hash_center = 2 * (center_sum > 0).float() - 1\n",
    "        return hash_center\n",
    "\n",
    "    # use algorithm 1 to generate hash centers\n",
    "    def get_hash_targets(self, n_class, bit):\n",
    "        H_K = hadamard(bit)\n",
    "        H_2K = np.concatenate((H_K, -H_K), 0)\n",
    "        hash_targets = torch.from_numpy(H_2K[:n_class]).float()\n",
    "\n",
    "        if H_2K.shape[0] < n_class:\n",
    "            hash_targets.resize_(n_class, bit)\n",
    "            for k in range(20):\n",
    "                for index in range(H_2K.shape[0], n_class):\n",
    "                    ones = torch.ones(bit)\n",
    "                    # Bernouli distribution\n",
    "                    sa = random.sample(list(range(bit)), bit // 2)\n",
    "                    ones[sa] = -1\n",
    "                    hash_targets[index] = ones\n",
    "                # to find average/min  pairwise distance\n",
    "                c = []\n",
    "                for i in range(n_class):\n",
    "                    for j in range(n_class):\n",
    "                        if i < j:\n",
    "                            TF = sum(hash_targets[i] != hash_targets[j])\n",
    "                            c.append(TF)\n",
    "                c = np.array(c)\n",
    "\n",
    "                # choose min(c) in the range of K/4 to K/3\n",
    "                # see in https://github.com/yuanli2333/Hadamard-Matrix-for-hashing/issues/1\n",
    "                # but it is hard when bit is  small\n",
    "                if c.min() > bit / 4 and c.mean() >= bit / 2:\n",
    "                    print(c.min(), c.mean())\n",
    "                    break\n",
    "        return hash_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010b059e-a572-4e84-b2c8-5f5e17d8f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model.fc = nn.Linear(num_ftrs, 450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b1d92b-24b6-4edb-b68e-fba7bc38aef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# read image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "ms_polyu_path = 'dataset/MS_PolyU/'\n",
    "casia_path = 'dataset/CASIA-Multi-Spectral-PalmprintV1/images/'\n",
    "\n",
    "r_img_path = ms_polyu_path + 'Red_ind/'\n",
    "b_img_path =  ms_polyu_path + 'Blue_ind/'\n",
    "n_img_path =  ms_polyu_path + 'NIR_ind/'\n",
    "g_img_path =  ms_polyu_path + 'Green_ind/'\n",
    "\n",
    "################ DATASET CLASS\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] \n",
    "# one_hot_embedding(1, 10)\n",
    "def part_init(istrain=True):\n",
    "    r_list = []\n",
    "    b_list = []\n",
    "    vein_list = []\n",
    "    prints_list = []\n",
    "    labels = []\n",
    "    \n",
    "        # split all data into train, test data\n",
    "    train_ratio = 1\n",
    "    train_num = int(500 * train_ratio)\n",
    "    print(\"split train users:\",train_num)\n",
    "    if istrain:\n",
    "        for i in tqdm.tqdm(range(train_num)):\n",
    "            for j in range(8):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "                g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "                b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "                n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "                rb = r_normed - b_normed * 0.5\n",
    "                rb =  (rb * 128+128).astype(np.uint8)\n",
    "\n",
    "                imgprint = np.dstack((r_img,g_img,b_img))\n",
    "                imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "                vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                labels.append(one_hot_embedding(i, train_num))\n",
    "#                 labels.append(i)\n",
    "    else:\n",
    "        for i in tqdm.tqdm(range(train_num)):\n",
    "            for j in range(8,12):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "                g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "                b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "                n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "                rb = r_normed - b_normed * 0.5\n",
    "                rb =  (rb * 128+128).astype(np.uint8)\n",
    "                imgprint = np.dstack((r_img,g_img,b_img))\n",
    "                imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "                vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                labels.append(one_hot_embedding(i, train_num))\n",
    "#                 labels.append(i)\n",
    "\n",
    "\n",
    "\n",
    "    # return np.array(r_list), np.array(b_list), np.array(n_list), np.array(labels),np.array(r_list_test), np.array(b_list_test), np.array(n_list_test), np.array(labels_test)\n",
    "    return  vein_list,prints_list, labels\n",
    "\n",
    "# r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "class load_data(Dataset):\n",
    "    \"\"\"Loads the Data.\"\"\"\n",
    "    def __init__(self, training=True):\n",
    "\n",
    "        self.training = training\n",
    "#         r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomPerspective(),\n",
    "        transforms.RandomAffine(30),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        self.transform_test = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        if self.training:\n",
    "            print('\\n...... Train files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels= part_init(istrain=True)\n",
    "            print('\\nTrain files loaded ......\\n')\n",
    "        else:\n",
    "            print('\\n...... Test files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels = part_init(istrain=False)\n",
    "            print('\\nTest files loaded ......\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vein_list)\n",
    "\n",
    "         \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.training:\n",
    "            prints_img = self.transform(self.prints_list[idx])\n",
    "            vein_img = self.transform(self.vein_list[idx])\n",
    "        else:\n",
    "            prints_img = self.transform_test(self.prints_list[idx])\n",
    "            vein_img = self.transform_test(self.vein_list[idx])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        n_img = np.dstack((prints_img[0,:,:],prints_img[1,:,:],prints_img[2,:,:],vein_img[0,:,:],vein_img[1,:,:]))\n",
    "        \n",
    "        return n_img,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33159f71-22b4-4950-9972-b2cfa3d3277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda': 0.1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[CSQ]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 64, 'net': 'w', 'dataset': 'palmprint', 'n_class': 500, 'epoch': 500, 'test_map': 10, 'device': device(type='cuda', index=0), 'bit_list': [1024]}\n"
     ]
    }
   ],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"lambda\": 0.1,\n",
    "        \"optimizer\": {\"type\": optim.RMSprop, \"optim_params\": {\"lr\": 1e-5, \"weight_decay\": 10 ** -5}},\n",
    "        \"info\": \"[CSQ]\",\n",
    "        \"resize_size\": 256,\n",
    "        \"crop_size\": 224,\n",
    "        \"batch_size\": 64,\n",
    "        # \"net\": AlexNet,\n",
    "        \"net\": \"w\",\n",
    "        \"dataset\": \"palmprint\",\n",
    "        \"n_class\":500,\n",
    "        # \"dataset\": \"imagenet\",\n",
    "        # \"dataset\": \"coco\",\n",
    "        # \"dataset\": \"nuswide_21\",\n",
    "        # \"dataset\": \"nuswide_21_m\",\n",
    "        \"epoch\": 500,\n",
    "        \"test_map\": 10,\n",
    "        # \"device\":torch.device(\"cpu\"),\n",
    "        \"device\": torch.device(\"cuda:0\"),\n",
    "        \"bit_list\": [1024],\n",
    "    }\n",
    "#     config = config_dataset(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "print(config)\n",
    "bit = 1024\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d7fd0b-2f15-4bd8-91ab-f1e224e7e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...... Train files loading\n",
      "\n",
      "split train users: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:14<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train files loaded ......\n",
      "\n",
      "\n",
      "...... Test files loading\n",
      "\n",
      "split train users: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:27<00:00, 18.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test files loaded ......\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset = get_data(config)\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,prefetch_factor=8)  # ,prefetch_factor=2\n",
    "test_loader = DataLoader(load_data(training=False), batch_size=64, shuffle=False)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6306800-6cbc-456e-bea1-66480629db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 32\n",
    "dataset_loader = train_loader\n",
    "num_train = 4000\n",
    "num_test = 2000\n",
    "num_dataset = 6000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62da0f6d-5f5e-418d-9c04-28ed7f96ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ae9993-d8d2-44b0-bae1-4458ba730ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "# from network import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.linalg import hadamard  # direct import  hadamrd matrix from scipy\n",
    "import random\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "\n",
    "class CSQLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(CSQLoss, self).__init__()\n",
    "        self.is_single_label = config[\"dataset\"] not in {\"nuswide_21\", \"nuswide_21_m\", \"coco\"}\n",
    "        self.hash_targets = self.get_hash_targets(config[\"n_class\"], bit).to(config[\"device\"])\n",
    "        self.multi_label_random_center = torch.randint(2, (bit,)).float().to(config[\"device\"])\n",
    "        self.criterion = torch.nn.BCELoss().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        u = u.tanh()\n",
    "        hash_center = self.label2center(y)\n",
    "        center_loss = self.criterion(0.5 * (u + 1), 0.5 * (hash_center + 1))\n",
    "\n",
    "        Q_loss = (u.abs() - 1).pow(2).mean()\n",
    "        return center_loss + config[\"lambda\"] * Q_loss\n",
    "\n",
    "    def label2center(self, y):\n",
    "        if self.is_single_label:\n",
    "            hash_center = self.hash_targets[y.argmax(axis=1)]\n",
    "        else:\n",
    "            # to get sign no need to use mean, use sum here\n",
    "            center_sum = y @ self.hash_targets\n",
    "            random_center = self.multi_label_random_center.repeat(center_sum.shape[0], 1)\n",
    "            center_sum[center_sum == 0] = random_center[center_sum == 0]\n",
    "            hash_center = 2 * (center_sum > 0).float() - 1\n",
    "        return hash_center\n",
    "\n",
    "    # use algorithm 1 to generate hash centers\n",
    "    def get_hash_targets(self, n_class, bit):\n",
    "        H_K = hadamard(bit)\n",
    "        H_2K = np.concatenate((H_K, -H_K), 0)\n",
    "        hash_targets = torch.from_numpy(H_2K[:n_class]).float()\n",
    "\n",
    "        if H_2K.shape[0] < n_class:\n",
    "            hash_targets.resize_(n_class, bit)\n",
    "            for k in range(20):\n",
    "                for index in range(H_2K.shape[0], n_class):\n",
    "                    ones = torch.ones(bit)\n",
    "                    # Bernouli distribution\n",
    "                    sa = random.sample(list(range(bit)), bit // 2)\n",
    "                    ones[sa] = -1\n",
    "                    hash_targets[index] = ones\n",
    "                # to find average/min  pairwise distance\n",
    "                c = []\n",
    "                for i in range(n_class):\n",
    "                    for j in range(n_class):\n",
    "                        if i < j:\n",
    "                            TF = sum(hash_targets[i] != hash_targets[j])\n",
    "                            c.append(TF)\n",
    "                c = np.array(c)\n",
    "\n",
    "                # choose min(c) in the range of K/4 to K/3\n",
    "                # see in https://github.com/yuanli2333/Hadamard-Matrix-for-hashing/issues/1\n",
    "                # but it is hard when bit is  small\n",
    "                if c.min() > bit / 4 and c.mean() >= bit / 2:\n",
    "                    print(c.min(), c.mean())\n",
    "                    break\n",
    "        return hash_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fbdca84-303c-405a-b2b7-31d138e9298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4d94-f3ec-4d50-915e-3e60310f83f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92f556f-0fd8-4248-894e-db92eb2062cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_train\"] = num_train\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = config[\"optimizer\"][\"type\"](model.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
    "\n",
    "criterion = CSQLoss(config, bit)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=args.momentum, weight_decay=3e-05)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0136acba-d04b-4a4e-b76a-fa22ea3bf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37e5dc6d-0958-4215-ba97-1a473024789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9950956  0.09891789]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "A = np.random.randn(1,2)*10\n",
    "# print(A)\n",
    "# print(normalized(A,0))\n",
    "print(normalized(A,1))# ok verified\n",
    "\n",
    "\n",
    "def test(net,test_loader):\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    FEATS = []\n",
    "    GT = []\n",
    "    net.eval()\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            output = net(rbn)\n",
    "            FEATS.append(output.cpu().numpy())\n",
    "#             FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "    GCNFEATS = np.concatenate(FEATS)\n",
    "    GT = np.concatenate(GT)\n",
    "    GCNFEATS = normalized(GCNFEATS,1)\n",
    "    print('- feats shape:', GCNFEATS.shape)\n",
    "    print('- GT shape:', GT.shape)\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    def cossim(a,b):\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    pred_scores = []\n",
    "    gt_label = []\n",
    "\n",
    "    for i in range(2000):\n",
    "        for j in range(i+1,2000):\n",
    "            # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "            a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "            pred_scores.append(a)\n",
    "            gt_label.append(i//4 == j//4)\n",
    "\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    gt_label = np.array(gt_label)\n",
    "\n",
    "    Gen = pred_scores[gt_label]\n",
    "    Imp = pred_scores[gt_label==False]\n",
    "    Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "#     import seaborn as sns\n",
    "#     sns.distplot(Gen,  kde=False, label='Gen')\n",
    "#     # df =gapminder[gapminder.continent == 'Americas']\n",
    "#     sns.distplot(Imp,  kde=False,label='Imp')\n",
    "#     # Plot formatting\n",
    "#     plt.legend(prop={'size': 12})\n",
    "#     plt.title('Life Expectancy of Two Continents')\n",
    "#     plt.xlabel('Life Exp (years)')\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "    from pyeer.eer_info import get_eer_stats\n",
    "    from pyeer.report import generate_eer_report, export_error_rates\n",
    "    from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "    # Calculating stats for classifier A\n",
    "    stats_a = get_eer_stats(Gen, Imp)\n",
    "    print(stats_a.eer)\n",
    "\n",
    "    return stats_a.eer\n",
    "\n",
    "##### INSPECT FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53117228-f6d8-4b16-b4b1-5cc939b42d58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSQ][501/2000][11:57:02] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.01\n",
      "[CSQ] epoch:501, bit:1024, dataset:palmprint,eer:0.01000, Best eer: 0.01000\n",
      "\b loss:0.72358\n",
      "[CSQ][502/2000][11:58:11] bit:1024, dataset:palmprint, training... loss:0.72343\n",
      "[CSQ][503/2000][11:58:20] bit:1024, dataset:palmprint, training... loss:0.72347\n",
      "[CSQ][504/2000][11:58:28] bit:1024, dataset:palmprint, training... loss:0.72345\n",
      "[CSQ][505/2000][11:58:38] bit:1024, dataset:palmprint, training... loss:0.72347\n",
      "[CSQ][506/2000][11:58:47] bit:1024, dataset:palmprint, training... loss:0.72358\n",
      "[CSQ][507/2000][11:58:56] bit:1024, dataset:palmprint, training... loss:0.72302\n",
      "[CSQ][508/2000][11:59:05] bit:1024, dataset:palmprint, training... loss:0.72333\n",
      "[CSQ][509/2000][11:59:15] bit:1024, dataset:palmprint, training... loss:0.72300\n",
      "[CSQ][510/2000][11:59:24] bit:1024, dataset:palmprint, training... loss:0.72314\n",
      "[CSQ][511/2000][11:59:33] bit:1024, dataset:palmprint, training... loss:0.72292\n",
      "[CSQ][512/2000][11:59:44] bit:1024, dataset:palmprint, training... loss:0.72327\n",
      "[CSQ][513/2000][11:59:54] bit:1024, dataset:palmprint, training... loss:0.72294\n",
      "[CSQ][514/2000][12:00:05] bit:1024, dataset:palmprint, training... loss:0.72297\n",
      "[CSQ][515/2000][12:00:13] bit:1024, dataset:palmprint, training... loss:0.72268\n",
      "[CSQ][516/2000][12:00:22] bit:1024, dataset:palmprint, training... loss:0.72282\n",
      "[CSQ][517/2000][12:00:32] bit:1024, dataset:palmprint, training... loss:0.72290\n",
      "[CSQ][518/2000][12:00:41] bit:1024, dataset:palmprint, training... loss:0.72264\n",
      "[CSQ][519/2000][12:00:50] bit:1024, dataset:palmprint, training... loss:0.72245\n",
      "[CSQ][520/2000][12:01:00] bit:1024, dataset:palmprint, training... loss:0.72285\n",
      "[CSQ][521/2000][12:01:09] bit:1024, dataset:palmprint, training... loss:0.72272\n",
      "[CSQ][522/2000][12:01:19] bit:1024, dataset:palmprint, training... loss:0.72217\n",
      "[CSQ][523/2000][12:01:29] bit:1024, dataset:palmprint, training... loss:0.72269\n",
      "[CSQ][524/2000][12:01:39] bit:1024, dataset:palmprint, training... loss:0.72255\n",
      "[CSQ][525/2000][12:01:48] bit:1024, dataset:palmprint, training... loss:0.72213\n",
      "[CSQ][526/2000][12:01:58] bit:1024, dataset:palmprint, training... loss:0.72198\n",
      "[CSQ][527/2000][12:02:09] bit:1024, dataset:palmprint, training... loss:0.72204\n",
      "[CSQ][528/2000][12:02:21] bit:1024, dataset:palmprint, training... loss:0.72241\n",
      "[CSQ][529/2000][12:02:31] bit:1024, dataset:palmprint, training... loss:0.72183\n",
      "[CSQ][530/2000][12:02:40] bit:1024, dataset:palmprint, training... loss:0.72189\n",
      "[CSQ][531/2000][12:02:49] bit:1024, dataset:palmprint, training... loss:0.72225\n",
      "[CSQ][532/2000][12:02:58] bit:1024, dataset:palmprint, training... loss:0.72178\n",
      "[CSQ][533/2000][12:03:07] bit:1024, dataset:palmprint, training... loss:0.72190\n",
      "[CSQ][534/2000][12:03:17] bit:1024, dataset:palmprint, training... loss:0.72169\n",
      "[CSQ][535/2000][12:03:26] bit:1024, dataset:palmprint, training... loss:0.72146\n",
      "[CSQ][536/2000][12:03:36] bit:1024, dataset:palmprint, training... loss:0.72159\n",
      "[CSQ][537/2000][12:03:46] bit:1024, dataset:palmprint, training... loss:0.72141\n",
      "[CSQ][538/2000][12:03:56] bit:1024, dataset:palmprint, training... loss:0.72126\n",
      "[CSQ][539/2000][12:04:07] bit:1024, dataset:palmprint, training... loss:0.72142\n",
      "[CSQ][540/2000][12:04:17] bit:1024, dataset:palmprint, training... loss:0.72151\n",
      "[CSQ][541/2000][12:04:28] bit:1024, dataset:palmprint, training... loss:0.72150\n",
      "[CSQ][542/2000][12:04:38] bit:1024, dataset:palmprint, training... loss:0.72132\n",
      "[CSQ][543/2000][12:04:48] bit:1024, dataset:palmprint, training... loss:0.72107\n",
      "[CSQ][544/2000][12:04:59] bit:1024, dataset:palmprint, training... loss:0.72114\n",
      "[CSQ][545/2000][12:05:09] bit:1024, dataset:palmprint, training... loss:0.72132\n",
      "[CSQ][546/2000][12:05:18] bit:1024, dataset:palmprint, training... loss:0.72110\n",
      "[CSQ][547/2000][12:05:27] bit:1024, dataset:palmprint, training... loss:0.72081\n",
      "[CSQ][548/2000][12:05:37] bit:1024, dataset:palmprint, training... loss:0.72087\n",
      "[CSQ][549/2000][12:05:46] bit:1024, dataset:palmprint, training... loss:0.72065\n",
      "[CSQ][550/2000][12:05:55] bit:1024, dataset:palmprint, training... loss:0.72109\n",
      "[CSQ][551/2000][12:06:04] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.009\n",
      "[CSQ] epoch:551, bit:1024, dataset:palmprint,eer:0.00900, Best eer: 0.00900\n",
      "\b loss:0.72096\n",
      "[CSQ][552/2000][12:07:23] bit:1024, dataset:palmprint, training... loss:0.72092\n",
      "[CSQ][553/2000][12:07:31] bit:1024, dataset:palmprint, training... loss:0.72042\n",
      "[CSQ][554/2000][12:07:40] bit:1024, dataset:palmprint, training... loss:0.72069\n",
      "[CSQ][555/2000][12:07:48] bit:1024, dataset:palmprint, training... loss:0.72078\n",
      "[CSQ][556/2000][12:07:57] bit:1024, dataset:palmprint, training... loss:0.72061\n",
      "[CSQ][557/2000][12:08:06] bit:1024, dataset:palmprint, training... loss:0.72050\n",
      "[CSQ][558/2000][12:08:14] bit:1024, dataset:palmprint, training... loss:0.72050\n",
      "[CSQ][559/2000][12:08:24] bit:1024, dataset:palmprint, training... loss:0.72043\n",
      "[CSQ][560/2000][12:08:33] bit:1024, dataset:palmprint, training... loss:0.72050\n",
      "[CSQ][561/2000][12:08:43] bit:1024, dataset:palmprint, training... loss:0.72017\n",
      "[CSQ][562/2000][12:08:52] bit:1024, dataset:palmprint, training... loss:0.72056\n",
      "[CSQ][563/2000][12:09:01] bit:1024, dataset:palmprint, training... loss:0.72016\n",
      "[CSQ][564/2000][12:09:10] bit:1024, dataset:palmprint, training... loss:0.72007\n",
      "[CSQ][565/2000][12:09:20] bit:1024, dataset:palmprint, training... loss:0.71967\n",
      "[CSQ][566/2000][12:09:30] bit:1024, dataset:palmprint, training... loss:0.71985\n",
      "[CSQ][567/2000][12:09:40] bit:1024, dataset:palmprint, training... loss:0.72019\n",
      "[CSQ][568/2000][12:09:50] bit:1024, dataset:palmprint, training... loss:0.71985\n",
      "[CSQ][569/2000][12:10:00] bit:1024, dataset:palmprint, training... loss:0.71968\n",
      "[CSQ][570/2000][12:10:10] bit:1024, dataset:palmprint, training... loss:0.71986\n",
      "[CSQ][571/2000][12:10:20] bit:1024, dataset:palmprint, training... loss:0.71949\n",
      "[CSQ][572/2000][12:10:31] bit:1024, dataset:palmprint, training... loss:0.71978\n",
      "[CSQ][573/2000][12:10:42] bit:1024, dataset:palmprint, training... loss:0.71997\n",
      "[CSQ][574/2000][12:10:53] bit:1024, dataset:palmprint, training... loss:0.71973\n",
      "[CSQ][575/2000][12:11:02] bit:1024, dataset:palmprint, training... loss:0.71952\n",
      "[CSQ][576/2000][12:11:11] bit:1024, dataset:palmprint, training... loss:0.71946\n",
      "[CSQ][577/2000][12:11:20] bit:1024, dataset:palmprint, training... loss:0.71920\n",
      "[CSQ][578/2000][12:11:29] bit:1024, dataset:palmprint, training... loss:0.71877\n",
      "[CSQ][579/2000][12:11:38] bit:1024, dataset:palmprint, training... loss:0.71933\n",
      "[CSQ][580/2000][12:11:48] bit:1024, dataset:palmprint, training... loss:0.71939\n",
      "[CSQ][581/2000][12:11:58] bit:1024, dataset:palmprint, training... loss:0.71937\n",
      "[CSQ][582/2000][12:12:08] bit:1024, dataset:palmprint, training... loss:0.71926\n",
      "[CSQ][583/2000][12:12:17] bit:1024, dataset:palmprint, training... loss:0.71926\n",
      "[CSQ][584/2000][12:12:27] bit:1024, dataset:palmprint, training... loss:0.71917\n",
      "[CSQ][585/2000][12:12:38] bit:1024, dataset:palmprint, training... loss:0.71911\n",
      "[CSQ][586/2000][12:12:48] bit:1024, dataset:palmprint, training... loss:0.71902\n",
      "[CSQ][587/2000][12:12:59] bit:1024, dataset:palmprint, training... loss:0.71907\n",
      "[CSQ][588/2000][12:13:09] bit:1024, dataset:palmprint, training... loss:0.71863\n",
      "[CSQ][589/2000][12:13:18] bit:1024, dataset:palmprint, training... loss:0.71884\n",
      "[CSQ][590/2000][12:13:27] bit:1024, dataset:palmprint, training... loss:0.71872\n",
      "[CSQ][591/2000][12:13:36] bit:1024, dataset:palmprint, training... loss:0.71875\n",
      "[CSQ][592/2000][12:13:45] bit:1024, dataset:palmprint, training... loss:0.71859\n",
      "[CSQ][593/2000][12:13:55] bit:1024, dataset:palmprint, training... loss:0.71844\n",
      "[CSQ][594/2000][12:14:05] bit:1024, dataset:palmprint, training... loss:0.71844\n",
      "[CSQ][595/2000][12:14:14] bit:1024, dataset:palmprint, training... loss:0.71847\n",
      "[CSQ][596/2000][12:14:25] bit:1024, dataset:palmprint, training... loss:0.71846\n",
      "[CSQ][597/2000][12:14:34] bit:1024, dataset:palmprint, training... loss:0.71839\n",
      "[CSQ][598/2000][12:14:45] bit:1024, dataset:palmprint, training... loss:0.71825\n",
      "[CSQ][599/2000][12:14:55] bit:1024, dataset:palmprint, training... loss:0.71843\n",
      "[CSQ][600/2000][12:15:05] bit:1024, dataset:palmprint, training... loss:0.71840\n",
      "[CSQ][601/2000][12:15:16] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.009\n",
      "[CSQ] epoch:601, bit:1024, dataset:palmprint,eer:0.00900, Best eer: 0.00900\n",
      "\b loss:0.71809\n",
      "[CSQ][602/2000][12:16:36] bit:1024, dataset:palmprint, training... loss:0.71812\n",
      "[CSQ][603/2000][12:16:46] bit:1024, dataset:palmprint, training... loss:0.71808\n",
      "[CSQ][604/2000][12:16:55] bit:1024, dataset:palmprint, training... loss:0.71782\n",
      "[CSQ][605/2000][12:17:05] bit:1024, dataset:palmprint, training... loss:0.71786\n",
      "[CSQ][606/2000][12:17:16] bit:1024, dataset:palmprint, training... loss:0.71776\n",
      "[CSQ][607/2000][12:17:25] bit:1024, dataset:palmprint, training... loss:0.71740\n",
      "[CSQ][608/2000][12:17:33] bit:1024, dataset:palmprint, training... loss:0.71805\n",
      "[CSQ][609/2000][12:17:42] bit:1024, dataset:palmprint, training... loss:0.71798\n",
      "[CSQ][610/2000][12:17:50] bit:1024, dataset:palmprint, training... loss:0.71755\n",
      "[CSQ][611/2000][12:17:59] bit:1024, dataset:palmprint, training... loss:0.71750\n",
      "[CSQ][612/2000][12:18:07] bit:1024, dataset:palmprint, training... loss:0.71762\n",
      "[CSQ][613/2000][12:18:17] bit:1024, dataset:palmprint, training... loss:0.71771\n",
      "[CSQ][614/2000][12:18:26] bit:1024, dataset:palmprint, training... loss:0.71736\n",
      "[CSQ][615/2000][12:18:35] bit:1024, dataset:palmprint, training... loss:0.71748\n",
      "[CSQ][616/2000][12:18:45] bit:1024, dataset:palmprint, training... loss:0.71731\n",
      "[CSQ][617/2000][12:18:54] bit:1024, dataset:palmprint, training... loss:0.71752\n",
      "[CSQ][618/2000][12:19:05] bit:1024, dataset:palmprint, training... loss:0.71735\n",
      "[CSQ][619/2000][12:19:16] bit:1024, dataset:palmprint, training... loss:0.71714\n",
      "[CSQ][620/2000][12:19:26] bit:1024, dataset:palmprint, training... loss:0.71714\n",
      "[CSQ][621/2000][12:19:37] bit:1024, dataset:palmprint, training... loss:0.71738\n",
      "[CSQ][622/2000][12:19:47] bit:1024, dataset:palmprint, training... loss:0.71700\n",
      "[CSQ][623/2000][12:19:58] bit:1024, dataset:palmprint, training... loss:0.71710\n",
      "[CSQ][624/2000][12:20:10] bit:1024, dataset:palmprint, training... loss:0.71681\n",
      "[CSQ][625/2000][12:20:20] bit:1024, dataset:palmprint, training... loss:0.71689\n",
      "[CSQ][626/2000][12:20:30] bit:1024, dataset:palmprint, training... loss:0.71707\n",
      "[CSQ][627/2000][12:20:40] bit:1024, dataset:palmprint, training... loss:0.71704\n",
      "[CSQ][628/2000][12:20:51] bit:1024, dataset:palmprint, training... loss:0.71690\n",
      "[CSQ][629/2000][12:21:01] bit:1024, dataset:palmprint, training... loss:0.71689\n",
      "[CSQ][630/2000][12:21:10] bit:1024, dataset:palmprint, training... loss:0.71617\n",
      "[CSQ][631/2000][12:21:20] bit:1024, dataset:palmprint, training... loss:0.71697\n",
      "[CSQ][632/2000][12:21:30] bit:1024, dataset:palmprint, training... loss:0.71657\n",
      "[CSQ][633/2000][12:21:40] bit:1024, dataset:palmprint, training... loss:0.71654\n",
      "[CSQ][634/2000][12:21:49] bit:1024, dataset:palmprint, training... loss:0.71627\n",
      "[CSQ][635/2000][12:21:59] bit:1024, dataset:palmprint, training... loss:0.71629\n",
      "[CSQ][636/2000][12:22:10] bit:1024, dataset:palmprint, training... loss:0.71655\n",
      "[CSQ][637/2000][12:22:20] bit:1024, dataset:palmprint, training... loss:0.71614\n",
      "[CSQ][638/2000][12:22:31] bit:1024, dataset:palmprint, training... loss:0.71679\n",
      "[CSQ][639/2000][12:22:42] bit:1024, dataset:palmprint, training... loss:0.71627\n",
      "[CSQ][640/2000][12:22:51] bit:1024, dataset:palmprint, training... loss:0.71638\n",
      "[CSQ][641/2000][12:23:00] bit:1024, dataset:palmprint, training... loss:0.71628\n",
      "[CSQ][642/2000][12:23:10] bit:1024, dataset:palmprint, training... loss:0.71591\n",
      "[CSQ][643/2000][12:23:19] bit:1024, dataset:palmprint, training... loss:0.71607\n",
      "[CSQ][644/2000][12:23:28] bit:1024, dataset:palmprint, training... loss:0.71632\n",
      "[CSQ][645/2000][12:23:39] bit:1024, dataset:palmprint, training... loss:0.71607\n",
      "[CSQ][646/2000][12:23:49] bit:1024, dataset:palmprint, training... loss:0.71644\n",
      "[CSQ][647/2000][12:24:00] bit:1024, dataset:palmprint, training... loss:0.71583\n",
      "[CSQ][648/2000][12:24:10] bit:1024, dataset:palmprint, training... loss:0.71614\n",
      "[CSQ][649/2000][12:24:20] bit:1024, dataset:palmprint, training... loss:0.71559\n",
      "[CSQ][650/2000][12:24:31] bit:1024, dataset:palmprint, training... loss:0.71590\n",
      "[CSQ][651/2000][12:24:42] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.008\n",
      "[CSQ] epoch:651, bit:1024, dataset:palmprint,eer:0.00800, Best eer: 0.00800\n",
      "\b loss:0.71564\n",
      "[CSQ][652/2000][12:26:04] bit:1024, dataset:palmprint, training... loss:0.71599\n",
      "[CSQ][653/2000][12:26:13] bit:1024, dataset:palmprint, training... loss:0.71596\n",
      "[CSQ][654/2000][12:26:22] bit:1024, dataset:palmprint, training... loss:0.71551\n",
      "[CSQ][655/2000][12:26:31] bit:1024, dataset:palmprint, training... loss:0.71526\n",
      "[CSQ][656/2000][12:26:41] bit:1024, dataset:palmprint, training... loss:0.71542\n",
      "[CSQ][657/2000][12:26:51] bit:1024, dataset:palmprint, training... loss:0.71539\n",
      "[CSQ][658/2000][12:27:02] bit:1024, dataset:palmprint, training... loss:0.71542\n",
      "[CSQ][659/2000][12:27:13] bit:1024, dataset:palmprint, training... loss:0.71532\n",
      "[CSQ][660/2000][12:27:22] bit:1024, dataset:palmprint, training... loss:0.71509\n",
      "[CSQ][661/2000][12:27:31] bit:1024, dataset:palmprint, training... loss:0.71536\n",
      "[CSQ][662/2000][12:27:39] bit:1024, dataset:palmprint, training... loss:0.71534\n",
      "[CSQ][663/2000][12:27:48] bit:1024, dataset:palmprint, training... loss:0.71484\n",
      "[CSQ][664/2000][12:27:57] bit:1024, dataset:palmprint, training... loss:0.71528\n",
      "[CSQ][665/2000][12:28:05] bit:1024, dataset:palmprint, training... loss:0.71500\n",
      "[CSQ][666/2000][12:28:14] bit:1024, dataset:palmprint, training... loss:0.71498\n",
      "[CSQ][667/2000][12:28:23] bit:1024, dataset:palmprint, training... loss:0.71506\n",
      "[CSQ][668/2000][12:28:33] bit:1024, dataset:palmprint, training... loss:0.71485\n",
      "[CSQ][669/2000][12:28:42] bit:1024, dataset:palmprint, training... loss:0.71479\n",
      "[CSQ][670/2000][12:28:52] bit:1024, dataset:palmprint, training... loss:0.71497\n",
      "[CSQ][671/2000][12:29:01] bit:1024, dataset:palmprint, training... loss:0.71501\n",
      "[CSQ][672/2000][12:29:11] bit:1024, dataset:palmprint, training... loss:0.71470\n",
      "[CSQ][673/2000][12:29:20] bit:1024, dataset:palmprint, training... loss:0.71455\n",
      "[CSQ][674/2000][12:29:31] bit:1024, dataset:palmprint, training... loss:0.71435\n",
      "[CSQ][675/2000][12:29:41] bit:1024, dataset:palmprint, training... loss:0.71452\n",
      "[CSQ][676/2000][12:29:51] bit:1024, dataset:palmprint, training... loss:0.71493\n",
      "[CSQ][677/2000][12:30:02] bit:1024, dataset:palmprint, training... loss:0.71451\n",
      "[CSQ][678/2000][12:30:13] bit:1024, dataset:palmprint, training... loss:0.71445\n",
      "[CSQ][679/2000][12:30:23] bit:1024, dataset:palmprint, training... loss:0.71453\n",
      "[CSQ][680/2000][12:30:34] bit:1024, dataset:palmprint, training... loss:0.71457\n",
      "[CSQ][681/2000][12:30:44] bit:1024, dataset:palmprint, training... loss:0.71417\n",
      "[CSQ][682/2000][12:30:54] bit:1024, dataset:palmprint, training... loss:0.71394\n",
      "[CSQ][683/2000][12:31:04] bit:1024, dataset:palmprint, training... loss:0.71439\n",
      "[CSQ][684/2000][12:31:13] bit:1024, dataset:palmprint, training... loss:0.71408\n",
      "[CSQ][685/2000][12:31:23] bit:1024, dataset:palmprint, training... loss:0.71399\n",
      "[CSQ][686/2000][12:31:32] bit:1024, dataset:palmprint, training... loss:0.71416\n",
      "[CSQ][687/2000][12:31:42] bit:1024, dataset:palmprint, training... loss:0.71378\n",
      "[CSQ][688/2000][12:31:51] bit:1024, dataset:palmprint, training... loss:0.71429\n",
      "[CSQ][689/2000][12:32:02] bit:1024, dataset:palmprint, training... loss:0.71402\n",
      "[CSQ][690/2000][12:32:13] bit:1024, dataset:palmprint, training... loss:0.71402\n",
      "[CSQ][691/2000][12:32:24] bit:1024, dataset:palmprint, training... loss:0.71392\n",
      "[CSQ][692/2000][12:32:34] bit:1024, dataset:palmprint, training... loss:0.71347\n",
      "[CSQ][693/2000][12:32:43] bit:1024, dataset:palmprint, training... loss:0.71382\n",
      "[CSQ][694/2000][12:32:53] bit:1024, dataset:palmprint, training... loss:0.71391\n",
      "[CSQ][695/2000][12:33:02] bit:1024, dataset:palmprint, training... loss:0.71343\n",
      "[CSQ][696/2000][12:33:11] bit:1024, dataset:palmprint, training... loss:0.71344\n",
      "[CSQ][697/2000][12:33:21] bit:1024, dataset:palmprint, training... loss:0.71366\n",
      "[CSQ][698/2000][12:33:31] bit:1024, dataset:palmprint, training... loss:0.71351\n",
      "[CSQ][699/2000][12:33:41] bit:1024, dataset:palmprint, training... loss:0.71390\n",
      "[CSQ][700/2000][12:33:51] bit:1024, dataset:palmprint, training... loss:0.71343\n",
      "[CSQ][701/2000][12:34:00] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.008\n",
      "[CSQ] epoch:701, bit:1024, dataset:palmprint,eer:0.00800, Best eer: 0.00800\n",
      "\b loss:0.71350\n",
      "[CSQ][702/2000][12:35:51] bit:1024, dataset:palmprint, training... loss:0.71359\n",
      "[CSQ][703/2000][12:36:00] bit:1024, dataset:palmprint, training... loss:0.71332\n",
      "[CSQ][704/2000][12:36:10] bit:1024, dataset:palmprint, training... loss:0.71317\n",
      "[CSQ][705/2000][12:36:20] bit:1024, dataset:palmprint, training... loss:0.71335\n",
      "[CSQ][706/2000][12:36:29] bit:1024, dataset:palmprint, training... loss:0.71342\n",
      "[CSQ][707/2000][12:36:39] bit:1024, dataset:palmprint, training... loss:0.71313\n",
      "[CSQ][708/2000][12:36:49] bit:1024, dataset:palmprint, training... loss:0.71306\n",
      "[CSQ][709/2000][12:37:01] bit:1024, dataset:palmprint, training... loss:0.71318\n",
      "[CSQ][710/2000][12:37:12] bit:1024, dataset:palmprint, training... loss:0.71281\n",
      "[CSQ][711/2000][12:37:20] bit:1024, dataset:palmprint, training... loss:0.71308\n",
      "[CSQ][712/2000][12:37:29] bit:1024, dataset:palmprint, training... loss:0.71361\n",
      "[CSQ][713/2000][12:37:38] bit:1024, dataset:palmprint, training... loss:0.71338\n",
      "[CSQ][714/2000][12:37:46] bit:1024, dataset:palmprint, training... loss:0.71262\n",
      "[CSQ][715/2000][12:37:55] bit:1024, dataset:palmprint, training... loss:0.71285\n",
      "[CSQ][716/2000][12:38:03] bit:1024, dataset:palmprint, training... loss:0.71264\n",
      "[CSQ][717/2000][12:38:13] bit:1024, dataset:palmprint, training... loss:0.71282\n",
      "[CSQ][718/2000][12:38:24] bit:1024, dataset:palmprint, training... loss:0.71272\n",
      "[CSQ][719/2000][12:38:33] bit:1024, dataset:palmprint, training... loss:0.71282\n",
      "[CSQ][720/2000][12:38:42] bit:1024, dataset:palmprint, training... loss:0.71232\n",
      "[CSQ][721/2000][12:38:52] bit:1024, dataset:palmprint, training... loss:0.71207\n",
      "[CSQ][722/2000][12:39:07] bit:1024, dataset:palmprint, training... loss:0.71270\n",
      "[CSQ][723/2000][12:39:18] bit:1024, dataset:palmprint, training... loss:0.71230\n",
      "[CSQ][724/2000][12:39:30] bit:1024, dataset:palmprint, training... loss:0.71238\n",
      "[CSQ][725/2000][12:39:41] bit:1024, dataset:palmprint, training... loss:0.71220\n",
      "[CSQ][726/2000][12:39:51] bit:1024, dataset:palmprint, training... loss:0.71206\n",
      "[CSQ][727/2000][12:40:00] bit:1024, dataset:palmprint, training... loss:0.71271\n",
      "[CSQ][728/2000][12:40:10] bit:1024, dataset:palmprint, training... loss:0.71237\n",
      "[CSQ][729/2000][12:40:19] bit:1024, dataset:palmprint, training... loss:0.71216\n",
      "[CSQ][730/2000][12:40:28] bit:1024, dataset:palmprint, training... loss:0.71191\n",
      "[CSQ][731/2000][12:40:38] bit:1024, dataset:palmprint, training... loss:0.71242\n",
      "[CSQ][732/2000][12:40:48] bit:1024, dataset:palmprint, training... loss:0.71188\n",
      "[CSQ][733/2000][12:40:58] bit:1024, dataset:palmprint, training... loss:0.71223\n",
      "[CSQ][734/2000][12:41:08] bit:1024, dataset:palmprint, training... loss:0.71218\n",
      "[CSQ][735/2000][12:41:18] bit:1024, dataset:palmprint, training... loss:0.71189\n",
      "[CSQ][736/2000][12:41:29] bit:1024, dataset:palmprint, training... loss:0.71224\n",
      "[CSQ][737/2000][12:41:44] bit:1024, dataset:palmprint, training... loss:0.71229\n",
      "[CSQ][738/2000][12:41:54] bit:1024, dataset:palmprint, training... loss:0.71213\n",
      "[CSQ][739/2000][12:42:04] bit:1024, dataset:palmprint, training... loss:0.71170\n",
      "[CSQ][740/2000][12:42:14] bit:1024, dataset:palmprint, training... loss:0.71194\n",
      "[CSQ][741/2000][12:42:23] bit:1024, dataset:palmprint, training... loss:0.71194\n",
      "[CSQ][742/2000][12:42:34] bit:1024, dataset:palmprint, training... loss:0.71204\n",
      "[CSQ][743/2000][12:42:44] bit:1024, dataset:palmprint, training... loss:0.71156\n",
      "[CSQ][744/2000][12:42:56] bit:1024, dataset:palmprint, training... loss:0.71208\n",
      "[CSQ][745/2000][12:43:07] bit:1024, dataset:palmprint, training... loss:0.71190\n",
      "[CSQ][746/2000][12:43:18] bit:1024, dataset:palmprint, training... loss:0.71160\n",
      "[CSQ][747/2000][12:43:29] bit:1024, dataset:palmprint, training... loss:0.71154\n",
      "[CSQ][748/2000][12:43:40] bit:1024, dataset:palmprint, training... loss:0.71132\n",
      "[CSQ][749/2000][12:43:50] bit:1024, dataset:palmprint, training... loss:0.71144\n",
      "[CSQ][750/2000][12:43:59] bit:1024, dataset:palmprint, training... loss:0.71142\n",
      "[CSQ][751/2000][12:44:08] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.007666666666666666\n",
      "[CSQ] epoch:751, bit:1024, dataset:palmprint,eer:0.00767, Best eer: 0.00767\n",
      "\b loss:0.71129\n",
      "[CSQ][752/2000][12:45:27] bit:1024, dataset:palmprint, training... loss:0.71129\n",
      "[CSQ][753/2000][12:45:36] bit:1024, dataset:palmprint, training... loss:0.71160\n",
      "[CSQ][754/2000][12:45:46] bit:1024, dataset:palmprint, training... loss:0.71132\n",
      "[CSQ][755/2000][12:45:55] bit:1024, dataset:palmprint, training... loss:0.71146\n",
      "[CSQ][756/2000][12:46:05] bit:1024, dataset:palmprint, training... loss:0.71116\n",
      "[CSQ][757/2000][12:46:14] bit:1024, dataset:palmprint, training... loss:0.71104\n",
      "[CSQ][758/2000][12:46:25] bit:1024, dataset:palmprint, training... loss:0.71105\n",
      "[CSQ][759/2000][12:46:35] bit:1024, dataset:palmprint, training... loss:0.71133\n",
      "[CSQ][760/2000][12:46:45] bit:1024, dataset:palmprint, training... loss:0.71141\n",
      "[CSQ][761/2000][12:46:56] bit:1024, dataset:palmprint, training... loss:0.71078\n",
      "[CSQ][762/2000][12:47:08] bit:1024, dataset:palmprint, training... loss:0.71081\n",
      "[CSQ][763/2000][12:47:18] bit:1024, dataset:palmprint, training... loss:0.71056\n",
      "[CSQ][764/2000][12:47:34] bit:1024, dataset:palmprint, training... loss:0.71090\n",
      "[CSQ][765/2000][12:47:45] bit:1024, dataset:palmprint, training... loss:0.71076\n",
      "[CSQ][766/2000][12:47:53] bit:1024, dataset:palmprint, training... loss:0.71098\n",
      "[CSQ][767/2000][12:48:02] bit:1024, dataset:palmprint, training... loss:0.71090\n",
      "[CSQ][768/2000][12:48:11] bit:1024, dataset:palmprint, training... loss:0.71064\n",
      "[CSQ][769/2000][12:48:19] bit:1024, dataset:palmprint, training... loss:0.71065\n",
      "[CSQ][770/2000][12:48:28] bit:1024, dataset:palmprint, training... loss:0.71048\n",
      "[CSQ][771/2000][12:48:37] bit:1024, dataset:palmprint, training... loss:0.71064\n",
      "[CSQ][772/2000][12:48:46] bit:1024, dataset:palmprint, training... loss:0.71049\n",
      "[CSQ][773/2000][12:48:57] bit:1024, dataset:palmprint, training... loss:0.71066\n",
      "[CSQ][774/2000][12:49:07] bit:1024, dataset:palmprint, training... loss:0.71056\n",
      "[CSQ][775/2000][12:49:17] bit:1024, dataset:palmprint, training... loss:0.71038\n",
      "[CSQ][776/2000][12:49:28] bit:1024, dataset:palmprint, training... loss:0.71004\n",
      "[CSQ][777/2000][12:49:39] bit:1024, dataset:palmprint, training... loss:0.71122\n",
      "[CSQ][778/2000][12:49:49] bit:1024, dataset:palmprint, training... loss:0.71004\n",
      "[CSQ][779/2000][12:49:59] bit:1024, dataset:palmprint, training... loss:0.71005\n",
      "[CSQ][780/2000][12:50:07] bit:1024, dataset:palmprint, training... loss:0.71016\n",
      "[CSQ][781/2000][12:50:16] bit:1024, dataset:palmprint, training... loss:0.71001\n",
      "[CSQ][782/2000][12:50:24] bit:1024, dataset:palmprint, training... loss:0.71018\n",
      "[CSQ][783/2000][12:50:33] bit:1024, dataset:palmprint, training... loss:0.71015\n",
      "[CSQ][784/2000][12:50:41] bit:1024, dataset:palmprint, training... loss:0.71024\n",
      "[CSQ][785/2000][12:50:49] bit:1024, dataset:palmprint, training... loss:0.71010\n",
      "[CSQ][786/2000][12:50:58] bit:1024, dataset:palmprint, training... loss:0.71039\n",
      "[CSQ][787/2000][12:51:06] bit:1024, dataset:palmprint, training... loss:0.71014\n",
      "[CSQ][788/2000][12:51:14] bit:1024, dataset:palmprint, training... loss:0.70964\n",
      "[CSQ][789/2000][12:51:23] bit:1024, dataset:palmprint, training... loss:0.70973\n",
      "[CSQ][790/2000][12:51:31] bit:1024, dataset:palmprint, training... loss:0.70951\n",
      "[CSQ][791/2000][12:51:39] bit:1024, dataset:palmprint, training... loss:0.70994\n",
      "[CSQ][792/2000][12:51:48] bit:1024, dataset:palmprint, training... loss:0.70955\n",
      "[CSQ][793/2000][12:51:58] bit:1024, dataset:palmprint, training... loss:0.70956\n",
      "[CSQ][794/2000][12:52:15] bit:1024, dataset:palmprint, training... loss:0.70980\n",
      "[CSQ][795/2000][12:52:24] bit:1024, dataset:palmprint, training... loss:0.70992\n",
      "[CSQ][796/2000][12:52:32] bit:1024, dataset:palmprint, training... loss:0.70976\n",
      "[CSQ][797/2000][12:52:48] bit:1024, dataset:palmprint, training... loss:0.70943\n",
      "[CSQ][798/2000][12:53:08] bit:1024, dataset:palmprint, training... loss:0.70982\n",
      "[CSQ][799/2000][12:53:26] bit:1024, dataset:palmprint, training... loss:0.71001\n",
      "[CSQ][800/2000][12:53:44] bit:1024, dataset:palmprint, training... loss:0.70972\n",
      "[CSQ][801/2000][12:54:03] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.007666666666666666\n",
      "[CSQ] epoch:801, bit:1024, dataset:palmprint,eer:0.00767, Best eer: 0.00767\n",
      "\b loss:0.70956\n",
      "[CSQ][802/2000][12:56:06] bit:1024, dataset:palmprint, training... loss:0.70954\n",
      "[CSQ][803/2000][12:56:15] bit:1024, dataset:palmprint, training... loss:0.70905\n",
      "[CSQ][804/2000][12:56:30] bit:1024, dataset:palmprint, training... loss:0.70965\n",
      "[CSQ][805/2000][12:56:48] bit:1024, dataset:palmprint, training... loss:0.70924\n",
      "[CSQ][806/2000][12:57:06] bit:1024, dataset:palmprint, training... loss:0.70877\n",
      "[CSQ][807/2000][12:57:24] bit:1024, dataset:palmprint, training... loss:0.70917\n",
      "[CSQ][808/2000][12:57:44] bit:1024, dataset:palmprint, training... loss:0.70977\n",
      "[CSQ][809/2000][12:58:03] bit:1024, dataset:palmprint, training... loss:0.70909\n",
      "[CSQ][810/2000][12:58:21] bit:1024, dataset:palmprint, training... loss:0.70900\n",
      "[CSQ][811/2000][12:58:32] bit:1024, dataset:palmprint, training... loss:0.70890\n",
      "[CSQ][812/2000][12:58:43] bit:1024, dataset:palmprint, training... loss:0.70952\n",
      "[CSQ][813/2000][12:58:52] bit:1024, dataset:palmprint, training... loss:0.70887\n",
      "[CSQ][814/2000][12:59:00] bit:1024, dataset:palmprint, training... loss:0.70950\n",
      "[CSQ][815/2000][12:59:09] bit:1024, dataset:palmprint, training... loss:0.70888\n",
      "[CSQ][816/2000][12:59:18] bit:1024, dataset:palmprint, training... loss:0.70865\n",
      "[CSQ][817/2000][12:59:31] bit:1024, dataset:palmprint, training... loss:0.70863\n",
      "[CSQ][818/2000][12:59:50] bit:1024, dataset:palmprint, training... loss:0.70837\n",
      "[CSQ][819/2000][13:00:08] bit:1024, dataset:palmprint, training... loss:0.70845\n",
      "[CSQ][820/2000][13:00:27] bit:1024, dataset:palmprint, training... loss:0.70873\n",
      "[CSQ][821/2000][13:00:47] bit:1024, dataset:palmprint, training... loss:0.70877\n",
      "[CSQ][822/2000][13:01:06] bit:1024, dataset:palmprint, training... loss:0.70881\n",
      "[CSQ][823/2000][13:01:24] bit:1024, dataset:palmprint, training... loss:0.70863\n",
      "[CSQ][824/2000][13:01:37] bit:1024, dataset:palmprint, training... loss:0.70910\n",
      "[CSQ][825/2000][13:01:48] bit:1024, dataset:palmprint, training... loss:0.70852\n",
      "[CSQ][826/2000][13:01:57] bit:1024, dataset:palmprint, training... loss:0.70850\n",
      "[CSQ][827/2000][13:02:11] bit:1024, dataset:palmprint, training... loss:0.70868\n",
      "[CSQ][828/2000][13:02:19] bit:1024, dataset:palmprint, training... loss:0.70822\n",
      "[CSQ][829/2000][13:02:28] bit:1024, dataset:palmprint, training... loss:0.70877\n",
      "[CSQ][830/2000][13:02:36] bit:1024, dataset:palmprint, training... loss:0.70833\n",
      "[CSQ][831/2000][13:02:45] bit:1024, dataset:palmprint, training... loss:0.70845\n",
      "[CSQ][832/2000][13:02:54] bit:1024, dataset:palmprint, training... loss:0.70816\n",
      "[CSQ][833/2000][13:03:03] bit:1024, dataset:palmprint, training... loss:0.70847\n",
      "[CSQ][834/2000][13:03:12] bit:1024, dataset:palmprint, training... loss:0.70843\n",
      "[CSQ][835/2000][13:03:21] bit:1024, dataset:palmprint, training... loss:0.70799\n",
      "[CSQ][836/2000][13:03:30] bit:1024, dataset:palmprint, training... loss:0.70810\n",
      "[CSQ][837/2000][13:03:39] bit:1024, dataset:palmprint, training... loss:0.70814\n",
      "[CSQ][838/2000][13:03:49] bit:1024, dataset:palmprint, training... loss:0.70819\n",
      "[CSQ][839/2000][13:03:59] bit:1024, dataset:palmprint, training... loss:0.70787\n",
      "[CSQ][840/2000][13:04:08] bit:1024, dataset:palmprint, training... loss:0.70806\n",
      "[CSQ][841/2000][13:04:18] bit:1024, dataset:palmprint, training... loss:0.70794\n",
      "[CSQ][842/2000][13:04:28] bit:1024, dataset:palmprint, training... loss:0.70783\n",
      "[CSQ][843/2000][13:04:37] bit:1024, dataset:palmprint, training... loss:0.70813\n",
      "[CSQ][844/2000][13:04:46] bit:1024, dataset:palmprint, training... loss:0.70764\n",
      "[CSQ][845/2000][13:04:56] bit:1024, dataset:palmprint, training... loss:0.70780\n",
      "[CSQ][846/2000][13:05:06] bit:1024, dataset:palmprint, training... loss:0.70800\n",
      "[CSQ][847/2000][13:05:16] bit:1024, dataset:palmprint, training... loss:0.70792\n",
      "[CSQ][848/2000][13:05:24] bit:1024, dataset:palmprint, training... loss:0.70756\n",
      "[CSQ][849/2000][13:05:35] bit:1024, dataset:palmprint, training... loss:0.70766\n",
      "[CSQ][850/2000][13:05:44] bit:1024, dataset:palmprint, training... loss:0.70779\n",
      "[CSQ][851/2000][13:05:53] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.006\n",
      "[CSQ] epoch:851, bit:1024, dataset:palmprint,eer:0.00600, Best eer: 0.00600\n",
      "\b loss:0.70763\n",
      "[CSQ][852/2000][13:07:06] bit:1024, dataset:palmprint, training... loss:0.70776\n",
      "[CSQ][853/2000][13:07:15] bit:1024, dataset:palmprint, training... loss:0.70775\n",
      "[CSQ][854/2000][13:07:25] bit:1024, dataset:palmprint, training... loss:0.70787\n",
      "[CSQ][855/2000][13:07:34] bit:1024, dataset:palmprint, training... loss:0.70754\n",
      "[CSQ][856/2000][13:07:44] bit:1024, dataset:palmprint, training... loss:0.70740\n",
      "[CSQ][857/2000][13:07:54] bit:1024, dataset:palmprint, training... loss:0.70726\n",
      "[CSQ][858/2000][13:08:02] bit:1024, dataset:palmprint, training... loss:0.70746\n",
      "[CSQ][859/2000][13:08:14] bit:1024, dataset:palmprint, training... loss:0.70721\n",
      "[CSQ][860/2000][13:08:23] bit:1024, dataset:palmprint, training... loss:0.70764\n",
      "[CSQ][861/2000][13:08:32] bit:1024, dataset:palmprint, training... loss:0.70739\n",
      "[CSQ][862/2000][13:08:42] bit:1024, dataset:palmprint, training... loss:0.70711\n",
      "[CSQ][863/2000][13:08:52] bit:1024, dataset:palmprint, training... loss:0.70714\n",
      "[CSQ][864/2000][13:09:02] bit:1024, dataset:palmprint, training... loss:0.70726\n",
      "[CSQ][865/2000][13:09:10] bit:1024, dataset:palmprint, training... loss:0.70748\n",
      "[CSQ][866/2000][13:09:21] bit:1024, dataset:palmprint, training... loss:0.70700\n",
      "[CSQ][867/2000][13:09:30] bit:1024, dataset:palmprint, training... loss:0.70723\n",
      "[CSQ][868/2000][13:09:39] bit:1024, dataset:palmprint, training... loss:0.70712\n",
      "[CSQ][869/2000][13:09:49] bit:1024, dataset:palmprint, training... loss:0.70656\n",
      "[CSQ][870/2000][13:09:59] bit:1024, dataset:palmprint, training... loss:0.70704\n",
      "[CSQ][871/2000][13:10:09] bit:1024, dataset:palmprint, training... loss:0.70732\n",
      "[CSQ][872/2000][13:10:17] bit:1024, dataset:palmprint, training... loss:0.70667\n",
      "[CSQ][873/2000][13:10:28] bit:1024, dataset:palmprint, training... loss:0.70690\n",
      "[CSQ][874/2000][13:10:37] bit:1024, dataset:palmprint, training... loss:0.70692\n",
      "[CSQ][875/2000][13:10:46] bit:1024, dataset:palmprint, training... loss:0.70652\n",
      "[CSQ][876/2000][13:10:55] bit:1024, dataset:palmprint, training... loss:0.70663\n",
      "[CSQ][877/2000][13:11:06] bit:1024, dataset:palmprint, training... loss:0.70689\n",
      "[CSQ][878/2000][13:11:15] bit:1024, dataset:palmprint, training... loss:0.70672\n",
      "[CSQ][879/2000][13:11:24] bit:1024, dataset:palmprint, training... loss:0.70655\n",
      "[CSQ][880/2000][13:11:35] bit:1024, dataset:palmprint, training... loss:0.70674\n",
      "[CSQ][881/2000][13:11:44] bit:1024, dataset:palmprint, training... loss:0.70708\n",
      "[CSQ][882/2000][13:11:53] bit:1024, dataset:palmprint, training... loss:0.70661\n",
      "[CSQ][883/2000][13:12:03] bit:1024, dataset:palmprint, training... loss:0.70635\n",
      "[CSQ][884/2000][13:12:13] bit:1024, dataset:palmprint, training... loss:0.70647\n",
      "[CSQ][885/2000][13:12:23] bit:1024, dataset:palmprint, training... loss:0.70649\n",
      "[CSQ][886/2000][13:12:32] bit:1024, dataset:palmprint, training... loss:0.70613\n",
      "[CSQ][887/2000][13:12:42] bit:1024, dataset:palmprint, training... loss:0.70660\n",
      "[CSQ][888/2000][13:12:52] bit:1024, dataset:palmprint, training... loss:0.70614\n",
      "[CSQ][889/2000][13:13:01] bit:1024, dataset:palmprint, training... loss:0.70655\n",
      "[CSQ][890/2000][13:13:10] bit:1024, dataset:palmprint, training... loss:0.70644\n",
      "[CSQ][891/2000][13:13:21] bit:1024, dataset:palmprint, training... loss:0.70618\n",
      "[CSQ][892/2000][13:13:30] bit:1024, dataset:palmprint, training... loss:0.70637\n",
      "[CSQ][893/2000][13:13:39] bit:1024, dataset:palmprint, training... loss:0.70577\n",
      "[CSQ][894/2000][13:13:50] bit:1024, dataset:palmprint, training... loss:0.70635\n",
      "[CSQ][895/2000][13:14:00] bit:1024, dataset:palmprint, training... loss:0.70624\n",
      "[CSQ][896/2000][13:14:09] bit:1024, dataset:palmprint, training... loss:0.70589\n",
      "[CSQ][897/2000][13:14:18] bit:1024, dataset:palmprint, training... loss:0.70654\n",
      "[CSQ][898/2000][13:14:29] bit:1024, dataset:palmprint, training... loss:0.70611\n",
      "[CSQ][899/2000][13:14:44] bit:1024, dataset:palmprint, training... loss:0.70576\n",
      "[CSQ][900/2000][13:14:52] bit:1024, dataset:palmprint, training... loss:0.70627\n",
      "[CSQ][901/2000][13:15:01] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.007\n",
      "[CSQ] epoch:901, bit:1024, dataset:palmprint,eer:0.00700, Best eer: 0.00600\n",
      "\b loss:0.70591\n",
      "[CSQ][902/2000][13:16:11] bit:1024, dataset:palmprint, training... loss:0.70612\n",
      "[CSQ][903/2000][13:16:20] bit:1024, dataset:palmprint, training... loss:0.70582\n",
      "[CSQ][904/2000][13:16:30] bit:1024, dataset:palmprint, training... loss:0.70607\n",
      "[CSQ][905/2000][13:16:39] bit:1024, dataset:palmprint, training... loss:0.70627\n",
      "[CSQ][906/2000][13:16:49] bit:1024, dataset:palmprint, training... loss:0.70609\n",
      "[CSQ][907/2000][13:16:58] bit:1024, dataset:palmprint, training... loss:0.70619\n",
      "[CSQ][908/2000][13:17:09] bit:1024, dataset:palmprint, training... loss:0.70566\n",
      "[CSQ][909/2000][13:17:18] bit:1024, dataset:palmprint, training... loss:0.70612\n",
      "[CSQ][910/2000][13:17:27] bit:1024, dataset:palmprint, training... loss:0.70582\n",
      "[CSQ][911/2000][13:17:37] bit:1024, dataset:palmprint, training... loss:0.70576\n",
      "[CSQ][912/2000][13:17:48] bit:1024, dataset:palmprint, training... loss:0.70545\n",
      "[CSQ][913/2000][13:17:57] bit:1024, dataset:palmprint, training... loss:0.70484\n",
      "[CSQ][914/2000][13:18:06] bit:1024, dataset:palmprint, training... loss:0.70557\n",
      "[CSQ][915/2000][13:18:16] bit:1024, dataset:palmprint, training... loss:0.70532\n",
      "[CSQ][916/2000][13:18:25] bit:1024, dataset:palmprint, training... loss:0.70565\n",
      "[CSQ][917/2000][13:18:34] bit:1024, dataset:palmprint, training... loss:0.70562\n",
      "[CSQ][918/2000][13:18:45] bit:1024, dataset:palmprint, training... loss:0.70513\n",
      "[CSQ][919/2000][13:18:54] bit:1024, dataset:palmprint, training... loss:0.70569\n",
      "[CSQ][920/2000][13:19:04] bit:1024, dataset:palmprint, training... loss:0.70514\n",
      "[CSQ][921/2000][13:19:13] bit:1024, dataset:palmprint, training... loss:0.70508\n",
      "[CSQ][922/2000][13:19:24] bit:1024, dataset:palmprint, training... loss:0.70566\n",
      "[CSQ][923/2000][13:19:33] bit:1024, dataset:palmprint, training... loss:0.70521\n",
      "[CSQ][924/2000][13:19:42] bit:1024, dataset:palmprint, training... loss:0.70557\n",
      "[CSQ][925/2000][13:19:51] bit:1024, dataset:palmprint, training... loss:0.70505\n",
      "[CSQ][926/2000][13:20:02] bit:1024, dataset:palmprint, training... loss:0.70527\n",
      "[CSQ][927/2000][13:20:11] bit:1024, dataset:palmprint, training... loss:0.70510\n",
      "[CSQ][928/2000][13:20:20] bit:1024, dataset:palmprint, training... loss:0.70528\n",
      "[CSQ][929/2000][13:20:30] bit:1024, dataset:palmprint, training... loss:0.70520\n",
      "[CSQ][930/2000][13:20:40] bit:1024, dataset:palmprint, training... loss:0.70493\n",
      "[CSQ][931/2000][13:20:48] bit:1024, dataset:palmprint, training... loss:0.70525\n",
      "[CSQ][932/2000][13:20:59] bit:1024, dataset:palmprint, training... loss:0.70510\n",
      "[CSQ][933/2000][13:21:09] bit:1024, dataset:palmprint, training... loss:0.70502\n",
      "[CSQ][934/2000][13:21:19] bit:1024, dataset:palmprint, training... loss:0.70480\n",
      "[CSQ][935/2000][13:21:28] bit:1024, dataset:palmprint, training... loss:0.70534\n",
      "[CSQ][936/2000][13:21:38] bit:1024, dataset:palmprint, training... loss:0.70497\n",
      "[CSQ][937/2000][13:21:48] bit:1024, dataset:palmprint, training... loss:0.70491\n",
      "[CSQ][938/2000][13:21:57] bit:1024, dataset:palmprint, training... loss:0.70486\n",
      "[CSQ][939/2000][13:22:06] bit:1024, dataset:palmprint, training... loss:0.70483\n",
      "[CSQ][940/2000][13:22:16] bit:1024, dataset:palmprint, training... loss:0.70476\n",
      "[CSQ][941/2000][13:22:26] bit:1024, dataset:palmprint, training... loss:0.70487\n",
      "[CSQ][942/2000][13:22:35] bit:1024, dataset:palmprint, training... loss:0.70457\n",
      "[CSQ][943/2000][13:22:45] bit:1024, dataset:palmprint, training... loss:0.70472\n",
      "[CSQ][944/2000][13:22:55] bit:1024, dataset:palmprint, training... loss:0.70447\n",
      "[CSQ][945/2000][13:23:04] bit:1024, dataset:palmprint, training... loss:0.70480\n",
      "[CSQ][946/2000][13:23:12] bit:1024, dataset:palmprint, training... loss:0.70496\n",
      "[CSQ][947/2000][13:23:23] bit:1024, dataset:palmprint, training... loss:0.70434\n",
      "[CSQ][948/2000][13:23:32] bit:1024, dataset:palmprint, training... loss:0.70389\n",
      "[CSQ][949/2000][13:23:41] bit:1024, dataset:palmprint, training... loss:0.70465\n",
      "[CSQ][950/2000][13:23:52] bit:1024, dataset:palmprint, training... loss:0.70447\n",
      "[CSQ][951/2000][13:24:02] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.005666666666666667\n",
      "[CSQ] epoch:951, bit:1024, dataset:palmprint,eer:0.00567, Best eer: 0.00567\n",
      "\b loss:0.70424\n",
      "[CSQ][952/2000][13:25:17] bit:1024, dataset:palmprint, training... loss:0.70448\n",
      "[CSQ][953/2000][13:25:26] bit:1024, dataset:palmprint, training... loss:0.70453\n",
      "[CSQ][954/2000][13:25:35] bit:1024, dataset:palmprint, training... loss:0.70438\n",
      "[CSQ][955/2000][13:25:46] bit:1024, dataset:palmprint, training... loss:0.70446\n",
      "[CSQ][956/2000][13:25:56] bit:1024, dataset:palmprint, training... loss:0.70395\n",
      "[CSQ][957/2000][13:26:05] bit:1024, dataset:palmprint, training... loss:0.70405\n",
      "[CSQ][958/2000][13:26:14] bit:1024, dataset:palmprint, training... loss:0.70448\n",
      "[CSQ][959/2000][13:26:25] bit:1024, dataset:palmprint, training... loss:0.70430\n",
      "[CSQ][960/2000][13:26:34] bit:1024, dataset:palmprint, training... loss:0.70406\n",
      "[CSQ][961/2000][13:26:43] bit:1024, dataset:palmprint, training... loss:0.70423\n",
      "[CSQ][962/2000][13:26:53] bit:1024, dataset:palmprint, training... loss:0.70420\n",
      "[CSQ][963/2000][13:27:04] bit:1024, dataset:palmprint, training... loss:0.70437\n",
      "[CSQ][964/2000][13:27:16] bit:1024, dataset:palmprint, training... loss:0.70396\n",
      "[CSQ][965/2000][13:27:26] bit:1024, dataset:palmprint, training... loss:0.70348\n",
      "[CSQ][966/2000][13:27:34] bit:1024, dataset:palmprint, training... loss:0.70430\n",
      "[CSQ][967/2000][13:27:43] bit:1024, dataset:palmprint, training... loss:0.70442\n",
      "[CSQ][968/2000][13:27:52] bit:1024, dataset:palmprint, training... loss:0.70412\n",
      "[CSQ][969/2000][13:28:01] bit:1024, dataset:palmprint, training... loss:0.70390\n",
      "[CSQ][970/2000][13:28:09] bit:1024, dataset:palmprint, training... loss:0.70400\n",
      "[CSQ][971/2000][13:28:18] bit:1024, dataset:palmprint, training... loss:0.70415\n",
      "[CSQ][972/2000][13:28:26] bit:1024, dataset:palmprint, training... loss:0.70401\n",
      "[CSQ][973/2000][13:28:35] bit:1024, dataset:palmprint, training... loss:0.70343\n",
      "[CSQ][974/2000][13:28:44] bit:1024, dataset:palmprint, training... loss:0.70407\n",
      "[CSQ][975/2000][13:28:53] bit:1024, dataset:palmprint, training... loss:0.70364\n",
      "[CSQ][976/2000][13:29:04] bit:1024, dataset:palmprint, training... loss:0.70388\n",
      "[CSQ][977/2000][13:29:13] bit:1024, dataset:palmprint, training... loss:0.70391\n",
      "[CSQ][978/2000][13:29:22] bit:1024, dataset:palmprint, training... loss:0.70367\n",
      "[CSQ][979/2000][13:29:33] bit:1024, dataset:palmprint, training... loss:0.70375\n",
      "[CSQ][980/2000][13:29:42] bit:1024, dataset:palmprint, training... loss:0.70404\n",
      "[CSQ][981/2000][13:29:51] bit:1024, dataset:palmprint, training... loss:0.70353\n",
      "[CSQ][982/2000][13:30:01] bit:1024, dataset:palmprint, training... loss:0.70387\n",
      "[CSQ][983/2000][13:30:11] bit:1024, dataset:palmprint, training... loss:0.70387\n",
      "[CSQ][984/2000][13:30:20] bit:1024, dataset:palmprint, training... loss:0.70383\n",
      "[CSQ][985/2000][13:30:29] bit:1024, dataset:palmprint, training... loss:0.70334\n",
      "[CSQ][986/2000][13:30:40] bit:1024, dataset:palmprint, training... loss:0.70341\n",
      "[CSQ][987/2000][13:30:49] bit:1024, dataset:palmprint, training... loss:0.70357\n",
      "[CSQ][988/2000][13:30:58] bit:1024, dataset:palmprint, training... loss:0.70349\n",
      "[CSQ][989/2000][13:31:09] bit:1024, dataset:palmprint, training... loss:0.70318\n",
      "[CSQ][990/2000][13:31:19] bit:1024, dataset:palmprint, training... loss:0.70289\n",
      "[CSQ][991/2000][13:31:28] bit:1024, dataset:palmprint, training... loss:0.70358\n",
      "[CSQ][992/2000][13:31:37] bit:1024, dataset:palmprint, training... loss:0.70325\n",
      "[CSQ][993/2000][13:31:48] bit:1024, dataset:palmprint, training... loss:0.70350\n",
      "[CSQ][994/2000][13:31:57] bit:1024, dataset:palmprint, training... loss:0.70329\n",
      "[CSQ][995/2000][13:32:06] bit:1024, dataset:palmprint, training... loss:0.70322\n",
      "[CSQ][996/2000][13:32:15] bit:1024, dataset:palmprint, training... loss:0.70276\n",
      "[CSQ][997/2000][13:32:26] bit:1024, dataset:palmprint, training... loss:0.70276\n",
      "[CSQ][998/2000][13:32:35] bit:1024, dataset:palmprint, training... loss:0.70324\n",
      "[CSQ][999/2000][13:32:44] bit:1024, dataset:palmprint, training... loss:0.70264\n",
      "[CSQ][1000/2000][13:32:54] bit:1024, dataset:palmprint, training... loss:0.70239\n",
      "[CSQ][1001/2000][13:33:04] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.006\n",
      "[CSQ] epoch:1001, bit:1024, dataset:palmprint,eer:0.00600, Best eer: 0.00567\n",
      "\b loss:0.70319\n",
      "[CSQ][1002/2000][13:34:20] bit:1024, dataset:palmprint, training... loss:0.70250\n",
      "[CSQ][1003/2000][13:34:29] bit:1024, dataset:palmprint, training... loss:0.70290\n",
      "[CSQ][1004/2000][13:34:38] bit:1024, dataset:palmprint, training... loss:0.70285\n",
      "[CSQ][1005/2000][13:34:49] bit:1024, dataset:palmprint, training... loss:0.70314\n",
      "[CSQ][1006/2000][13:34:58] bit:1024, dataset:palmprint, training... loss:0.70314\n",
      "[CSQ][1007/2000][13:35:06] bit:1024, dataset:palmprint, training... loss:0.70331\n",
      "[CSQ][1008/2000][13:35:16] bit:1024, dataset:palmprint, training... loss:0.70261\n",
      "[CSQ][1009/2000][13:35:26] bit:1024, dataset:palmprint, training... loss:0.70279\n",
      "[CSQ][1010/2000][13:35:35] bit:1024, dataset:palmprint, training... loss:0.70292\n",
      "[CSQ][1011/2000][13:35:44] bit:1024, dataset:palmprint, training... loss:0.70307\n",
      "[CSQ][1012/2000][13:35:55] bit:1024, dataset:palmprint, training... loss:0.70270\n",
      "[CSQ][1013/2000][13:36:04] bit:1024, dataset:palmprint, training... loss:0.70245\n",
      "[CSQ][1014/2000][13:36:12] bit:1024, dataset:palmprint, training... loss:0.70214\n",
      "[CSQ][1015/2000][13:36:22] bit:1024, dataset:palmprint, training... loss:0.70263\n",
      "[CSQ][1016/2000][13:36:33] bit:1024, dataset:palmprint, training... loss:0.70250\n",
      "[CSQ][1017/2000][13:36:42] bit:1024, dataset:palmprint, training... loss:0.70250\n",
      "[CSQ][1018/2000][13:36:51] bit:1024, dataset:palmprint, training... loss:0.70245\n",
      "[CSQ][1019/2000][13:37:01] bit:1024, dataset:palmprint, training... loss:0.70188\n",
      "[CSQ][1020/2000][13:37:11] bit:1024, dataset:palmprint, training... loss:0.70241\n",
      "[CSQ][1021/2000][13:37:20] bit:1024, dataset:palmprint, training... loss:0.70222\n",
      "[CSQ][1022/2000][13:37:29] bit:1024, dataset:palmprint, training... loss:0.70239\n",
      "[CSQ][1023/2000][13:37:40] bit:1024, dataset:palmprint, training... loss:0.70241\n",
      "[CSQ][1024/2000][13:37:49] bit:1024, dataset:palmprint, training... loss:0.70181\n",
      "[CSQ][1025/2000][13:37:58] bit:1024, dataset:palmprint, training... loss:0.70269\n",
      "[CSQ][1026/2000][13:38:08] bit:1024, dataset:palmprint, training... loss:0.70248\n",
      "[CSQ][1027/2000][13:38:18] bit:1024, dataset:palmprint, training... loss:0.70251\n",
      "[CSQ][1028/2000][13:38:27] bit:1024, dataset:palmprint, training... loss:0.70199\n",
      "[CSQ][1029/2000][13:38:37] bit:1024, dataset:palmprint, training... loss:0.70226\n",
      "[CSQ][1030/2000][13:38:47] bit:1024, dataset:palmprint, training... loss:0.70207\n",
      "[CSQ][1031/2000][13:38:56] bit:1024, dataset:palmprint, training... loss:0.70146\n",
      "[CSQ][1032/2000][13:39:05] bit:1024, dataset:palmprint, training... loss:0.70235\n",
      "[CSQ][1033/2000][13:39:15] bit:1024, dataset:palmprint, training... loss:0.70199\n",
      "[CSQ][1034/2000][13:39:25] bit:1024, dataset:palmprint, training... loss:0.70253\n",
      "[CSQ][1035/2000][13:39:34] bit:1024, dataset:palmprint, training... loss:0.70201\n",
      "[CSQ][1036/2000][13:39:44] bit:1024, dataset:palmprint, training... loss:0.70231\n",
      "[CSQ][1037/2000][13:39:57] bit:1024, dataset:palmprint, training... loss:0.70190\n",
      "[CSQ][1038/2000][13:40:06] bit:1024, dataset:palmprint, training... loss:0.70237\n",
      "[CSQ][1039/2000][13:40:15] bit:1024, dataset:palmprint, training... loss:0.70222\n",
      "[CSQ][1040/2000][13:40:23] bit:1024, dataset:palmprint, training... loss:0.70182\n",
      "[CSQ][1041/2000][13:40:32] bit:1024, dataset:palmprint, training... loss:0.70220\n",
      "[CSQ][1042/2000][13:40:40] bit:1024, dataset:palmprint, training... loss:0.70170\n",
      "[CSQ][1043/2000][13:40:49] bit:1024, dataset:palmprint, training... loss:0.70245\n",
      "[CSQ][1044/2000][13:40:58] bit:1024, dataset:palmprint, training... loss:0.70160\n",
      "[CSQ][1045/2000][13:41:07] bit:1024, dataset:palmprint, training... loss:0.70196\n",
      "[CSQ][1046/2000][13:41:15] bit:1024, dataset:palmprint, training... loss:0.70150\n",
      "[CSQ][1047/2000][13:41:24] bit:1024, dataset:palmprint, training... loss:0.70199\n",
      "[CSQ][1048/2000][13:41:33] bit:1024, dataset:palmprint, training... loss:0.70172\n",
      "[CSQ][1049/2000][13:41:41] bit:1024, dataset:palmprint, training... loss:0.70166\n",
      "[CSQ][1050/2000][13:41:51] bit:1024, dataset:palmprint, training... loss:0.70153\n",
      "[CSQ][1051/2000][13:42:00] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.004666666666666667\n",
      "[CSQ] epoch:1051, bit:1024, dataset:palmprint,eer:0.00467, Best eer: 0.00467\n",
      "\b loss:0.70189\n",
      "[CSQ][1052/2000][13:43:15] bit:1024, dataset:palmprint, training... loss:0.70180\n",
      "[CSQ][1053/2000][13:43:25] bit:1024, dataset:palmprint, training... loss:0.70155\n",
      "[CSQ][1054/2000][13:43:35] bit:1024, dataset:palmprint, training... loss:0.70141\n",
      "[CSQ][1055/2000][13:43:43] bit:1024, dataset:palmprint, training... loss:0.70163\n",
      "[CSQ][1056/2000][13:43:54] bit:1024, dataset:palmprint, training... loss:0.70144\n",
      "[CSQ][1057/2000][13:44:03] bit:1024, dataset:palmprint, training... loss:0.70106\n",
      "[CSQ][1058/2000][13:44:12] bit:1024, dataset:palmprint, training... loss:0.70181\n",
      "[CSQ][1059/2000][13:44:21] bit:1024, dataset:palmprint, training... loss:0.70117\n",
      "[CSQ][1060/2000][13:44:32] bit:1024, dataset:palmprint, training... loss:0.70172\n",
      "[CSQ][1061/2000][13:44:42] bit:1024, dataset:palmprint, training... loss:0.70096\n",
      "[CSQ][1062/2000][13:44:50] bit:1024, dataset:palmprint, training... loss:0.70097\n",
      "[CSQ][1063/2000][13:45:00] bit:1024, dataset:palmprint, training... loss:0.70139\n",
      "[CSQ][1064/2000][13:45:11] bit:1024, dataset:palmprint, training... loss:0.70123\n",
      "[CSQ][1065/2000][13:45:20] bit:1024, dataset:palmprint, training... loss:0.70128\n",
      "[CSQ][1066/2000][13:45:29] bit:1024, dataset:palmprint, training... loss:0.70152\n",
      "[CSQ][1067/2000][13:45:39] bit:1024, dataset:palmprint, training... loss:0.70120\n",
      "[CSQ][1068/2000][13:45:49] bit:1024, dataset:palmprint, training... loss:0.70057\n",
      "[CSQ][1069/2000][13:45:58] bit:1024, dataset:palmprint, training... loss:0.70116\n",
      "[CSQ][1070/2000][13:46:07] bit:1024, dataset:palmprint, training... loss:0.70104\n",
      "[CSQ][1071/2000][13:46:18] bit:1024, dataset:palmprint, training... loss:0.70144\n",
      "[CSQ][1072/2000][13:46:27] bit:1024, dataset:palmprint, training... loss:0.70098\n",
      "[CSQ][1073/2000][13:46:36] bit:1024, dataset:palmprint, training... loss:0.70126\n",
      "[CSQ][1074/2000][13:46:46] bit:1024, dataset:palmprint, training... loss:0.70110\n",
      "[CSQ][1075/2000][13:46:55] bit:1024, dataset:palmprint, training... loss:0.70077\n",
      "[CSQ][1076/2000][13:47:03] bit:1024, dataset:palmprint, training... loss:0.70112\n",
      "[CSQ][1077/2000][13:47:13] bit:1024, dataset:palmprint, training... loss:0.70097\n",
      "[CSQ][1078/2000][13:47:23] bit:1024, dataset:palmprint, training... loss:0.70080\n",
      "[CSQ][1079/2000][13:47:32] bit:1024, dataset:palmprint, training... loss:0.70094\n",
      "[CSQ][1080/2000][13:47:41] bit:1024, dataset:palmprint, training... loss:0.70076\n",
      "[CSQ][1081/2000][13:47:51] bit:1024, dataset:palmprint, training... loss:0.70065\n",
      "[CSQ][1082/2000][13:48:01] bit:1024, dataset:palmprint, training... loss:0.70096\n",
      "[CSQ][1083/2000][13:48:09] bit:1024, dataset:palmprint, training... loss:0.70068\n",
      "[CSQ][1084/2000][13:48:20] bit:1024, dataset:palmprint, training... loss:0.70062\n",
      "[CSQ][1085/2000][13:48:30] bit:1024, dataset:palmprint, training... loss:0.70074\n",
      "[CSQ][1086/2000][13:48:39] bit:1024, dataset:palmprint, training... loss:0.70088\n",
      "[CSQ][1087/2000][13:48:48] bit:1024, dataset:palmprint, training... loss:0.70095\n",
      "[CSQ][1088/2000][13:48:58] bit:1024, dataset:palmprint, training... loss:0.70008\n",
      "[CSQ][1089/2000][13:49:07] bit:1024, dataset:palmprint, training... loss:0.70061\n",
      "[CSQ][1090/2000][13:49:16] bit:1024, dataset:palmprint, training... loss:0.70062\n",
      "[CSQ][1091/2000][13:49:27] bit:1024, dataset:palmprint, training... loss:0.70063\n",
      "[CSQ][1092/2000][13:49:36] bit:1024, dataset:palmprint, training... loss:0.70042\n",
      "[CSQ][1093/2000][13:49:45] bit:1024, dataset:palmprint, training... loss:0.70070\n",
      "[CSQ][1094/2000][13:49:55] bit:1024, dataset:palmprint, training... loss:0.70024\n",
      "[CSQ][1095/2000][13:50:05] bit:1024, dataset:palmprint, training... loss:0.70055\n",
      "[CSQ][1096/2000][13:50:14] bit:1024, dataset:palmprint, training... loss:0.70076\n",
      "[CSQ][1097/2000][13:50:24] bit:1024, dataset:palmprint, training... loss:0.70060\n",
      "[CSQ][1098/2000][13:50:35] bit:1024, dataset:palmprint, training... loss:0.70027\n",
      "[CSQ][1099/2000][13:50:44] bit:1024, dataset:palmprint, training... loss:0.70024\n",
      "[CSQ][1100/2000][13:50:53] bit:1024, dataset:palmprint, training... loss:0.70069\n",
      "[CSQ][1101/2000][13:51:02] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.005333333333333333\n",
      "[CSQ] epoch:1101, bit:1024, dataset:palmprint,eer:0.00533, Best eer: 0.00467\n",
      "\b loss:0.70001\n",
      "[CSQ][1102/2000][13:52:17] bit:1024, dataset:palmprint, training... loss:0.70061\n",
      "[CSQ][1103/2000][13:52:26] bit:1024, dataset:palmprint, training... loss:0.70013\n",
      "[CSQ][1104/2000][13:52:35] bit:1024, dataset:palmprint, training... loss:0.69999\n",
      "[CSQ][1105/2000][13:52:45] bit:1024, dataset:palmprint, training... loss:0.70026\n",
      "[CSQ][1106/2000][13:53:00] bit:1024, dataset:palmprint, training... loss:0.70068\n",
      "[CSQ][1107/2000][13:53:08] bit:1024, dataset:palmprint, training... loss:0.70024\n",
      "[CSQ][1108/2000][13:53:17] bit:1024, dataset:palmprint, training... loss:0.70008\n",
      "[CSQ][1109/2000][13:53:26] bit:1024, dataset:palmprint, training... loss:0.70049\n",
      "[CSQ][1110/2000][13:53:34] bit:1024, dataset:palmprint, training... loss:0.69984\n",
      "[CSQ][1111/2000][13:53:43] bit:1024, dataset:palmprint, training... loss:0.69950\n",
      "[CSQ][1112/2000][13:53:52] bit:1024, dataset:palmprint, training... loss:0.69987\n",
      "[CSQ][1113/2000][13:54:00] bit:1024, dataset:palmprint, training... loss:0.69973\n",
      "[CSQ][1114/2000][13:54:09] bit:1024, dataset:palmprint, training... loss:0.70013\n",
      "[CSQ][1115/2000][13:54:17] bit:1024, dataset:palmprint, training... loss:0.70013\n",
      "[CSQ][1116/2000][13:54:26] bit:1024, dataset:palmprint, training... loss:0.69978\n",
      "[CSQ][1117/2000][13:54:34] bit:1024, dataset:palmprint, training... loss:0.70018\n",
      "[CSQ][1118/2000][13:54:43] bit:1024, dataset:palmprint, training... loss:0.70037\n",
      "[CSQ][1119/2000][13:54:51] bit:1024, dataset:palmprint, training... loss:0.70005\n",
      "[CSQ][1120/2000][13:55:00] bit:1024, dataset:palmprint, training... loss:0.69973\n",
      "[CSQ][1121/2000][13:55:10] bit:1024, dataset:palmprint, training... loss:0.70015\n",
      "[CSQ][1122/2000][13:55:20] bit:1024, dataset:palmprint, training... loss:0.70001\n",
      "[CSQ][1123/2000][13:55:29] bit:1024, dataset:palmprint, training... loss:0.69949\n",
      "[CSQ][1124/2000][13:55:38] bit:1024, dataset:palmprint, training... loss:0.70005\n",
      "[CSQ][1125/2000][13:55:49] bit:1024, dataset:palmprint, training... loss:0.69976\n",
      "[CSQ][1126/2000][13:55:58] bit:1024, dataset:palmprint, training... loss:0.69951\n",
      "[CSQ][1127/2000][13:56:07] bit:1024, dataset:palmprint, training... loss:0.70012\n",
      "[CSQ][1128/2000][13:56:17] bit:1024, dataset:palmprint, training... loss:0.70000\n",
      "[CSQ][1129/2000][13:56:27] bit:1024, dataset:palmprint, training... loss:0.69986\n",
      "[CSQ][1130/2000][13:56:36] bit:1024, dataset:palmprint, training... loss:0.70015\n",
      "[CSQ][1131/2000][13:56:45] bit:1024, dataset:palmprint, training... loss:0.69915\n",
      "[CSQ][1132/2000][13:56:56] bit:1024, dataset:palmprint, training... loss:0.69962\n",
      "[CSQ][1133/2000][13:57:05] bit:1024, dataset:palmprint, training... loss:0.69979\n",
      "[CSQ][1134/2000][13:57:14] bit:1024, dataset:palmprint, training... loss:0.69975\n",
      "[CSQ][1135/2000][13:57:24] bit:1024, dataset:palmprint, training... loss:0.69917\n",
      "[CSQ][1136/2000][13:57:34] bit:1024, dataset:palmprint, training... loss:0.69983\n",
      "[CSQ][1137/2000][13:57:43] bit:1024, dataset:palmprint, training... loss:0.69929\n",
      "[CSQ][1138/2000][13:57:52] bit:1024, dataset:palmprint, training... loss:0.69923\n",
      "[CSQ][1139/2000][13:58:03] bit:1024, dataset:palmprint, training... loss:0.69963\n",
      "[CSQ][1140/2000][13:58:12] bit:1024, dataset:palmprint, training... loss:0.69899\n",
      "[CSQ][1141/2000][13:58:21] bit:1024, dataset:palmprint, training... loss:0.69944\n",
      "[CSQ][1142/2000][13:58:31] bit:1024, dataset:palmprint, training... loss:0.69945\n",
      "[CSQ][1143/2000][13:58:42] bit:1024, dataset:palmprint, training... loss:0.69960\n",
      "[CSQ][1144/2000][13:58:51] bit:1024, dataset:palmprint, training... loss:0.69950\n",
      "[CSQ][1145/2000][13:59:00] bit:1024, dataset:palmprint, training... loss:0.69883\n",
      "[CSQ][1146/2000][13:59:10] bit:1024, dataset:palmprint, training... loss:0.69910\n",
      "[CSQ][1147/2000][13:59:19] bit:1024, dataset:palmprint, training... loss:0.69967\n",
      "[CSQ][1148/2000][13:59:29] bit:1024, dataset:palmprint, training... loss:0.69907\n",
      "[CSQ][1149/2000][13:59:38] bit:1024, dataset:palmprint, training... loss:0.69959\n",
      "[CSQ][1150/2000][13:59:48] bit:1024, dataset:palmprint, training... loss:0.69923\n",
      "[CSQ][1151/2000][13:59:58] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.004666666666666667\n",
      "[CSQ] epoch:1151, bit:1024, dataset:palmprint,eer:0.00467, Best eer: 0.00467\n",
      "\b loss:0.69911\n",
      "[CSQ][1152/2000][14:01:16] bit:1024, dataset:palmprint, training... loss:0.69886\n",
      "[CSQ][1153/2000][14:01:25] bit:1024, dataset:palmprint, training... loss:0.69924\n",
      "[CSQ][1154/2000][14:01:34] bit:1024, dataset:palmprint, training... loss:0.69859\n",
      "[CSQ][1155/2000][14:01:44] bit:1024, dataset:palmprint, training... loss:0.69935\n",
      "[CSQ][1156/2000][14:01:54] bit:1024, dataset:palmprint, training... loss:0.69893\n",
      "[CSQ][1157/2000][14:02:03] bit:1024, dataset:palmprint, training... loss:0.69911\n",
      "[CSQ][1158/2000][14:02:13] bit:1024, dataset:palmprint, training... loss:0.69925\n",
      "[CSQ][1159/2000][14:02:23] bit:1024, dataset:palmprint, training... loss:0.69939\n",
      "[CSQ][1160/2000][14:02:31] bit:1024, dataset:palmprint, training... loss:0.69900\n",
      "[CSQ][1161/2000][14:02:41] bit:1024, dataset:palmprint, training... loss:0.69887\n",
      "[CSQ][1162/2000][14:02:51] bit:1024, dataset:palmprint, training... loss:0.69883\n",
      "[CSQ][1163/2000][14:03:01] bit:1024, dataset:palmprint, training... loss:0.69889\n",
      "[CSQ][1164/2000][14:03:10] bit:1024, dataset:palmprint, training... loss:0.69851\n",
      "[CSQ][1165/2000][14:03:20] bit:1024, dataset:palmprint, training... loss:0.69893\n",
      "[CSQ][1166/2000][14:03:29] bit:1024, dataset:palmprint, training... loss:0.69865\n",
      "[CSQ][1167/2000][14:03:38] bit:1024, dataset:palmprint, training... loss:0.69895\n",
      "[CSQ][1168/2000][14:03:49] bit:1024, dataset:palmprint, training... loss:0.69916\n",
      "[CSQ][1169/2000][14:03:58] bit:1024, dataset:palmprint, training... loss:0.69885\n",
      "[CSQ][1170/2000][14:04:06] bit:1024, dataset:palmprint, training... loss:0.69922\n",
      "[CSQ][1171/2000][14:04:16] bit:1024, dataset:palmprint, training... loss:0.69835\n",
      "[CSQ][1172/2000][14:04:26] bit:1024, dataset:palmprint, training... loss:0.69838\n",
      "[CSQ][1173/2000][14:04:36] bit:1024, dataset:palmprint, training... loss:0.69874\n",
      "[CSQ][1174/2000][14:04:45] bit:1024, dataset:palmprint, training... loss:0.69919\n",
      "[CSQ][1175/2000][14:04:55] bit:1024, dataset:palmprint, training... loss:0.69897\n",
      "[CSQ][1176/2000][14:05:03] bit:1024, dataset:palmprint, training... loss:0.69888\n",
      "[CSQ][1177/2000][14:05:12] bit:1024, dataset:palmprint, training... loss:0.69868\n",
      "[CSQ][1178/2000][14:05:22] bit:1024, dataset:palmprint, training... loss:0.69842\n",
      "[CSQ][1179/2000][14:05:33] bit:1024, dataset:palmprint, training... loss:0.69870\n",
      "[CSQ][1180/2000][14:05:42] bit:1024, dataset:palmprint, training... loss:0.69833\n",
      "[CSQ][1181/2000][14:05:52] bit:1024, dataset:palmprint, training... loss:0.69833\n",
      "[CSQ][1182/2000][14:06:04] bit:1024, dataset:palmprint, training... loss:0.69837\n",
      "[CSQ][1183/2000][14:06:12] bit:1024, dataset:palmprint, training... loss:0.69834\n",
      "[CSQ][1184/2000][14:06:21] bit:1024, dataset:palmprint, training... loss:0.69842\n",
      "[CSQ][1185/2000][14:06:30] bit:1024, dataset:palmprint, training... loss:0.69814\n",
      "[CSQ][1186/2000][14:06:39] bit:1024, dataset:palmprint, training... loss:0.69872\n",
      "[CSQ][1187/2000][14:06:47] bit:1024, dataset:palmprint, training... loss:0.69884\n",
      "[CSQ][1188/2000][14:06:55] bit:1024, dataset:palmprint, training... loss:0.69890\n",
      "[CSQ][1189/2000][14:07:06] bit:1024, dataset:palmprint, training... loss:0.69830\n",
      "[CSQ][1190/2000][14:07:15] bit:1024, dataset:palmprint, training... loss:0.69840\n",
      "[CSQ][1191/2000][14:07:25] bit:1024, dataset:palmprint, training... loss:0.69834\n",
      "[CSQ][1192/2000][14:07:34] bit:1024, dataset:palmprint, training... loss:0.69816\n",
      "[CSQ][1193/2000][14:07:45] bit:1024, dataset:palmprint, training... loss:0.69818\n",
      "[CSQ][1194/2000][14:07:54] bit:1024, dataset:palmprint, training... loss:0.69871\n",
      "[CSQ][1195/2000][14:08:03] bit:1024, dataset:palmprint, training... loss:0.69852\n",
      "[CSQ][1196/2000][14:08:13] bit:1024, dataset:palmprint, training... loss:0.69829\n",
      "[CSQ][1197/2000][14:08:22] bit:1024, dataset:palmprint, training... loss:0.69843\n",
      "[CSQ][1198/2000][14:08:32] bit:1024, dataset:palmprint, training... loss:0.69793\n",
      "[CSQ][1199/2000][14:08:41] bit:1024, dataset:palmprint, training... loss:0.69785\n",
      "[CSQ][1200/2000][14:08:52] bit:1024, dataset:palmprint, training... loss:0.69855\n",
      "[CSQ][1201/2000][14:09:01] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.004333333333333333\n",
      "[CSQ] epoch:1201, bit:1024, dataset:palmprint,eer:0.00433, Best eer: 0.00433\n",
      "\b loss:0.69843\n",
      "[CSQ][1202/2000][14:10:42] bit:1024, dataset:palmprint, training... loss:0.69840\n",
      "[CSQ][1203/2000][14:10:51] bit:1024, dataset:palmprint, training... loss:0.69792\n",
      "[CSQ][1204/2000][14:11:02] bit:1024, dataset:palmprint, training... loss:0.69848\n",
      "[CSQ][1205/2000][14:11:11] bit:1024, dataset:palmprint, training... loss:0.69847\n",
      "[CSQ][1206/2000][14:11:20] bit:1024, dataset:palmprint, training... loss:0.69764\n",
      "[CSQ][1207/2000][14:11:30] bit:1024, dataset:palmprint, training... loss:0.69787\n",
      "[CSQ][1208/2000][14:11:40] bit:1024, dataset:palmprint, training... loss:0.69822\n",
      "[CSQ][1209/2000][14:11:50] bit:1024, dataset:palmprint, training... loss:0.69789\n",
      "[CSQ][1210/2000][14:11:58] bit:1024, dataset:palmprint, training... loss:0.69785\n",
      "[CSQ][1211/2000][14:12:09] bit:1024, dataset:palmprint, training... loss:0.69796\n",
      "[CSQ][1212/2000][14:12:18] bit:1024, dataset:palmprint, training... loss:0.69773\n",
      "[CSQ][1213/2000][14:12:27] bit:1024, dataset:palmprint, training... loss:0.69793\n",
      "[CSQ][1214/2000][14:12:36] bit:1024, dataset:palmprint, training... loss:0.69801\n",
      "[CSQ][1215/2000][14:12:46] bit:1024, dataset:palmprint, training... loss:0.69814\n",
      "[CSQ][1216/2000][14:12:55] bit:1024, dataset:palmprint, training... loss:0.69827\n",
      "[CSQ][1217/2000][14:13:04] bit:1024, dataset:palmprint, training... loss:0.69712\n",
      "[CSQ][1218/2000][14:13:14] bit:1024, dataset:palmprint, training... loss:0.69769\n",
      "[CSQ][1219/2000][14:13:23] bit:1024, dataset:palmprint, training... loss:0.69704\n",
      "[CSQ][1220/2000][14:13:32] bit:1024, dataset:palmprint, training... loss:0.69761\n",
      "[CSQ][1221/2000][14:13:41] bit:1024, dataset:palmprint, training... loss:0.69765\n",
      "[CSQ][1222/2000][14:13:51] bit:1024, dataset:palmprint, training... loss:0.69779\n",
      "[CSQ][1223/2000][14:14:00] bit:1024, dataset:palmprint, training... loss:0.69775\n",
      "[CSQ][1224/2000][14:14:09] bit:1024, dataset:palmprint, training... loss:0.69745\n",
      "[CSQ][1225/2000][14:14:19] bit:1024, dataset:palmprint, training... loss:0.69744\n",
      "[CSQ][1226/2000][14:14:28] bit:1024, dataset:palmprint, training... loss:0.69750\n",
      "[CSQ][1227/2000][14:14:37] bit:1024, dataset:palmprint, training... loss:0.69773\n",
      "[CSQ][1228/2000][14:14:47] bit:1024, dataset:palmprint, training... loss:0.69737\n",
      "[CSQ][1229/2000][14:14:56] bit:1024, dataset:palmprint, training... loss:0.69727\n",
      "[CSQ][1230/2000][14:15:05] bit:1024, dataset:palmprint, training... loss:0.69756\n",
      "[CSQ][1231/2000][14:15:15] bit:1024, dataset:palmprint, training... loss:0.69716\n",
      "[CSQ][1232/2000][14:15:24] bit:1024, dataset:palmprint, training... loss:0.69732\n",
      "[CSQ][1233/2000][14:15:33] bit:1024, dataset:palmprint, training... loss:0.69806\n",
      "[CSQ][1234/2000][14:15:42] bit:1024, dataset:palmprint, training... loss:0.69699\n",
      "[CSQ][1235/2000][14:15:52] bit:1024, dataset:palmprint, training... loss:0.69785\n",
      "[CSQ][1236/2000][14:16:01] bit:1024, dataset:palmprint, training... loss:0.69720\n",
      "[CSQ][1237/2000][14:16:10] bit:1024, dataset:palmprint, training... loss:0.69738\n",
      "[CSQ][1238/2000][14:16:19] bit:1024, dataset:palmprint, training... loss:0.69749\n",
      "[CSQ][1239/2000][14:16:28] bit:1024, dataset:palmprint, training... loss:0.69724\n",
      "[CSQ][1240/2000][14:16:37] bit:1024, dataset:palmprint, training... loss:0.69734\n",
      "[CSQ][1241/2000][14:16:47] bit:1024, dataset:palmprint, training... loss:0.69730\n",
      "[CSQ][1242/2000][14:16:57] bit:1024, dataset:palmprint, training... loss:0.69717\n",
      "[CSQ][1243/2000][14:17:06] bit:1024, dataset:palmprint, training... loss:0.69723\n",
      "[CSQ][1244/2000][14:17:15] bit:1024, dataset:palmprint, training... loss:0.69738\n",
      "[CSQ][1245/2000][14:17:25] bit:1024, dataset:palmprint, training... loss:0.69697\n",
      "[CSQ][1246/2000][14:17:34] bit:1024, dataset:palmprint, training... loss:0.69702\n",
      "[CSQ][1247/2000][14:17:43] bit:1024, dataset:palmprint, training... loss:0.69711\n",
      "[CSQ][1248/2000][14:17:53] bit:1024, dataset:palmprint, training... loss:0.69678\n",
      "[CSQ][1249/2000][14:18:06] bit:1024, dataset:palmprint, training... loss:0.69721\n",
      "[CSQ][1250/2000][14:18:14] bit:1024, dataset:palmprint, training... loss:0.69697\n",
      "[CSQ][1251/2000][14:18:23] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.005333333333333333\n",
      "[CSQ] epoch:1251, bit:1024, dataset:palmprint,eer:0.00533, Best eer: 0.00433\n",
      "\b loss:0.69761\n",
      "[CSQ][1252/2000][14:19:28] bit:1024, dataset:palmprint, training... loss:0.69711\n",
      "[CSQ][1253/2000][14:19:37] bit:1024, dataset:palmprint, training... loss:0.69699\n",
      "[CSQ][1254/2000][14:19:46] bit:1024, dataset:palmprint, training... loss:0.69685\n",
      "[CSQ][1255/2000][14:19:56] bit:1024, dataset:palmprint, training... loss:0.69680\n",
      "[CSQ][1256/2000][14:20:06] bit:1024, dataset:palmprint, training... loss:0.69690\n",
      "[CSQ][1257/2000][14:20:15] bit:1024, dataset:palmprint, training... loss:0.69663\n",
      "[CSQ][1258/2000][14:20:24] bit:1024, dataset:palmprint, training... loss:0.69720\n",
      "[CSQ][1259/2000][14:20:33] bit:1024, dataset:palmprint, training... loss:0.69667\n",
      "[CSQ][1260/2000][14:20:44] bit:1024, dataset:palmprint, training... loss:0.69662\n",
      "[CSQ][1261/2000][14:20:53] bit:1024, dataset:palmprint, training... loss:0.69677\n",
      "[CSQ][1262/2000][14:21:02] bit:1024, dataset:palmprint, training... loss:0.69705\n",
      "[CSQ][1263/2000][14:21:12] bit:1024, dataset:palmprint, training... loss:0.69757\n",
      "[CSQ][1264/2000][14:21:22] bit:1024, dataset:palmprint, training... loss:0.69693\n",
      "[CSQ][1265/2000][14:21:31] bit:1024, dataset:palmprint, training... loss:0.69691\n",
      "[CSQ][1266/2000][14:21:40] bit:1024, dataset:palmprint, training... loss:0.69680\n",
      "[CSQ][1267/2000][14:21:51] bit:1024, dataset:palmprint, training... loss:0.69636\n",
      "[CSQ][1268/2000][14:22:00] bit:1024, dataset:palmprint, training... loss:0.69690\n",
      "[CSQ][1269/2000][14:22:09] bit:1024, dataset:palmprint, training... loss:0.69657\n",
      "[CSQ][1270/2000][14:22:19] bit:1024, dataset:palmprint, training... loss:0.69631\n",
      "[CSQ][1271/2000][14:22:29] bit:1024, dataset:palmprint, training... loss:0.69673\n",
      "[CSQ][1272/2000][14:22:38] bit:1024, dataset:palmprint, training... loss:0.69653\n",
      "[CSQ][1273/2000][14:22:47] bit:1024, dataset:palmprint, training... loss:0.69685\n",
      "[CSQ][1274/2000][14:22:58] bit:1024, dataset:palmprint, training... loss:0.69629\n",
      "[CSQ][1275/2000][14:23:06] bit:1024, dataset:palmprint, training... loss:0.69599\n",
      "[CSQ][1276/2000][14:23:15] bit:1024, dataset:palmprint, training... loss:0.69650\n",
      "[CSQ][1277/2000][14:23:25] bit:1024, dataset:palmprint, training... loss:0.69674\n",
      "[CSQ][1278/2000][14:23:35] bit:1024, dataset:palmprint, training... loss:0.69660\n",
      "[CSQ][1279/2000][14:23:45] bit:1024, dataset:palmprint, training... loss:0.69693\n",
      "[CSQ][1280/2000][14:23:54] bit:1024, dataset:palmprint, training... loss:0.69619\n",
      "[CSQ][1281/2000][14:24:05] bit:1024, dataset:palmprint, training... loss:0.69634\n",
      "[CSQ][1282/2000][14:24:14] bit:1024, dataset:palmprint, training... loss:0.69635\n",
      "[CSQ][1283/2000][14:24:23] bit:1024, dataset:palmprint, training... loss:0.69662\n",
      "[CSQ][1284/2000][14:24:32] bit:1024, dataset:palmprint, training... loss:0.69656\n",
      "[CSQ][1285/2000][14:24:42] bit:1024, dataset:palmprint, training... loss:0.69632\n",
      "[CSQ][1286/2000][14:24:51] bit:1024, dataset:palmprint, training... loss:0.69688\n",
      "[CSQ][1287/2000][14:25:00] bit:1024, dataset:palmprint, training... loss:0.69643\n",
      "[CSQ][1288/2000][14:25:11] bit:1024, dataset:palmprint, training... loss:0.69639\n",
      "[CSQ][1289/2000][14:25:20] bit:1024, dataset:palmprint, training... loss:0.69580\n",
      "[CSQ][1290/2000][14:25:29] bit:1024, dataset:palmprint, training... loss:0.69618\n",
      "[CSQ][1291/2000][14:25:38] bit:1024, dataset:palmprint, training... loss:0.69621\n",
      "[CSQ][1292/2000][14:25:48] bit:1024, dataset:palmprint, training... loss:0.69586\n",
      "[CSQ][1293/2000][14:25:57] bit:1024, dataset:palmprint, training... loss:0.69608\n",
      "[CSQ][1294/2000][14:26:06] bit:1024, dataset:palmprint, training... loss:0.69649\n",
      "[CSQ][1295/2000][14:26:17] bit:1024, dataset:palmprint, training... loss:0.69592\n",
      "[CSQ][1296/2000][14:26:27] bit:1024, dataset:palmprint, training... loss:0.69624\n",
      "[CSQ][1297/2000][14:26:36] bit:1024, dataset:palmprint, training... loss:0.69588\n",
      "[CSQ][1298/2000][14:26:46] bit:1024, dataset:palmprint, training... loss:0.69631\n",
      "[CSQ][1299/2000][14:26:57] bit:1024, dataset:palmprint, training... loss:0.69642\n",
      "[CSQ][1300/2000][14:27:07] bit:1024, dataset:palmprint, training... loss:0.69602\n",
      "[CSQ][1301/2000][14:27:16] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.004666666666666667\n",
      "[CSQ] epoch:1301, bit:1024, dataset:palmprint,eer:0.00467, Best eer: 0.00433\n",
      "\b loss:0.69688\n",
      "[CSQ][1302/2000][14:28:35] bit:1024, dataset:palmprint, training... loss:0.69604\n",
      "[CSQ][1303/2000][14:28:44] bit:1024, dataset:palmprint, training... loss:0.69543\n",
      "[CSQ][1304/2000][14:28:56] bit:1024, dataset:palmprint, training... loss:0.69621\n",
      "[CSQ][1305/2000][14:29:05] bit:1024, dataset:palmprint, training... loss:0.69575\n",
      "[CSQ][1306/2000][14:29:15] bit:1024, dataset:palmprint, training... loss:0.69599\n",
      "[CSQ][1307/2000][14:29:24] bit:1024, dataset:palmprint, training... loss:0.69631\n",
      "[CSQ][1308/2000][14:29:35] bit:1024, dataset:palmprint, training... loss:0.69585\n",
      "[CSQ][1309/2000][14:29:44] bit:1024, dataset:palmprint, training... loss:0.69568\n",
      "[CSQ][1310/2000][14:29:54] bit:1024, dataset:palmprint, training... loss:0.69599\n",
      "[CSQ][1311/2000][14:30:03] bit:1024, dataset:palmprint, training... loss:0.69572\n",
      "[CSQ][1312/2000][14:30:14] bit:1024, dataset:palmprint, training... loss:0.69638\n",
      "[CSQ][1313/2000][14:30:24] bit:1024, dataset:palmprint, training... loss:0.69574\n",
      "[CSQ][1314/2000][14:30:33] bit:1024, dataset:palmprint, training... loss:0.69581\n",
      "[CSQ][1315/2000][14:30:46] bit:1024, dataset:palmprint, training... loss:0.69557\n",
      "[CSQ][1316/2000][14:30:54] bit:1024, dataset:palmprint, training... loss:0.69520\n",
      "[CSQ][1317/2000][14:31:03] bit:1024, dataset:palmprint, training... loss:0.69603\n",
      "[CSQ][1318/2000][14:31:12] bit:1024, dataset:palmprint, training... loss:0.69588\n",
      "[CSQ][1319/2000][14:31:21] bit:1024, dataset:palmprint, training... loss:0.69566\n",
      "[CSQ][1320/2000][14:31:30] bit:1024, dataset:palmprint, training... loss:0.69569\n",
      "[CSQ][1321/2000][14:31:38] bit:1024, dataset:palmprint, training... loss:0.69594\n",
      "[CSQ][1322/2000][14:31:47] bit:1024, dataset:palmprint, training... loss:0.69549\n",
      "[CSQ][1323/2000][14:31:56] bit:1024, dataset:palmprint, training... loss:0.69588\n",
      "[CSQ][1324/2000][14:32:04] bit:1024, dataset:palmprint, training... loss:0.69562\n",
      "[CSQ][1325/2000][14:32:12] bit:1024, dataset:palmprint, training... loss:0.69557\n",
      "[CSQ][1326/2000][14:32:21] bit:1024, dataset:palmprint, training... loss:0.69557\n",
      "[CSQ][1327/2000][14:32:30] bit:1024, dataset:palmprint, training... loss:0.69602\n",
      "[CSQ][1328/2000][14:32:38] bit:1024, dataset:palmprint, training... loss:0.69565\n",
      "[CSQ][1329/2000][14:32:48] bit:1024, dataset:palmprint, training... loss:0.69561\n",
      "[CSQ][1330/2000][14:32:59] bit:1024, dataset:palmprint, training... loss:0.69587\n",
      "[CSQ][1331/2000][14:33:09] bit:1024, dataset:palmprint, training... loss:0.69544\n",
      "[CSQ][1332/2000][14:33:17] bit:1024, dataset:palmprint, training... loss:0.69558\n",
      "[CSQ][1333/2000][14:33:27] bit:1024, dataset:palmprint, training... loss:0.69563\n",
      "[CSQ][1334/2000][14:33:37] bit:1024, dataset:palmprint, training... loss:0.69535\n",
      "[CSQ][1335/2000][14:33:46] bit:1024, dataset:palmprint, training... loss:0.69560\n",
      "[CSQ][1336/2000][14:33:55] bit:1024, dataset:palmprint, training... loss:0.69544\n",
      "[CSQ][1337/2000][14:34:06] bit:1024, dataset:palmprint, training... loss:0.69526\n",
      "[CSQ][1338/2000][14:34:15] bit:1024, dataset:palmprint, training... loss:0.69540\n",
      "[CSQ][1339/2000][14:34:24] bit:1024, dataset:palmprint, training... loss:0.69564\n",
      "[CSQ][1340/2000][14:34:34] bit:1024, dataset:palmprint, training... loss:0.69507\n",
      "[CSQ][1341/2000][14:34:44] bit:1024, dataset:palmprint, training... loss:0.69565\n",
      "[CSQ][1342/2000][14:34:53] bit:1024, dataset:palmprint, training... loss:0.69577\n",
      "[CSQ][1343/2000][14:35:02] bit:1024, dataset:palmprint, training... loss:0.69550\n",
      "[CSQ][1344/2000][14:35:13] bit:1024, dataset:palmprint, training... loss:0.69560\n",
      "[CSQ][1345/2000][14:35:22] bit:1024, dataset:palmprint, training... loss:0.69466\n",
      "[CSQ][1346/2000][14:35:31] bit:1024, dataset:palmprint, training... loss:0.69510\n",
      "[CSQ][1347/2000][14:35:41] bit:1024, dataset:palmprint, training... loss:0.69497\n",
      "[CSQ][1348/2000][14:35:51] bit:1024, dataset:palmprint, training... loss:0.69510\n",
      "[CSQ][1349/2000][14:36:00] bit:1024, dataset:palmprint, training... loss:0.69520\n",
      "[CSQ][1350/2000][14:36:09] bit:1024, dataset:palmprint, training... loss:0.69518\n",
      "[CSQ][1351/2000][14:36:19] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0036666666666666666\n",
      "[CSQ] epoch:1351, bit:1024, dataset:palmprint,eer:0.00367, Best eer: 0.00367\n",
      "\b loss:0.69540\n",
      "[CSQ][1352/2000][14:37:34] bit:1024, dataset:palmprint, training... loss:0.69548\n",
      "[CSQ][1353/2000][14:37:43] bit:1024, dataset:palmprint, training... loss:0.69531\n",
      "[CSQ][1354/2000][14:37:52] bit:1024, dataset:palmprint, training... loss:0.69535\n",
      "[CSQ][1355/2000][14:38:02] bit:1024, dataset:palmprint, training... loss:0.69513\n",
      "[CSQ][1356/2000][14:38:11] bit:1024, dataset:palmprint, training... loss:0.69493\n",
      "[CSQ][1357/2000][14:38:20] bit:1024, dataset:palmprint, training... loss:0.69523\n",
      "[CSQ][1358/2000][14:38:29] bit:1024, dataset:palmprint, training... loss:0.69498\n",
      "[CSQ][1359/2000][14:38:40] bit:1024, dataset:palmprint, training... loss:0.69514\n",
      "[CSQ][1360/2000][14:38:50] bit:1024, dataset:palmprint, training... loss:0.69497\n",
      "[CSQ][1361/2000][14:38:58] bit:1024, dataset:palmprint, training... loss:0.69478\n",
      "[CSQ][1362/2000][14:39:09] bit:1024, dataset:palmprint, training... loss:0.69533\n",
      "[CSQ][1363/2000][14:39:18] bit:1024, dataset:palmprint, training... loss:0.69494\n",
      "[CSQ][1364/2000][14:39:27] bit:1024, dataset:palmprint, training... loss:0.69494\n",
      "[CSQ][1365/2000][14:39:36] bit:1024, dataset:palmprint, training... loss:0.69500\n",
      "[CSQ][1366/2000][14:39:47] bit:1024, dataset:palmprint, training... loss:0.69502\n",
      "[CSQ][1367/2000][14:39:57] bit:1024, dataset:palmprint, training... loss:0.69485\n",
      "[CSQ][1368/2000][14:40:05] bit:1024, dataset:palmprint, training... loss:0.69451\n",
      "[CSQ][1369/2000][14:40:15] bit:1024, dataset:palmprint, training... loss:0.69579\n",
      "[CSQ][1370/2000][14:40:25] bit:1024, dataset:palmprint, training... loss:0.69497\n",
      "[CSQ][1371/2000][14:40:34] bit:1024, dataset:palmprint, training... loss:0.69438\n",
      "[CSQ][1372/2000][14:40:43] bit:1024, dataset:palmprint, training... loss:0.69524\n",
      "[CSQ][1373/2000][14:40:54] bit:1024, dataset:palmprint, training... loss:0.69497\n",
      "[CSQ][1374/2000][14:41:03] bit:1024, dataset:palmprint, training... loss:0.69506\n",
      "[CSQ][1375/2000][14:41:12] bit:1024, dataset:palmprint, training... loss:0.69500\n",
      "[CSQ][1376/2000][14:41:22] bit:1024, dataset:palmprint, training... loss:0.69489\n",
      "[CSQ][1377/2000][14:41:31] bit:1024, dataset:palmprint, training... loss:0.69509\n",
      "[CSQ][1378/2000][14:41:41] bit:1024, dataset:palmprint, training... loss:0.69482\n",
      "[CSQ][1379/2000][14:41:50] bit:1024, dataset:palmprint, training... loss:0.69486\n",
      "[CSQ][1380/2000][14:42:00] bit:1024, dataset:palmprint, training... loss:0.69447\n",
      "[CSQ][1381/2000][14:42:10] bit:1024, dataset:palmprint, training... loss:0.69401\n",
      "[CSQ][1382/2000][14:42:19] bit:1024, dataset:palmprint, training... loss:0.69431\n",
      "[CSQ][1383/2000][14:42:29] bit:1024, dataset:palmprint, training... loss:0.69469\n",
      "[CSQ][1384/2000][14:42:39] bit:1024, dataset:palmprint, training... loss:0.69466\n",
      "[CSQ][1385/2000][14:42:48] bit:1024, dataset:palmprint, training... loss:0.69459\n",
      "[CSQ][1386/2000][14:42:57] bit:1024, dataset:palmprint, training... loss:0.69486\n",
      "[CSQ][1387/2000][14:43:07] bit:1024, dataset:palmprint, training... loss:0.69480\n",
      "[CSQ][1388/2000][14:43:17] bit:1024, dataset:palmprint, training... loss:0.69421\n",
      "[CSQ][1389/2000][14:43:27] bit:1024, dataset:palmprint, training... loss:0.69460\n",
      "[CSQ][1390/2000][14:43:38] bit:1024, dataset:palmprint, training... loss:0.69457\n",
      "[CSQ][1391/2000][14:43:47] bit:1024, dataset:palmprint, training... loss:0.69445\n",
      "[CSQ][1392/2000][14:43:55] bit:1024, dataset:palmprint, training... loss:0.69459\n",
      "[CSQ][1393/2000][14:44:03] bit:1024, dataset:palmprint, training... loss:0.69493\n",
      "[CSQ][1394/2000][14:44:12] bit:1024, dataset:palmprint, training... loss:0.69450\n",
      "[CSQ][1395/2000][14:44:21] bit:1024, dataset:palmprint, training... loss:0.69496\n",
      "[CSQ][1396/2000][14:44:29] bit:1024, dataset:palmprint, training... loss:0.69437\n",
      "[CSQ][1397/2000][14:44:40] bit:1024, dataset:palmprint, training... loss:0.69452\n",
      "[CSQ][1398/2000][14:44:50] bit:1024, dataset:palmprint, training... loss:0.69462\n",
      "[CSQ][1399/2000][14:44:59] bit:1024, dataset:palmprint, training... loss:0.69452\n",
      "[CSQ][1400/2000][14:45:08] bit:1024, dataset:palmprint, training... loss:0.69450\n",
      "[CSQ][1401/2000][14:45:18] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.004333333333333333\n",
      "[CSQ] epoch:1401, bit:1024, dataset:palmprint,eer:0.00433, Best eer: 0.00367\n",
      "\b loss:0.69468\n",
      "[CSQ][1402/2000][14:46:32] bit:1024, dataset:palmprint, training... loss:0.69483\n",
      "[CSQ][1403/2000][14:46:41] bit:1024, dataset:palmprint, training... loss:0.69403\n",
      "[CSQ][1404/2000][14:46:50] bit:1024, dataset:palmprint, training... loss:0.69423\n",
      "[CSQ][1405/2000][14:47:01] bit:1024, dataset:palmprint, training... loss:0.69400\n",
      "[CSQ][1406/2000][14:47:10] bit:1024, dataset:palmprint, training... loss:0.69451\n",
      "[CSQ][1407/2000][14:47:19] bit:1024, dataset:palmprint, training... loss:0.69449\n",
      "[CSQ][1408/2000][14:47:30] bit:1024, dataset:palmprint, training... loss:0.69421\n",
      "[CSQ][1409/2000][14:47:40] bit:1024, dataset:palmprint, training... loss:0.69472\n",
      "[CSQ][1410/2000][14:47:50] bit:1024, dataset:palmprint, training... loss:0.69390\n",
      "[CSQ][1411/2000][14:47:58] bit:1024, dataset:palmprint, training... loss:0.69454\n",
      "[CSQ][1412/2000][14:48:09] bit:1024, dataset:palmprint, training... loss:0.69411\n",
      "[CSQ][1413/2000][14:48:18] bit:1024, dataset:palmprint, training... loss:0.69386\n",
      "[CSQ][1414/2000][14:48:27] bit:1024, dataset:palmprint, training... loss:0.69439\n",
      "[CSQ][1415/2000][14:48:36] bit:1024, dataset:palmprint, training... loss:0.69420\n",
      "[CSQ][1416/2000][14:48:46] bit:1024, dataset:palmprint, training... loss:0.69410\n",
      "[CSQ][1417/2000][14:48:55] bit:1024, dataset:palmprint, training... loss:0.69401\n",
      "[CSQ][1418/2000][14:49:04] bit:1024, dataset:palmprint, training... loss:0.69378\n",
      "[CSQ][1419/2000][14:49:15] bit:1024, dataset:palmprint, training... loss:0.69419\n",
      "[CSQ][1420/2000][14:49:24] bit:1024, dataset:palmprint, training... loss:0.69373\n",
      "[CSQ][1421/2000][14:49:33] bit:1024, dataset:palmprint, training... loss:0.69371\n",
      "[CSQ][1422/2000][14:49:43] bit:1024, dataset:palmprint, training... loss:0.69380\n",
      "[CSQ][1423/2000][14:49:54] bit:1024, dataset:palmprint, training... loss:0.69348\n",
      "[CSQ][1424/2000][14:50:03] bit:1024, dataset:palmprint, training... loss:0.69401\n",
      "[CSQ][1425/2000][14:50:12] bit:1024, dataset:palmprint, training... loss:0.69388\n",
      "[CSQ][1426/2000][14:50:22] bit:1024, dataset:palmprint, training... loss:0.69370\n",
      "[CSQ][1427/2000][14:50:32] bit:1024, dataset:palmprint, training... loss:0.69404\n",
      "[CSQ][1428/2000][14:50:42] bit:1024, dataset:palmprint, training... loss:0.69393\n",
      "[CSQ][1429/2000][14:50:51] bit:1024, dataset:palmprint, training... loss:0.69398\n",
      "[CSQ][1430/2000][14:51:01] bit:1024, dataset:palmprint, training... loss:0.69377\n",
      "[CSQ][1431/2000][14:51:10] bit:1024, dataset:palmprint, training... loss:0.69370\n",
      "[CSQ][1432/2000][14:51:19] bit:1024, dataset:palmprint, training... loss:0.69386\n",
      "[CSQ][1433/2000][14:51:29] bit:1024, dataset:palmprint, training... loss:0.69400\n",
      "[CSQ][1434/2000][14:51:38] bit:1024, dataset:palmprint, training... loss:0.69354\n",
      "[CSQ][1435/2000][14:51:47] bit:1024, dataset:palmprint, training... loss:0.69381\n",
      "[CSQ][1436/2000][14:51:57] bit:1024, dataset:palmprint, training... loss:0.69418\n",
      "[CSQ][1437/2000][14:52:07] bit:1024, dataset:palmprint, training... loss:0.69399\n",
      "[CSQ][1438/2000][14:52:16] bit:1024, dataset:palmprint, training... loss:0.69394\n",
      "[CSQ][1439/2000][14:52:25] bit:1024, dataset:palmprint, training... loss:0.69384\n",
      "[CSQ][1440/2000][14:52:36] bit:1024, dataset:palmprint, training... loss:0.69392\n",
      "[CSQ][1441/2000][14:52:46] bit:1024, dataset:palmprint, training... loss:0.69345\n",
      "[CSQ][1442/2000][14:52:56] bit:1024, dataset:palmprint, training... loss:0.69378\n",
      "[CSQ][1443/2000][14:53:04] bit:1024, dataset:palmprint, training... loss:0.69397\n",
      "[CSQ][1444/2000][14:53:15] bit:1024, dataset:palmprint, training... loss:0.69337\n",
      "[CSQ][1445/2000][14:53:24] bit:1024, dataset:palmprint, training... loss:0.69325\n",
      "[CSQ][1446/2000][14:53:33] bit:1024, dataset:palmprint, training... loss:0.69344\n",
      "[CSQ][1447/2000][14:53:42] bit:1024, dataset:palmprint, training... loss:0.69354\n",
      "[CSQ][1448/2000][14:53:52] bit:1024, dataset:palmprint, training... loss:0.69360\n",
      "[CSQ][1449/2000][14:54:02] bit:1024, dataset:palmprint, training... loss:0.69375\n",
      "[CSQ][1450/2000][14:54:11] bit:1024, dataset:palmprint, training... loss:0.69306\n",
      "[CSQ][1451/2000][14:54:22] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0023333333333333335\n",
      "[CSQ] epoch:1451, bit:1024, dataset:palmprint,eer:0.00233, Best eer: 0.00233\n",
      "\b loss:0.69388\n",
      "[CSQ][1452/2000][14:55:36] bit:1024, dataset:palmprint, training... loss:0.69319\n",
      "[CSQ][1453/2000][14:55:45] bit:1024, dataset:palmprint, training... loss:0.69345\n",
      "[CSQ][1454/2000][14:55:53] bit:1024, dataset:palmprint, training... loss:0.69379\n",
      "[CSQ][1455/2000][14:56:02] bit:1024, dataset:palmprint, training... loss:0.69382\n",
      "[CSQ][1456/2000][14:56:10] bit:1024, dataset:palmprint, training... loss:0.69383\n",
      "[CSQ][1457/2000][14:56:19] bit:1024, dataset:palmprint, training... loss:0.69334\n",
      "[CSQ][1458/2000][14:56:27] bit:1024, dataset:palmprint, training... loss:0.69352\n",
      "[CSQ][1459/2000][14:56:36] bit:1024, dataset:palmprint, training... loss:0.69328\n",
      "[CSQ][1460/2000][14:56:46] bit:1024, dataset:palmprint, training... loss:0.69367\n",
      "[CSQ][1461/2000][14:56:56] bit:1024, dataset:palmprint, training... loss:0.69354\n",
      "[CSQ][1462/2000][14:57:05] bit:1024, dataset:palmprint, training... loss:0.69343\n",
      "[CSQ][1463/2000][14:57:13] bit:1024, dataset:palmprint, training... loss:0.69356\n",
      "[CSQ][1464/2000][14:57:24] bit:1024, dataset:palmprint, training... loss:0.69297\n",
      "[CSQ][1465/2000][14:57:34] bit:1024, dataset:palmprint, training... loss:0.69319\n",
      "[CSQ][1466/2000][14:57:42] bit:1024, dataset:palmprint, training... loss:0.69379\n",
      "[CSQ][1467/2000][14:57:53] bit:1024, dataset:palmprint, training... loss:0.69297\n",
      "[CSQ][1468/2000][14:58:03] bit:1024, dataset:palmprint, training... loss:0.69298\n",
      "[CSQ][1469/2000][14:58:12] bit:1024, dataset:palmprint, training... loss:0.69317\n",
      "[CSQ][1470/2000][14:58:21] bit:1024, dataset:palmprint, training... loss:0.69339\n",
      "[CSQ][1471/2000][14:58:31] bit:1024, dataset:palmprint, training... loss:0.69304\n",
      "[CSQ][1472/2000][14:58:41] bit:1024, dataset:palmprint, training... loss:0.69310\n",
      "[CSQ][1473/2000][14:58:50] bit:1024, dataset:palmprint, training... loss:0.69344\n",
      "[CSQ][1474/2000][14:58:59] bit:1024, dataset:palmprint, training... loss:0.69326\n",
      "[CSQ][1475/2000][14:59:09] bit:1024, dataset:palmprint, training... loss:0.69324\n",
      "[CSQ][1476/2000][14:59:18] bit:1024, dataset:palmprint, training... loss:0.69309\n",
      "[CSQ][1477/2000][14:59:27] bit:1024, dataset:palmprint, training... loss:0.69308\n",
      "[CSQ][1478/2000][14:59:38] bit:1024, dataset:palmprint, training... loss:0.69292\n",
      "[CSQ][1479/2000][14:59:47] bit:1024, dataset:palmprint, training... loss:0.69344\n",
      "[CSQ][1480/2000][14:59:56] bit:1024, dataset:palmprint, training... loss:0.69354\n",
      "[CSQ][1481/2000][15:00:07] bit:1024, dataset:palmprint, training... loss:0.69345\n",
      "[CSQ][1482/2000][15:00:15] bit:1024, dataset:palmprint, training... loss:0.69339\n",
      "[CSQ][1483/2000][15:00:24] bit:1024, dataset:palmprint, training... loss:0.69330\n",
      "[CSQ][1484/2000][15:00:34] bit:1024, dataset:palmprint, training... loss:0.69320\n",
      "[CSQ][1485/2000][15:00:43] bit:1024, dataset:palmprint, training... loss:0.69319\n",
      "[CSQ][1486/2000][15:00:53] bit:1024, dataset:palmprint, training... loss:0.69284\n",
      "[CSQ][1487/2000][15:01:02] bit:1024, dataset:palmprint, training... loss:0.69321\n",
      "[CSQ][1488/2000][15:01:12] bit:1024, dataset:palmprint, training... loss:0.69316\n",
      "[CSQ][1489/2000][15:01:21] bit:1024, dataset:palmprint, training... loss:0.69322\n",
      "[CSQ][1490/2000][15:01:30] bit:1024, dataset:palmprint, training... loss:0.69325\n",
      "[CSQ][1491/2000][15:01:41] bit:1024, dataset:palmprint, training... loss:0.69274\n",
      "[CSQ][1492/2000][15:01:50] bit:1024, dataset:palmprint, training... loss:0.69253\n",
      "[CSQ][1493/2000][15:02:00] bit:1024, dataset:palmprint, training... loss:0.69230\n",
      "[CSQ][1494/2000][15:02:09] bit:1024, dataset:palmprint, training... loss:0.69287\n",
      "[CSQ][1495/2000][15:02:19] bit:1024, dataset:palmprint, training... loss:0.69338\n",
      "[CSQ][1496/2000][15:02:29] bit:1024, dataset:palmprint, training... loss:0.69326\n",
      "[CSQ][1497/2000][15:02:38] bit:1024, dataset:palmprint, training... loss:0.69301\n",
      "[CSQ][1498/2000][15:02:48] bit:1024, dataset:palmprint, training... loss:0.69265\n",
      "[CSQ][1499/2000][15:02:57] bit:1024, dataset:palmprint, training... loss:0.69288\n",
      "[CSQ][1500/2000][15:03:06] bit:1024, dataset:palmprint, training... loss:0.69282\n",
      "[CSQ][1501/2000][15:03:15] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0026666666666666666\n",
      "[CSQ] epoch:1501, bit:1024, dataset:palmprint,eer:0.00267, Best eer: 0.00233\n",
      "\b loss:0.69281\n",
      "[CSQ][1502/2000][15:04:27] bit:1024, dataset:palmprint, training... loss:0.69307\n",
      "[CSQ][1503/2000][15:04:35] bit:1024, dataset:palmprint, training... loss:0.69310\n",
      "[CSQ][1504/2000][15:04:45] bit:1024, dataset:palmprint, training... loss:0.69277\n",
      "[CSQ][1505/2000][15:04:55] bit:1024, dataset:palmprint, training... loss:0.69291\n",
      "[CSQ][1506/2000][15:05:04] bit:1024, dataset:palmprint, training... loss:0.69256\n",
      "[CSQ][1507/2000][15:05:13] bit:1024, dataset:palmprint, training... loss:0.69281\n",
      "[CSQ][1508/2000][15:05:22] bit:1024, dataset:palmprint, training... loss:0.69307\n",
      "[CSQ][1509/2000][15:05:32] bit:1024, dataset:palmprint, training... loss:0.69223\n",
      "[CSQ][1510/2000][15:05:41] bit:1024, dataset:palmprint, training... loss:0.69260\n",
      "[CSQ][1511/2000][15:05:51] bit:1024, dataset:palmprint, training... loss:0.69254\n",
      "[CSQ][1512/2000][15:06:01] bit:1024, dataset:palmprint, training... loss:0.69272\n",
      "[CSQ][1513/2000][15:06:10] bit:1024, dataset:palmprint, training... loss:0.69310\n",
      "[CSQ][1514/2000][15:06:19] bit:1024, dataset:palmprint, training... loss:0.69240\n",
      "[CSQ][1515/2000][15:06:30] bit:1024, dataset:palmprint, training... loss:0.69263\n",
      "[CSQ][1516/2000][15:06:39] bit:1024, dataset:palmprint, training... loss:0.69270\n",
      "[CSQ][1517/2000][15:06:48] bit:1024, dataset:palmprint, training... loss:0.69255\n",
      "[CSQ][1518/2000][15:06:58] bit:1024, dataset:palmprint, training... loss:0.69259\n",
      "[CSQ][1519/2000][15:07:08] bit:1024, dataset:palmprint, training... loss:0.69244\n",
      "[CSQ][1520/2000][15:07:17] bit:1024, dataset:palmprint, training... loss:0.69262\n",
      "[CSQ][1521/2000][15:07:26] bit:1024, dataset:palmprint, training... loss:0.69218\n",
      "[CSQ][1522/2000][15:07:37] bit:1024, dataset:palmprint, training... loss:0.69256\n",
      "[CSQ][1523/2000][15:07:51] bit:1024, dataset:palmprint, training... loss:0.69240\n",
      "[CSQ][1524/2000][15:08:00] bit:1024, dataset:palmprint, training... loss:0.69253\n",
      "[CSQ][1525/2000][15:08:09] bit:1024, dataset:palmprint, training... loss:0.69256\n",
      "[CSQ][1526/2000][15:08:17] bit:1024, dataset:palmprint, training... loss:0.69290\n",
      "[CSQ][1527/2000][15:08:26] bit:1024, dataset:palmprint, training... loss:0.69236\n",
      "[CSQ][1528/2000][15:08:35] bit:1024, dataset:palmprint, training... loss:0.69276\n",
      "[CSQ][1529/2000][15:08:43] bit:1024, dataset:palmprint, training... loss:0.69234\n",
      "[CSQ][1530/2000][15:08:52] bit:1024, dataset:palmprint, training... loss:0.69243\n",
      "[CSQ][1531/2000][15:09:00] bit:1024, dataset:palmprint, training... loss:0.69252\n",
      "[CSQ][1532/2000][15:09:09] bit:1024, dataset:palmprint, training... loss:0.69266\n",
      "[CSQ][1533/2000][15:09:18] bit:1024, dataset:palmprint, training... loss:0.69246\n",
      "[CSQ][1534/2000][15:09:27] bit:1024, dataset:palmprint, training... loss:0.69275\n",
      "[CSQ][1535/2000][15:09:35] bit:1024, dataset:palmprint, training... loss:0.69253\n",
      "[CSQ][1536/2000][15:09:44] bit:1024, dataset:palmprint, training... loss:0.69273\n",
      "[CSQ][1537/2000][15:09:53] bit:1024, dataset:palmprint, training... loss:0.69205\n",
      "[CSQ][1538/2000][15:10:03] bit:1024, dataset:palmprint, training... loss:0.69211\n",
      "[CSQ][1539/2000][15:10:12] bit:1024, dataset:palmprint, training... loss:0.69186\n",
      "[CSQ][1540/2000][15:10:22] bit:1024, dataset:palmprint, training... loss:0.69228\n",
      "[CSQ][1541/2000][15:10:31] bit:1024, dataset:palmprint, training... loss:0.69244\n",
      "[CSQ][1542/2000][15:10:42] bit:1024, dataset:palmprint, training... loss:0.69293\n",
      "[CSQ][1543/2000][15:10:51] bit:1024, dataset:palmprint, training... loss:0.69176\n",
      "[CSQ][1544/2000][15:11:00] bit:1024, dataset:palmprint, training... loss:0.69242\n",
      "[CSQ][1545/2000][15:11:10] bit:1024, dataset:palmprint, training... loss:0.69249\n",
      "[CSQ][1546/2000][15:11:19] bit:1024, dataset:palmprint, training... loss:0.69173\n",
      "[CSQ][1547/2000][15:11:28] bit:1024, dataset:palmprint, training... loss:0.69199\n",
      "[CSQ][1548/2000][15:11:38] bit:1024, dataset:palmprint, training... loss:0.69259\n",
      "[CSQ][1549/2000][15:11:48] bit:1024, dataset:palmprint, training... loss:0.69202\n",
      "[CSQ][1550/2000][15:11:57] bit:1024, dataset:palmprint, training... loss:0.69214\n",
      "[CSQ][1551/2000][15:12:06] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0033333333333333335\n",
      "[CSQ] epoch:1551, bit:1024, dataset:palmprint,eer:0.00333, Best eer: 0.00233\n",
      "\b loss:0.69223\n",
      "[CSQ][1552/2000][15:13:19] bit:1024, dataset:palmprint, training... loss:0.69267\n",
      "[CSQ][1553/2000][15:13:28] bit:1024, dataset:palmprint, training... loss:0.69197\n",
      "[CSQ][1554/2000][15:13:37] bit:1024, dataset:palmprint, training... loss:0.69213\n",
      "[CSQ][1555/2000][15:13:47] bit:1024, dataset:palmprint, training... loss:0.69190\n",
      "[CSQ][1556/2000][15:13:57] bit:1024, dataset:palmprint, training... loss:0.69215\n",
      "[CSQ][1557/2000][15:14:06] bit:1024, dataset:palmprint, training... loss:0.69159\n",
      "[CSQ][1558/2000][15:14:16] bit:1024, dataset:palmprint, training... loss:0.69212\n",
      "[CSQ][1559/2000][15:14:27] bit:1024, dataset:palmprint, training... loss:0.69192\n",
      "[CSQ][1560/2000][15:14:36] bit:1024, dataset:palmprint, training... loss:0.69191\n",
      "[CSQ][1561/2000][15:14:45] bit:1024, dataset:palmprint, training... loss:0.69200\n",
      "[CSQ][1562/2000][15:14:56] bit:1024, dataset:palmprint, training... loss:0.69235\n",
      "[CSQ][1563/2000][15:15:05] bit:1024, dataset:palmprint, training... loss:0.69200\n",
      "[CSQ][1564/2000][15:15:13] bit:1024, dataset:palmprint, training... loss:0.69168\n",
      "[CSQ][1565/2000][15:15:23] bit:1024, dataset:palmprint, training... loss:0.69210\n",
      "[CSQ][1566/2000][15:15:32] bit:1024, dataset:palmprint, training... loss:0.69216\n",
      "[CSQ][1567/2000][15:15:41] bit:1024, dataset:palmprint, training... loss:0.69224\n",
      "[CSQ][1568/2000][15:15:51] bit:1024, dataset:palmprint, training... loss:0.69224\n",
      "[CSQ][1569/2000][15:16:01] bit:1024, dataset:palmprint, training... loss:0.69166\n",
      "[CSQ][1570/2000][15:16:10] bit:1024, dataset:palmprint, training... loss:0.69183\n",
      "[CSQ][1571/2000][15:16:19] bit:1024, dataset:palmprint, training... loss:0.69215\n",
      "[CSQ][1572/2000][15:16:30] bit:1024, dataset:palmprint, training... loss:0.69219\n",
      "[CSQ][1573/2000][15:16:39] bit:1024, dataset:palmprint, training... loss:0.69160\n",
      "[CSQ][1574/2000][15:16:47] bit:1024, dataset:palmprint, training... loss:0.69201\n",
      "[CSQ][1575/2000][15:16:57] bit:1024, dataset:palmprint, training... loss:0.69202\n",
      "[CSQ][1576/2000][15:17:07] bit:1024, dataset:palmprint, training... loss:0.69162\n",
      "[CSQ][1577/2000][15:17:16] bit:1024, dataset:palmprint, training... loss:0.69169\n",
      "[CSQ][1578/2000][15:17:25] bit:1024, dataset:palmprint, training... loss:0.69212\n",
      "[CSQ][1579/2000][15:17:35] bit:1024, dataset:palmprint, training... loss:0.69180\n",
      "[CSQ][1580/2000][15:17:45] bit:1024, dataset:palmprint, training... loss:0.69175\n",
      "[CSQ][1581/2000][15:17:54] bit:1024, dataset:palmprint, training... loss:0.69190\n",
      "[CSQ][1582/2000][15:18:04] bit:1024, dataset:palmprint, training... loss:0.69157\n",
      "[CSQ][1583/2000][15:18:13] bit:1024, dataset:palmprint, training... loss:0.69198\n",
      "[CSQ][1584/2000][15:18:22] bit:1024, dataset:palmprint, training... loss:0.69181\n",
      "[CSQ][1585/2000][15:18:31] bit:1024, dataset:palmprint, training... loss:0.69193\n",
      "[CSQ][1586/2000][15:18:42] bit:1024, dataset:palmprint, training... loss:0.69167\n",
      "[CSQ][1587/2000][15:18:51] bit:1024, dataset:palmprint, training... loss:0.69141\n",
      "[CSQ][1588/2000][15:19:00] bit:1024, dataset:palmprint, training... loss:0.69214\n",
      "[CSQ][1589/2000][15:19:10] bit:1024, dataset:palmprint, training... loss:0.69204\n",
      "[CSQ][1590/2000][15:19:20] bit:1024, dataset:palmprint, training... loss:0.69192\n",
      "[CSQ][1591/2000][15:19:30] bit:1024, dataset:palmprint, training... loss:0.69119\n",
      "[CSQ][1592/2000][15:19:39] bit:1024, dataset:palmprint, training... loss:0.69151\n",
      "[CSQ][1593/2000][15:19:50] bit:1024, dataset:palmprint, training... loss:0.69128\n",
      "[CSQ][1594/2000][15:19:59] bit:1024, dataset:palmprint, training... loss:0.69123\n",
      "[CSQ][1595/2000][15:20:08] bit:1024, dataset:palmprint, training... loss:0.69124\n",
      "[CSQ][1596/2000][15:20:17] bit:1024, dataset:palmprint, training... loss:0.69238\n",
      "[CSQ][1597/2000][15:20:27] bit:1024, dataset:palmprint, training... loss:0.69155\n",
      "[CSQ][1598/2000][15:20:36] bit:1024, dataset:palmprint, training... loss:0.69170\n",
      "[CSQ][1599/2000][15:20:46] bit:1024, dataset:palmprint, training... loss:0.69141\n",
      "[CSQ][1600/2000][15:21:01] bit:1024, dataset:palmprint, training... loss:0.69164\n",
      "[CSQ][1601/2000][15:21:09] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0033333333333333335\n",
      "[CSQ] epoch:1601, bit:1024, dataset:palmprint,eer:0.00333, Best eer: 0.00233\n",
      "\b loss:0.69180\n",
      "[CSQ][1602/2000][15:22:16] bit:1024, dataset:palmprint, training... loss:0.69109\n",
      "[CSQ][1603/2000][15:22:25] bit:1024, dataset:palmprint, training... loss:0.69131\n",
      "[CSQ][1604/2000][15:22:35] bit:1024, dataset:palmprint, training... loss:0.69113\n",
      "[CSQ][1605/2000][15:22:44] bit:1024, dataset:palmprint, training... loss:0.69144\n",
      "[CSQ][1606/2000][15:22:54] bit:1024, dataset:palmprint, training...."
     ]
    }
   ],
   "source": [
    "Best_eer = 1.0\n",
    "\n",
    "config[\"epoch\"] = 2000\n",
    "for epoch in range(500,config[\"epoch\"]):\n",
    "\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "    print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "        config[\"info\"], epoch + 1, config[\"epoch\"], current_time, bit, config[\"dataset\"]), end=\"\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_idx, img in enumerate(train_loader):\n",
    "        image = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "        label = img[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        u = model(image)\n",
    "\n",
    "        loss = criterion(u, label.float(), 0, config)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    if epoch % 50 ==0:\n",
    "        eer = test(model,test_loader)\n",
    "        if eer < Best_eer:\n",
    "            Best_eer = eer\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': Best_eer,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, True, remark='GCN2wayhashing')\n",
    "        print(\"%s epoch:%d, bit:%d, dataset:%s,eer:%.5f, Best eer: %.5f\" % (\n",
    "            config[\"info\"], epoch + 1, bit, config[\"dataset\"], eer, Best_eer))\n",
    "    print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))##loss:0.625\n",
    "    scheduler.step()\n",
    "\n",
    "#     if (epoch + 1) % config[\"test_map\"] == 0:\n",
    "#         # print(\"calculating test binary code......\")\n",
    "#         tst_binary, tst_label = compute_result(test_loader, net, device=device)\n",
    "\n",
    "#         # print(\"calculating dataset binary code.......\")\\\n",
    "#         trn_binary, trn_label = compute_result(dataset_loader, net, device=device)\n",
    "\n",
    "#         # print(\"calculating map.......\")\n",
    "#         mAP = CalcTopMap(trn_binary.numpy(), tst_binary.numpy(), trn_label.numpy(), tst_label.numpy(),\n",
    "#                          config[\"topK\"])\n",
    "\n",
    "#         if mAP > Best_mAP:\n",
    "#             Best_mAP = mAP\n",
    "\n",
    "#             if \"save_path\" in config:\n",
    "#                 if not os.path.exists(config[\"save_path\"]):\n",
    "#                     os.makedirs(config[\"save_path\"])\n",
    "#                 print(\"save in \", config[\"save_path\"])\n",
    "#                 np.save(os.path.join(config[\"save_path\"], config[\"dataset\"] + str(mAP) + \"-\" + \"trn_binary.npy\"),\n",
    "#                         trn_binary.numpy())\n",
    "#                 torch.save(net.state_dict(),\n",
    "#                            os.path.join(config[\"save_path\"], config[\"dataset\"] + \"-\" + str(mAP) + \"-model.pt\"))\n",
    "#         print(\"%s epoch:%d, bit:%d, dataset:%s, MAP:%.3f, Best MAP: %.3f\" % (\n",
    "#             config[\"info\"], epoch + 1, bit, config[\"dataset\"], mAP, Best_mAP))\n",
    "#         print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cefd0a-9a44-4860-8b8a-306dd326dc92",
   "metadata": {},
   "source": [
    "0.025666666666666667  0.01 lamda \"lr\": 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46c207d-c655-4440-a022-7b52fb8f09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if epoch % 50 ==0:\n",
    "# save_checkpoint({\n",
    "#     'epoch': epoch + 1,\n",
    "#     'state_dict': net.state_dict(),\n",
    "#     'best_prec1': 0,\n",
    "#     'optimizer' : optimizer.state_dict(),\n",
    "# }, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5106d6-538f-49b0-8727-639819292ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, img in tqdm.tqdm(enumerate(train_loader)):\n",
    "#     print((img[0].size(), img[1].size()))\n",
    "#     break\n",
    "    \n",
    "# for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "#     print((img[0].size(), img[1].size()))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07f76d-3767-4c30-a673-4ade0f99d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img[0].permute(0, 3, 1, 2).size())\n",
    "# print(img[0].permute(0, 3, 1, 2).double().dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac54206-2374-4eca-89a9-9dedd82149ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b584df4-e536-405f-b2a8-aecca260adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = model(img[0].permute(0, 3, 1, 2).float())\n",
    "# print(outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ae5d-1f0e-4125-9030-2d3e7a03eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best_gcn2wayca.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4c16a-6902-4aec-8243-638a3d87d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a085d-7905-4d95-a92b-dca883df4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=False)  # ,prefetch_factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20120f-d310-4dc6-8b6c-3f00a94f442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break\n",
    "print(img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2253928-e81e-4878-b0a0-09cde7302ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### HELPER FUNCTION FOR FEATURE EXTRACTION\n",
    "# # print(model)\n",
    "\n",
    "# def get_features(name):\n",
    "#     def hook(model, input, output):\n",
    "#         features[name] = output.detach()\n",
    "#     return hook\n",
    "# ##### REGISTER HOOK\n",
    "# model.model_print[16].register_forward_hook(get_features('feats'))\n",
    "# # model.fc1.register_forward_hook(get_features('feats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1841c-fd4b-4c4e-b867-77a297778675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(net,test_loader):\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    FEATS = []\n",
    "    GT = []\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            output = net(rbn)\n",
    "            FEATS.append(output.cpu().numpy())\n",
    "#             FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "    GCNFEATS = np.concatenate(FEATS)\n",
    "    GT = np.concatenate(GT)\n",
    "    print('- feats shape:', GCNFEATS.shape)\n",
    "    print('- GT shape:', GT.shape)\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    def cossim(a,b):\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    pred_scores = []\n",
    "    gt_label = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(3000)):\n",
    "        for j in range(i+1,3000):\n",
    "            # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "            a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "            pred_scores.append(a)\n",
    "            gt_label.append(i//6 == j//6)\n",
    "\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    gt_label = np.array(gt_label)\n",
    "\n",
    "    Gen = pred_scores[gt_label]\n",
    "    Imp = pred_scores[gt_label==False]\n",
    "    Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "#     import seaborn as sns\n",
    "#     sns.distplot(Gen,  kde=False, label='Gen')\n",
    "#     # df =gapminder[gapminder.continent == 'Americas']\n",
    "#     sns.distplot(Imp,  kde=False,label='Imp')\n",
    "#     # Plot formatting\n",
    "#     plt.legend(prop={'size': 12})\n",
    "#     plt.title('Life Expectancy of Two Continents')\n",
    "#     plt.xlabel('Life Exp (years)')\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "    from pyeer.eer_info import get_eer_stats\n",
    "    from pyeer.report import generate_eer_report, export_error_rates\n",
    "    from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "    # Calculating stats for classifier A\n",
    "    stats_a = get_eer_stats(Gen, Imp)\n",
    "    print(stats_a.eer)\n",
    "\n",
    "    return stats_a.eer\n",
    "\n",
    "test(net,test_loader)\n",
    "##### INSPECT FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b1964-473b-4444-b003-4fcf3cd1242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# np.save('FEATS_gcn2wayca.npy', GCNFEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8c159-8bc4-4f96-869a-98be084c706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATSRES18 = np.load('FEATS_res18.npy')\n",
    "# print(FEATSRES18.shape)\n",
    "# FEATSRES18 = np.squeeze(FEATSRES18);\n",
    "# print(FEATSRES18.shape)\n",
    "# # # NewFEATS = np.dstack((FEATS,FEATSRES18))\n",
    "# # FEATS = np.concatenate((FEATS,FEATSRES18), axis=1)\n",
    "# # print(NewFEATS.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc60cae-2740-45df-9277-817c10f4fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "pred_scores = []\n",
    "gt_label = []\n",
    "\n",
    "for i in tqdm.tqdm(range(3000)):\n",
    "    for j in range(i+1,3000):\n",
    "        # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "        a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "#         b = cossim(FEATSRES18[i,:],FEATSRES18[j,:])\n",
    "#         pred_scores.append((1.8*a+0.2*b)/2)\n",
    "        pred_scores.append(a)\n",
    "#         pred_scores.append(b)\n",
    "        gt_label.append(i//6 == j//6)\n",
    "        \n",
    "\n",
    "# for i in tqdm.tqdm(range(600)):\n",
    "#         # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "#         pred_scores.append(cossim(FEATS[i,:],FEATS[]))\n",
    "#         # gt_label.append(i//12 == j//12)\n",
    "pred_scores = np.array(pred_scores)\n",
    "gt_label = np.array(gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbcf98-18e0-4fda-8bfd-53925629bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Gen = pred_scores[gt_label]\n",
    "Imp = pred_scores[gt_label==False]\n",
    "Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.distplot(Gen,  kde=False, label='Gen')\n",
    "# df =gapminder[gapminder.continent == 'Americas']\n",
    "sns.distplot(Imp,  kde=False,label='Imp')\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.title('Life Expectancy of Two Continents')\n",
    "plt.xlabel('Life Exp (years)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from pyeer.report import generate_eer_report, export_error_rates\n",
    "from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "# Calculating stats for classifier A\n",
    "stats_a = get_eer_stats(Gen, Imp)\n",
    "print(stats_a.eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673f170-9bc8-499b-a6c2-29209e84ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)#, positive_label\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "\n",
    "eer = compute_eer(gt_label, pred_scores)\n",
    "print('The equal error rate is {:.3f}'.format(eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3bd27-046c-42d5-a20d-07a9a4343902",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Gen \n",
    "del Imp\n",
    "del pred_scores\n",
    "del gt_label\n",
    "del GCNFEATS\n",
    "del GT\n",
    "del test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
