{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c20581d-f5c4-47a1-8b6e-9289655b0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from utils import accuracy, AverageMeter, save_checkpoint, visualize_graph, get_parameters_size\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from net_factory import get_network_fn\n",
    "\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# del test_loader\n",
    "import tqdm\n",
    "# test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6983cbb4-bc52-4e7d-9ebf-a700dd4ba4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GCN MNIST Training')\n",
    "\n",
    "parser.add_argument('--epochs', default=50, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--pretrained', default='', type=str, metavar='PATH',\n",
    "                    help='path to pretrained checkpoint (default: none)')\n",
    "parser.add_argument('--gpu', default=0, type=int,\n",
    "                    metavar='N', help='GPU device ID (default: -1)')\n",
    "parser.add_argument('--dataset_dir', default='../../MNIST', type=str, metavar='PATH',\n",
    "                    help='path to dataset (default: ../MNIST)')\n",
    "parser.add_argument('--comment', default='', type=str, metavar='INFO',\n",
    "                    help='Extra description for tensorboard')\n",
    "parser.add_argument('--model', default='gcn', type=str, metavar='NETWORK',\n",
    "                    help='Network to train')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "use_cuda = (args.gpu >= 0) and torch.cuda.is_available()\n",
    "best_prec1 = 0\n",
    "writer = SummaryWriter(comment='_'+args.model+'_'+args.comment)\n",
    "iteration = 0\n",
    "\n",
    "# # from loaddataset import load_data\n",
    "# from loaddataset import load_data\n",
    "\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n",
    "# test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cdaa3e-43a8-4a65-b846-dbc1f66065e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-nas/Gabor_CNN_PyTorch/gcn/layers/GConv.py:67: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  for i in range(x.size(1)):\n",
      "/root/autodl-nas/Gabor_CNN_PyTorch/demo/net_factory.py:194: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return x + self.pe[:, :, :x.size(2), :x.size(3)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model\n",
    "model = get_network_fn(name='GCNhashingOneChannel')#GCNCNN\n",
    "# print(model)\n",
    "\n",
    "# Try to visulize the model\n",
    "try:\n",
    "\tvisualize_graph(model, writer, input_size=(1, 1, 128, 128))\n",
    "except:\n",
    "\tprint('\\nNetwork Visualization Failed! But the training procedure continue.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acf49d1-1fd2-4255-ad2d-759efd90d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1024])\n"
     ]
    }
   ],
   "source": [
    "out = model(torch.rand(5, 1, 128, 128))\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce89666-4993-4a4a-9625-722d15d45121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best_gcn2waycqhashing.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained,map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n",
    "# print(checkpoint['best_prec1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29785785-5148-4172-abf0-46105050db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSQLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(CSQLoss, self).__init__()\n",
    "        self.is_single_label = config[\"dataset\"] not in {\"nuswide_21\", \"nuswide_21_m\", \"coco\"}\n",
    "        self.hash_targets = self.get_hash_targets(config[\"n_class\"], bit).to(config[\"device\"])\n",
    "        self.multi_label_random_center = torch.randint(2, (bit,)).float().to(config[\"device\"])\n",
    "        self.criterion = torch.nn.BCELoss().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        u = u.tanh()\n",
    "        hash_center = self.label2center(y)\n",
    "        center_loss = self.criterion(0.5 * (u + 1), 0.5 * (hash_center + 1))\n",
    "\n",
    "        Q_loss = (u.abs() - 1).pow(2).mean()\n",
    "        return center_loss + config[\"lambda\"] * Q_loss\n",
    "\n",
    "    def label2center(self, y):\n",
    "        if self.is_single_label:\n",
    "            hash_center = self.hash_targets[y.argmax(axis=1)]\n",
    "        else:\n",
    "            # to get sign no need to use mean, use sum here\n",
    "            center_sum = y @ self.hash_targets\n",
    "            random_center = self.multi_label_random_center.repeat(center_sum.shape[0], 1)\n",
    "            center_sum[center_sum == 0] = random_center[center_sum == 0]\n",
    "            hash_center = 2 * (center_sum > 0).float() - 1\n",
    "        return hash_center\n",
    "\n",
    "    # use algorithm 1 to generate hash centers\n",
    "    def get_hash_targets(self, n_class, bit):\n",
    "        H_K = hadamard(bit)\n",
    "        H_2K = np.concatenate((H_K, -H_K), 0)\n",
    "        hash_targets = torch.from_numpy(H_2K[:n_class]).float()\n",
    "\n",
    "        if H_2K.shape[0] < n_class:\n",
    "            hash_targets.resize_(n_class, bit)\n",
    "            for k in range(20):\n",
    "                for index in range(H_2K.shape[0], n_class):\n",
    "                    ones = torch.ones(bit)\n",
    "                    # Bernouli distribution\n",
    "                    sa = random.sample(list(range(bit)), bit // 2)\n",
    "                    ones[sa] = -1\n",
    "                    hash_targets[index] = ones\n",
    "                # to find average/min  pairwise distance\n",
    "                c = []\n",
    "                for i in range(n_class):\n",
    "                    for j in range(n_class):\n",
    "                        if i < j:\n",
    "                            TF = sum(hash_targets[i] != hash_targets[j])\n",
    "                            c.append(TF)\n",
    "                c = np.array(c)\n",
    "\n",
    "                # choose min(c) in the range of K/4 to K/3\n",
    "                # see in https://github.com/yuanli2333/Hadamard-Matrix-for-hashing/issues/1\n",
    "                # but it is hard when bit is  small\n",
    "                if c.min() > bit / 4 and c.mean() >= bit / 2:\n",
    "                    print(c.min(), c.mean())\n",
    "                    break\n",
    "        return hash_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010b059e-a572-4e84-b2c8-5f5e17d8f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model.fc = nn.Linear(num_ftrs, 450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b1d92b-24b6-4edb-b68e-fba7bc38aef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# read image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "ms_polyu_path = 'dataset/MS_PolyU/'\n",
    "casia_path = 'dataset/CASIA-Multi-Spectral-PalmprintV1/images/'\n",
    "\n",
    "r_img_path = ms_polyu_path + 'Red_ind/'\n",
    "b_img_path =  ms_polyu_path + 'Blue_ind/'\n",
    "n_img_path =  ms_polyu_path + 'NIR_ind/'\n",
    "g_img_path =  ms_polyu_path + 'Green_ind/'\n",
    "\n",
    "################ DATASET CLASS\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] \n",
    "# one_hot_embedding(1, 10)\n",
    "def part_init(istrain=True):\n",
    "    r_list = []\n",
    "    b_list = []\n",
    "    vein_list = []\n",
    "    prints_list = []\n",
    "    labels = []\n",
    "    \n",
    "        # split all data into train, test data\n",
    "    train_ratio = 1\n",
    "    train_num = int(500 * train_ratio)\n",
    "    print(\"split train users:\",train_num)\n",
    "    if istrain:\n",
    "        for i in tqdm.tqdm(range(train_num)):\n",
    "            for j in range(8):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "#                 g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "#                 b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "#                 n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "#                 rb = r_normed - b_normed * 0.5\n",
    "#                 rb =  (rb * 128+128).astype(np.uint8)\n",
    "\n",
    "                imgprint = r_img\n",
    "#                 imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "#                 vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                labels.append(one_hot_embedding(i, train_num))\n",
    "#                 labels.append(i)\n",
    "    else:\n",
    "        for i in tqdm.tqdm(range(train_num)):\n",
    "            for j in range(8,12):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "#                 g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "#                 b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "#                 n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "#                 r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "#                 rb = r_normed - b_normed * 0.5\n",
    "#                 rb =  (rb * 128+128).astype(np.uint8)\n",
    "                imgprint = r_img\n",
    "#                 imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "#                 vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                labels.append(one_hot_embedding(i, train_num))\n",
    "#                 labels.append(i)\n",
    "\n",
    "\n",
    "\n",
    "    # return np.array(r_list), np.array(b_list), np.array(n_list), np.array(labels),np.array(r_list_test), np.array(b_list_test), np.array(n_list_test), np.array(labels_test)\n",
    "    return  vein_list,prints_list, labels\n",
    "\n",
    "# r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "class load_data(Dataset):\n",
    "    \"\"\"Loads the Data.\"\"\"\n",
    "    def __init__(self, training=True):\n",
    "\n",
    "        self.training = training\n",
    "#         r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomPerspective(),\n",
    "        transforms.RandomAffine(30),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        self.transform_test = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        if self.training:\n",
    "            print('\\n...... Train files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels= part_init(istrain=True)\n",
    "            print('\\nTrain files loaded ......\\n')\n",
    "        else:\n",
    "            print('\\n...... Test files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels = part_init(istrain=False)\n",
    "            print('\\nTest files loaded ......\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prints_list)\n",
    "\n",
    "         \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.training:\n",
    "            prints_img = self.transform(self.prints_list[idx])\n",
    "#             vein_img = self.transform(self.vein_list[idx])\n",
    "        else:\n",
    "            prints_img = self.transform_test(self.prints_list[idx])\n",
    "#             vein_img = self.transform_test(self.vein_list[idx])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        n_img = prints_img\n",
    "        \n",
    "        return n_img,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33159f71-22b4-4950-9972-b2cfa3d3277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda': 0.1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[CSQ]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 64, 'net': 'w', 'dataset': 'palmprint', 'n_class': 500, 'epoch': 2000, 'test_map': 10, 'device': device(type='cuda', index=0), 'bit_list': [1024]}\n"
     ]
    }
   ],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"lambda\": 0.1,\n",
    "        \"optimizer\": {\"type\": optim.RMSprop, \"optim_params\": {\"lr\": 1e-5, \"weight_decay\": 10 ** -5}},\n",
    "        \"info\": \"[CSQ]\",\n",
    "        \"resize_size\": 256,\n",
    "        \"crop_size\": 224,\n",
    "        \"batch_size\": 64,\n",
    "        # \"net\": AlexNet,\n",
    "        \"net\": \"w\",\n",
    "        \"dataset\": \"palmprint\",\n",
    "        \"n_class\":500,\n",
    "        # \"dataset\": \"imagenet\",\n",
    "        # \"dataset\": \"coco\",\n",
    "        # \"dataset\": \"nuswide_21\",\n",
    "        # \"dataset\": \"nuswide_21_m\",\n",
    "        \"epoch\": 2000,\n",
    "        \"test_map\": 10,\n",
    "        # \"device\":torch.device(\"cpu\"),\n",
    "        \"device\": torch.device(\"cuda:0\"),\n",
    "        \"bit_list\": [1024],\n",
    "    }\n",
    "#     config = config_dataset(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "print(config)\n",
    "bit = 1024\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d7fd0b-2f15-4bd8-91ab-f1e224e7e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...... Train files loading\n",
      "\n",
      "split train users: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:17<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train files loaded ......\n",
      "\n",
      "\n",
      "...... Test files loading\n",
      "\n",
      "split train users: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 56.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test files loaded ......\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset = get_data(config)\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,prefetch_factor=8)  # ,prefetch_factor=2\n",
    "test_loader = DataLoader(load_data(training=False), batch_size=64, shuffle=False)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7eef5a-543a-4f1c-88cf-8cc8cff9e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:06, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([512, 1, 128, 128]), torch.Size([512, 500]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([64, 1, 128, 128]), torch.Size([64, 500]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for _, img in tqdm.tqdm(enumerate(train_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break\n",
    "    \n",
    "for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6306800-6cbc-456e-bea1-66480629db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 32\n",
    "dataset_loader = train_loader\n",
    "num_train = 4000\n",
    "num_test = 2000\n",
    "num_dataset = 6000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62da0f6d-5f5e-418d-9c04-28ed7f96ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ae9993-d8d2-44b0-bae1-4458ba730ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "# from network import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.linalg import hadamard  # direct import  hadamrd matrix from scipy\n",
    "import random\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "\n",
    "class CSQLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(CSQLoss, self).__init__()\n",
    "        self.is_single_label = config[\"dataset\"] not in {\"nuswide_21\", \"nuswide_21_m\", \"coco\"}\n",
    "        self.hash_targets = self.get_hash_targets(config[\"n_class\"], bit).to(config[\"device\"])\n",
    "        self.multi_label_random_center = torch.randint(2, (bit,)).float().to(config[\"device\"])\n",
    "        self.criterion = torch.nn.BCELoss().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        u = u.tanh()\n",
    "        hash_center = self.label2center(y)\n",
    "        center_loss = self.criterion(0.5 * (u + 1), 0.5 * (hash_center + 1))\n",
    "\n",
    "        Q_loss = (u.abs() - 1).pow(2).mean()\n",
    "        return center_loss + config[\"lambda\"] * Q_loss\n",
    "\n",
    "    def label2center(self, y):\n",
    "        if self.is_single_label:\n",
    "            hash_center = self.hash_targets[y.argmax(axis=1)]\n",
    "        else:\n",
    "            # to get sign no need to use mean, use sum here\n",
    "            center_sum = y @ self.hash_targets\n",
    "            random_center = self.multi_label_random_center.repeat(center_sum.shape[0], 1)\n",
    "            center_sum[center_sum == 0] = random_center[center_sum == 0]\n",
    "            hash_center = 2 * (center_sum > 0).float() - 1\n",
    "        return hash_center\n",
    "\n",
    "    # use algorithm 1 to generate hash centers\n",
    "    def get_hash_targets(self, n_class, bit):\n",
    "        H_K = hadamard(bit)\n",
    "        H_2K = np.concatenate((H_K, -H_K), 0)\n",
    "        hash_targets = torch.from_numpy(H_2K[:n_class]).float()\n",
    "\n",
    "        if H_2K.shape[0] < n_class:\n",
    "            hash_targets.resize_(n_class, bit)\n",
    "            for k in range(20):\n",
    "                for index in range(H_2K.shape[0], n_class):\n",
    "                    ones = torch.ones(bit)\n",
    "                    # Bernouli distribution\n",
    "                    sa = random.sample(list(range(bit)), bit // 2)\n",
    "                    ones[sa] = -1\n",
    "                    hash_targets[index] = ones\n",
    "                # to find average/min  pairwise distance\n",
    "                c = []\n",
    "                for i in range(n_class):\n",
    "                    for j in range(n_class):\n",
    "                        if i < j:\n",
    "                            TF = sum(hash_targets[i] != hash_targets[j])\n",
    "                            c.append(TF)\n",
    "                c = np.array(c)\n",
    "\n",
    "                # choose min(c) in the range of K/4 to K/3\n",
    "                # see in https://github.com/yuanli2333/Hadamard-Matrix-for-hashing/issues/1\n",
    "                # but it is hard when bit is  small\n",
    "                if c.min() > bit / 4 and c.mean() >= bit / 2:\n",
    "                    print(c.min(), c.mean())\n",
    "                    break\n",
    "        return hash_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fbdca84-303c-405a-b2b7-31d138e9298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4d94-f3ec-4d50-915e-3e60310f83f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b92f556f-0fd8-4248-894e-db92eb2062cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_train\"] = num_train\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = config[\"optimizer\"][\"type\"](model.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
    "\n",
    "criterion = CSQLoss(config, bit)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=args.momentum, weight_decay=3e-05)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0136acba-d04b-4a4e-b76a-fa22ea3bf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37e5dc6d-0958-4215-ba97-1a473024789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9851886  0.17147427]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "A = np.random.randn(1,2)*10\n",
    "# print(A)\n",
    "# print(normalized(A,0))\n",
    "print(normalized(A,1))# ok verified\n",
    "\n",
    "\n",
    "def test(net,test_loader):\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    FEATS = []\n",
    "    GT = []\n",
    "    net.eval()\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            output = net(rbn)\n",
    "            FEATS.append(output.cpu().numpy())\n",
    "#             FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "    GCNFEATS = np.concatenate(FEATS)\n",
    "    GT = np.concatenate(GT)\n",
    "    GCNFEATS = normalized(GCNFEATS,1)\n",
    "    print('- feats shape:', GCNFEATS.shape)\n",
    "    print('- GT shape:', GT.shape)\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    def cossim(a,b):\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    pred_scores = []\n",
    "    gt_label = []\n",
    "\n",
    "    for i in range(2000):\n",
    "        for j in range(i+1,2000):\n",
    "            # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "            a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "            pred_scores.append(a)\n",
    "            gt_label.append(i//4 == j//4)\n",
    "\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    gt_label = np.array(gt_label)\n",
    "\n",
    "    Gen = pred_scores[gt_label]\n",
    "    Imp = pred_scores[gt_label==False]\n",
    "    Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "#     import seaborn as sns\n",
    "#     sns.distplot(Gen,  kde=False, label='Gen')\n",
    "#     # df =gapminder[gapminder.continent == 'Americas']\n",
    "#     sns.distplot(Imp,  kde=False,label='Imp')\n",
    "#     # Plot formatting\n",
    "#     plt.legend(prop={'size': 12})\n",
    "#     plt.title('Life Expectancy of Two Continents')\n",
    "#     plt.xlabel('Life Exp (years)')\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "    from pyeer.eer_info import get_eer_stats\n",
    "    from pyeer.report import generate_eer_report, export_error_rates\n",
    "    from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "    # Calculating stats for classifier A\n",
    "    stats_a = get_eer_stats(Gen, Imp)\n",
    "    print(stats_a.eer)\n",
    "\n",
    "    return stats_a.eer\n",
    "\n",
    "##### INSPECT FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53117228-f6d8-4b16-b4b1-5cc939b42d58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSQ][ 1/2000][15:53:18] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.05366666666666667\n",
      "[CSQ] epoch:1, bit:1024, dataset:palmprint,eer:0.05367, Best eer: 0.05367\n",
      "\b loss:0.83516\n",
      "[CSQ][ 2/2000][15:54:30] bit:1024, dataset:palmprint, training... loss:0.82142\n",
      "[CSQ][ 3/2000][15:54:39] bit:1024, dataset:palmprint, training... loss:0.81349\n",
      "[CSQ][ 4/2000][15:54:47] bit:1024, dataset:palmprint, training... loss:0.80772\n",
      "[CSQ][ 5/2000][15:54:55] bit:1024, dataset:palmprint, training... loss:0.80281\n",
      "[CSQ][ 6/2000][15:55:03] bit:1024, dataset:palmprint, training... loss:0.79878\n",
      "[CSQ][ 7/2000][15:55:11] bit:1024, dataset:palmprint, training... loss:0.79613\n",
      "[CSQ][ 8/2000][15:55:19] bit:1024, dataset:palmprint, training... loss:0.79408\n",
      "[CSQ][ 9/2000][15:55:28] bit:1024, dataset:palmprint, training... loss:0.79211\n",
      "[CSQ][10/2000][15:55:36] bit:1024, dataset:palmprint, training... loss:0.79150\n",
      "[CSQ][11/2000][15:55:45] bit:1024, dataset:palmprint, training... loss:0.78971\n",
      "[CSQ][12/2000][15:55:55] bit:1024, dataset:palmprint, training... loss:0.78898\n",
      "[CSQ][13/2000][15:56:03] bit:1024, dataset:palmprint, training... loss:0.78761\n",
      "[CSQ][14/2000][15:56:10] bit:1024, dataset:palmprint, training... loss:0.78667\n",
      "[CSQ][15/2000][15:56:18] bit:1024, dataset:palmprint, training... loss:0.78639\n",
      "[CSQ][16/2000][15:56:26] bit:1024, dataset:palmprint, training... loss:0.78533\n",
      "[CSQ][17/2000][15:56:34] bit:1024, dataset:palmprint, training... loss:0.78468\n",
      "[CSQ][18/2000][15:56:42] bit:1024, dataset:palmprint, training... loss:0.78419\n",
      "[CSQ][19/2000][15:56:50] bit:1024, dataset:palmprint, training... loss:0.78374\n",
      "[CSQ][20/2000][15:56:58] bit:1024, dataset:palmprint, training... loss:0.78334\n",
      "[CSQ][21/2000][15:57:06] bit:1024, dataset:palmprint, training... loss:0.78282\n",
      "[CSQ][22/2000][15:57:13] bit:1024, dataset:palmprint, training... loss:0.78236\n",
      "[CSQ][23/2000][15:57:22] bit:1024, dataset:palmprint, training... loss:0.78192\n",
      "[CSQ][24/2000][15:57:30] bit:1024, dataset:palmprint, training... loss:0.78128\n",
      "[CSQ][25/2000][15:57:38] bit:1024, dataset:palmprint, training... loss:0.78121\n",
      "[CSQ][26/2000][15:57:47] bit:1024, dataset:palmprint, training... loss:0.78083\n",
      "[CSQ][27/2000][15:57:55] bit:1024, dataset:palmprint, training... loss:0.78031\n",
      "[CSQ][28/2000][15:58:03] bit:1024, dataset:palmprint, training... loss:0.78007\n",
      "[CSQ][29/2000][15:58:11] bit:1024, dataset:palmprint, training... loss:0.77964\n",
      "[CSQ][30/2000][15:58:19] bit:1024, dataset:palmprint, training... loss:0.77996\n",
      "[CSQ][31/2000][15:58:27] bit:1024, dataset:palmprint, training... loss:0.77951\n",
      "[CSQ][32/2000][15:58:36] bit:1024, dataset:palmprint, training... loss:0.77904\n",
      "[CSQ][33/2000][15:58:44] bit:1024, dataset:palmprint, training... loss:0.77882\n",
      "[CSQ][34/2000][15:58:51] bit:1024, dataset:palmprint, training... loss:0.77855\n",
      "[CSQ][35/2000][15:59:00] bit:1024, dataset:palmprint, training... loss:0.77807\n",
      "[CSQ][36/2000][15:59:08] bit:1024, dataset:palmprint, training... loss:0.77773\n",
      "[CSQ][37/2000][15:59:16] bit:1024, dataset:palmprint, training... loss:0.77764\n",
      "[CSQ][38/2000][15:59:25] bit:1024, dataset:palmprint, training... loss:0.77721\n",
      "[CSQ][39/2000][15:59:33] bit:1024, dataset:palmprint, training... loss:0.77719\n",
      "[CSQ][40/2000][15:59:40] bit:1024, dataset:palmprint, training... loss:0.77665\n",
      "[CSQ][41/2000][15:59:49] bit:1024, dataset:palmprint, training... loss:0.77617\n",
      "[CSQ][42/2000][15:59:57] bit:1024, dataset:palmprint, training... loss:0.77629\n",
      "[CSQ][43/2000][16:00:05] bit:1024, dataset:palmprint, training... loss:0.77610\n",
      "[CSQ][44/2000][16:00:14] bit:1024, dataset:palmprint, training... loss:0.77571\n",
      "[CSQ][45/2000][16:00:22] bit:1024, dataset:palmprint, training... loss:0.77577\n",
      "[CSQ][46/2000][16:00:30] bit:1024, dataset:palmprint, training... loss:0.77510\n",
      "[CSQ][47/2000][16:00:39] bit:1024, dataset:palmprint, training... loss:0.77454\n",
      "[CSQ][48/2000][16:00:47] bit:1024, dataset:palmprint, training... loss:0.77424\n",
      "[CSQ][49/2000][16:00:54] bit:1024, dataset:palmprint, training... loss:0.77412\n",
      "[CSQ][50/2000][16:01:03] bit:1024, dataset:palmprint, training... loss:0.77385\n",
      "[CSQ][51/2000][16:01:11] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.034666666666666665\n",
      "[CSQ] epoch:51, bit:1024, dataset:palmprint,eer:0.03467, Best eer: 0.03467\n",
      "\b loss:0.77374\n",
      "[CSQ][52/2000][16:02:25] bit:1024, dataset:palmprint, training... loss:0.77354\n",
      "[CSQ][53/2000][16:02:33] bit:1024, dataset:palmprint, training... loss:0.77293\n",
      "[CSQ][54/2000][16:02:42] bit:1024, dataset:palmprint, training... loss:0.77248\n",
      "[CSQ][55/2000][16:02:51] bit:1024, dataset:palmprint, training... loss:0.77222\n",
      "[CSQ][56/2000][16:02:59] bit:1024, dataset:palmprint, training... loss:0.77220\n",
      "[CSQ][57/2000][16:03:07] bit:1024, dataset:palmprint, training... loss:0.77158\n",
      "[CSQ][58/2000][16:03:16] bit:1024, dataset:palmprint, training... loss:0.77140\n",
      "[CSQ][59/2000][16:03:24] bit:1024, dataset:palmprint, training... loss:0.77107\n",
      "[CSQ][60/2000][16:03:32] bit:1024, dataset:palmprint, training... loss:0.77081\n",
      "[CSQ][61/2000][16:03:40] bit:1024, dataset:palmprint, training... loss:0.77072\n",
      "[CSQ][62/2000][16:03:49] bit:1024, dataset:palmprint, training... loss:0.77032\n",
      "[CSQ][63/2000][16:03:57] bit:1024, dataset:palmprint, training... loss:0.77017\n",
      "[CSQ][64/2000][16:04:05] bit:1024, dataset:palmprint, training... loss:0.76980\n",
      "[CSQ][65/2000][16:04:12] bit:1024, dataset:palmprint, training... loss:0.76963\n",
      "[CSQ][66/2000][16:04:21] bit:1024, dataset:palmprint, training... loss:0.76943\n",
      "[CSQ][67/2000][16:04:29] bit:1024, dataset:palmprint, training... loss:0.76867\n",
      "[CSQ][68/2000][16:04:37] bit:1024, dataset:palmprint, training... loss:0.76844\n",
      "[CSQ][69/2000][16:04:45] bit:1024, dataset:palmprint, training... loss:0.76825\n",
      "[CSQ][70/2000][16:04:53] bit:1024, dataset:palmprint, training... loss:0.76789\n",
      "[CSQ][71/2000][16:05:01] bit:1024, dataset:palmprint, training... loss:0.76759\n",
      "[CSQ][72/2000][16:05:10] bit:1024, dataset:palmprint, training... loss:0.76733\n",
      "[CSQ][73/2000][16:05:18] bit:1024, dataset:palmprint, training... loss:0.76702\n",
      "[CSQ][74/2000][16:05:26] bit:1024, dataset:palmprint, training... loss:0.76734\n",
      "[CSQ][75/2000][16:05:35] bit:1024, dataset:palmprint, training... loss:0.76699\n",
      "[CSQ][76/2000][16:05:42] bit:1024, dataset:palmprint, training... loss:0.76612\n",
      "[CSQ][77/2000][16:05:50] bit:1024, dataset:palmprint, training... loss:0.76622\n",
      "[CSQ][78/2000][16:05:59] bit:1024, dataset:palmprint, training... loss:0.76609\n",
      "[CSQ][79/2000][16:06:07] bit:1024, dataset:palmprint, training... loss:0.76586\n",
      "[CSQ][80/2000][16:06:15] bit:1024, dataset:palmprint, training... loss:0.76553\n",
      "[CSQ][81/2000][16:06:24] bit:1024, dataset:palmprint, training... loss:0.76505\n",
      "[CSQ][82/2000][16:06:32] bit:1024, dataset:palmprint, training... loss:0.76475\n",
      "[CSQ][83/2000][16:06:39] bit:1024, dataset:palmprint, training... loss:0.76479\n",
      "[CSQ][84/2000][16:06:48] bit:1024, dataset:palmprint, training... loss:0.76476\n",
      "[CSQ][85/2000][16:06:56] bit:1024, dataset:palmprint, training... loss:0.76436\n",
      "[CSQ][86/2000][16:07:05] bit:1024, dataset:palmprint, training... loss:0.76398\n",
      "[CSQ][87/2000][16:07:17] bit:1024, dataset:palmprint, training... loss:0.76343\n",
      "[CSQ][88/2000][16:07:25] bit:1024, dataset:palmprint, training... loss:0.76303\n",
      "[CSQ][89/2000][16:07:33] bit:1024, dataset:palmprint, training... loss:0.76329\n",
      "[CSQ][90/2000][16:07:41] bit:1024, dataset:palmprint, training... loss:0.76249\n",
      "[CSQ][91/2000][16:07:48] bit:1024, dataset:palmprint, training... loss:0.76305\n",
      "[CSQ][92/2000][16:07:56] bit:1024, dataset:palmprint, training... loss:0.76244\n",
      "[CSQ][93/2000][16:08:04] bit:1024, dataset:palmprint, training... loss:0.76244\n",
      "[CSQ][94/2000][16:08:11] bit:1024, dataset:palmprint, training... loss:0.76203\n",
      "[CSQ][95/2000][16:08:19] bit:1024, dataset:palmprint, training... loss:0.76166\n",
      "[CSQ][96/2000][16:08:27] bit:1024, dataset:palmprint, training... loss:0.76122\n",
      "[CSQ][97/2000][16:08:35] bit:1024, dataset:palmprint, training... loss:0.76125\n",
      "[CSQ][98/2000][16:08:43] bit:1024, dataset:palmprint, training... loss:0.76128\n",
      "[CSQ][99/2000][16:08:52] bit:1024, dataset:palmprint, training... loss:0.76093\n",
      "[CSQ][100/2000][16:09:00] bit:1024, dataset:palmprint, training... loss:0.75999\n",
      "[CSQ][101/2000][16:09:08] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.023\n",
      "[CSQ] epoch:101, bit:1024, dataset:palmprint,eer:0.02300, Best eer: 0.02300\n",
      "\b loss:0.76023\n",
      "[CSQ][102/2000][16:10:26] bit:1024, dataset:palmprint, training... loss:0.76019\n",
      "[CSQ][103/2000][16:10:34] bit:1024, dataset:palmprint, training... loss:0.76025\n",
      "[CSQ][104/2000][16:10:43] bit:1024, dataset:palmprint, training... loss:0.75944\n",
      "[CSQ][105/2000][16:10:51] bit:1024, dataset:palmprint, training... loss:0.75933\n",
      "[CSQ][106/2000][16:10:59] bit:1024, dataset:palmprint, training... loss:0.75908\n",
      "[CSQ][107/2000][16:11:08] bit:1024, dataset:palmprint, training... loss:0.75964\n",
      "[CSQ][108/2000][16:11:17] bit:1024, dataset:palmprint, training... loss:0.75855\n",
      "[CSQ][109/2000][16:11:24] bit:1024, dataset:palmprint, training... loss:0.75837\n",
      "[CSQ][110/2000][16:11:33] bit:1024, dataset:palmprint, training... loss:0.75930\n",
      "[CSQ][111/2000][16:11:41] bit:1024, dataset:palmprint, training... loss:0.75817\n",
      "[CSQ][112/2000][16:11:50] bit:1024, dataset:palmprint, training... loss:0.75810\n",
      "[CSQ][113/2000][16:11:58] bit:1024, dataset:palmprint, training... loss:0.75819\n",
      "[CSQ][114/2000][16:12:06] bit:1024, dataset:palmprint, training... loss:0.75749\n",
      "[CSQ][115/2000][16:12:14] bit:1024, dataset:palmprint, training... loss:0.75740\n",
      "[CSQ][116/2000][16:12:22] bit:1024, dataset:palmprint, training... loss:0.75702\n",
      "[CSQ][117/2000][16:12:30] bit:1024, dataset:palmprint, training... loss:0.75738\n",
      "[CSQ][118/2000][16:12:38] bit:1024, dataset:palmprint, training... loss:0.75683\n",
      "[CSQ][119/2000][16:12:47] bit:1024, dataset:palmprint, training... loss:0.75730\n",
      "[CSQ][120/2000][16:12:55] bit:1024, dataset:palmprint, training... loss:0.75646\n",
      "[CSQ][121/2000][16:13:02] bit:1024, dataset:palmprint, training... loss:0.75640\n",
      "[CSQ][122/2000][16:13:11] bit:1024, dataset:palmprint, training... loss:0.75616\n",
      "[CSQ][123/2000][16:13:20] bit:1024, dataset:palmprint, training... loss:0.75638\n",
      "[CSQ][124/2000][16:13:28] bit:1024, dataset:palmprint, training... loss:0.75600\n",
      "[CSQ][125/2000][16:13:36] bit:1024, dataset:palmprint, training... loss:0.75554\n",
      "[CSQ][126/2000][16:13:44] bit:1024, dataset:palmprint, training... loss:0.75543\n",
      "[CSQ][127/2000][16:13:52] bit:1024, dataset:palmprint, training... loss:0.75505\n",
      "[CSQ][128/2000][16:14:01] bit:1024, dataset:palmprint, training... loss:0.75526\n",
      "[CSQ][129/2000][16:14:09] bit:1024, dataset:palmprint, training... loss:0.75547\n",
      "[CSQ][130/2000][16:14:17] bit:1024, dataset:palmprint, training... loss:0.75532\n",
      "[CSQ][131/2000][16:14:25] bit:1024, dataset:palmprint, training... loss:0.75483\n",
      "[CSQ][132/2000][16:14:33] bit:1024, dataset:palmprint, training... loss:0.75485\n",
      "[CSQ][133/2000][16:14:41] bit:1024, dataset:palmprint, training... loss:0.75421\n",
      "[CSQ][134/2000][16:14:49] bit:1024, dataset:palmprint, training... loss:0.75434\n",
      "[CSQ][135/2000][16:14:58] bit:1024, dataset:palmprint, training... loss:0.75486\n",
      "[CSQ][136/2000][16:15:06] bit:1024, dataset:palmprint, training... loss:0.75408\n",
      "[CSQ][137/2000][16:15:14] bit:1024, dataset:palmprint, training... loss:0.75414\n",
      "[CSQ][138/2000][16:15:22] bit:1024, dataset:palmprint, training... loss:0.75377\n",
      "[CSQ][139/2000][16:15:29] bit:1024, dataset:palmprint, training... loss:0.75360\n",
      "[CSQ][140/2000][16:15:38] bit:1024, dataset:palmprint, training... loss:0.75399\n",
      "[CSQ][141/2000][16:15:46] bit:1024, dataset:palmprint, training... loss:0.75336\n",
      "[CSQ][142/2000][16:15:54] bit:1024, dataset:palmprint, training... loss:0.75339\n",
      "[CSQ][143/2000][16:16:03] bit:1024, dataset:palmprint, training... loss:0.75317\n",
      "[CSQ][144/2000][16:16:10] bit:1024, dataset:palmprint, training... loss:0.75302\n",
      "[CSQ][145/2000][16:16:18] bit:1024, dataset:palmprint, training... loss:0.75260\n",
      "[CSQ][146/2000][16:16:27] bit:1024, dataset:palmprint, training... loss:0.75273\n",
      "[CSQ][147/2000][16:16:35] bit:1024, dataset:palmprint, training... loss:0.75306\n",
      "[CSQ][148/2000][16:16:44] bit:1024, dataset:palmprint, training... loss:0.75251\n",
      "[CSQ][149/2000][16:16:52] bit:1024, dataset:palmprint, training... loss:0.75192\n",
      "[CSQ][150/2000][16:17:00] bit:1024, dataset:palmprint, training... loss:0.75251\n",
      "[CSQ][151/2000][16:17:08] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.018666666666666668\n",
      "[CSQ] epoch:151, bit:1024, dataset:palmprint,eer:0.01867, Best eer: 0.01867\n",
      "\b loss:0.75211\n",
      "[CSQ][152/2000][16:18:26] bit:1024, dataset:palmprint, training... loss:0.75168\n",
      "[CSQ][153/2000][16:18:35] bit:1024, dataset:palmprint, training... loss:0.75125\n",
      "[CSQ][154/2000][16:18:43] bit:1024, dataset:palmprint, training... loss:0.75178\n",
      "[CSQ][155/2000][16:18:55] bit:1024, dataset:palmprint, training... loss:0.75172\n",
      "[CSQ][156/2000][16:19:03] bit:1024, dataset:palmprint, training... loss:0.75148\n",
      "[CSQ][157/2000][16:19:11] bit:1024, dataset:palmprint, training... loss:0.75178\n",
      "[CSQ][158/2000][16:19:19] bit:1024, dataset:palmprint, training... loss:0.75099\n",
      "[CSQ][159/2000][16:19:26] bit:1024, dataset:palmprint, training... loss:0.75083\n",
      "[CSQ][160/2000][16:19:34] bit:1024, dataset:palmprint, training... loss:0.75110\n",
      "[CSQ][161/2000][16:19:42] bit:1024, dataset:palmprint, training... loss:0.75039\n",
      "[CSQ][162/2000][16:19:49] bit:1024, dataset:palmprint, training... loss:0.75077\n",
      "[CSQ][163/2000][16:19:57] bit:1024, dataset:palmprint, training... loss:0.74954\n",
      "[CSQ][164/2000][16:20:05] bit:1024, dataset:palmprint, training... loss:0.74980\n",
      "[CSQ][165/2000][16:20:13] bit:1024, dataset:palmprint, training... loss:0.75006\n",
      "[CSQ][166/2000][16:20:21] bit:1024, dataset:palmprint, training... loss:0.74964\n",
      "[CSQ][167/2000][16:20:30] bit:1024, dataset:palmprint, training... loss:0.74945\n",
      "[CSQ][168/2000][16:20:38] bit:1024, dataset:palmprint, training... loss:0.74984\n",
      "[CSQ][169/2000][16:20:47] bit:1024, dataset:palmprint, training... loss:0.74948\n",
      "[CSQ][170/2000][16:20:55] bit:1024, dataset:palmprint, training... loss:0.74888\n",
      "[CSQ][171/2000][16:21:03] bit:1024, dataset:palmprint, training... loss:0.74901\n",
      "[CSQ][172/2000][16:21:11] bit:1024, dataset:palmprint, training... loss:0.74986\n",
      "[CSQ][173/2000][16:21:19] bit:1024, dataset:palmprint, training... loss:0.74953\n",
      "[CSQ][174/2000][16:21:27] bit:1024, dataset:palmprint, training... loss:0.74907\n",
      "[CSQ][175/2000][16:21:35] bit:1024, dataset:palmprint, training... loss:0.74906\n",
      "[CSQ][176/2000][16:21:43] bit:1024, dataset:palmprint, training... loss:0.74869\n",
      "[CSQ][177/2000][16:21:51] bit:1024, dataset:palmprint, training... loss:0.74816\n",
      "[CSQ][178/2000][16:22:00] bit:1024, dataset:palmprint, training... loss:0.74806\n",
      "[CSQ][179/2000][16:22:08] bit:1024, dataset:palmprint, training... loss:0.74821\n",
      "[CSQ][180/2000][16:22:15] bit:1024, dataset:palmprint, training... loss:0.74785\n",
      "[CSQ][181/2000][16:22:24] bit:1024, dataset:palmprint, training... loss:0.74798\n",
      "[CSQ][182/2000][16:22:32] bit:1024, dataset:palmprint, training... loss:0.74843\n",
      "[CSQ][183/2000][16:22:40] bit:1024, dataset:palmprint, training... loss:0.74756\n",
      "[CSQ][184/2000][16:22:49] bit:1024, dataset:palmprint, training... loss:0.74779\n",
      "[CSQ][185/2000][16:22:57] bit:1024, dataset:palmprint, training... loss:0.74725\n",
      "[CSQ][186/2000][16:23:05] bit:1024, dataset:palmprint, training... loss:0.74743\n",
      "[CSQ][187/2000][16:23:14] bit:1024, dataset:palmprint, training... loss:0.74706\n",
      "[CSQ][188/2000][16:23:22] bit:1024, dataset:palmprint, training... loss:0.74716\n",
      "[CSQ][189/2000][16:23:29] bit:1024, dataset:palmprint, training... loss:0.74729\n",
      "[CSQ][190/2000][16:23:38] bit:1024, dataset:palmprint, training... loss:0.74757\n",
      "[CSQ][191/2000][16:23:46] bit:1024, dataset:palmprint, training... loss:0.74647\n",
      "[CSQ][192/2000][16:23:54] bit:1024, dataset:palmprint, training... loss:0.74751\n",
      "[CSQ][193/2000][16:24:03] bit:1024, dataset:palmprint, training... loss:0.74665\n",
      "[CSQ][194/2000][16:24:11] bit:1024, dataset:palmprint, training... loss:0.74707\n",
      "[CSQ][195/2000][16:24:18] bit:1024, dataset:palmprint, training... loss:0.74582\n",
      "[CSQ][196/2000][16:24:27] bit:1024, dataset:palmprint, training... loss:0.74659\n",
      "[CSQ][197/2000][16:24:35] bit:1024, dataset:palmprint, training... loss:0.74624\n",
      "[CSQ][198/2000][16:24:43] bit:1024, dataset:palmprint, training... loss:0.74582\n",
      "[CSQ][199/2000][16:24:52] bit:1024, dataset:palmprint, training... loss:0.74594\n",
      "[CSQ][200/2000][16:25:00] bit:1024, dataset:palmprint, training... loss:0.74561\n",
      "[CSQ][201/2000][16:25:08] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.019\n",
      "[CSQ] epoch:201, bit:1024, dataset:palmprint,eer:0.01900, Best eer: 0.01867\n",
      "\b loss:0.74602\n",
      "[CSQ][202/2000][16:26:13] bit:1024, dataset:palmprint, training... loss:0.74528\n",
      "[CSQ][203/2000][16:26:21] bit:1024, dataset:palmprint, training... loss:0.74556\n",
      "[CSQ][204/2000][16:26:29] bit:1024, dataset:palmprint, training... loss:0.74487\n",
      "[CSQ][205/2000][16:26:37] bit:1024, dataset:palmprint, training... loss:0.74537\n",
      "[CSQ][206/2000][16:26:46] bit:1024, dataset:palmprint, training... loss:0.74563\n",
      "[CSQ][207/2000][16:26:54] bit:1024, dataset:palmprint, training... loss:0.74499\n",
      "[CSQ][208/2000][16:27:02] bit:1024, dataset:palmprint, training... loss:0.74548\n",
      "[CSQ][209/2000][16:27:09] bit:1024, dataset:palmprint, training... loss:0.74463\n",
      "[CSQ][210/2000][16:27:18] bit:1024, dataset:palmprint, training... loss:0.74487\n",
      "[CSQ][211/2000][16:27:26] bit:1024, dataset:palmprint, training... loss:0.74581\n",
      "[CSQ][212/2000][16:27:34] bit:1024, dataset:palmprint, training... loss:0.74434\n",
      "[CSQ][213/2000][16:27:43] bit:1024, dataset:palmprint, training... loss:0.74485\n",
      "[CSQ][214/2000][16:27:51] bit:1024, dataset:palmprint, training... loss:0.74458\n",
      "[CSQ][215/2000][16:27:58] bit:1024, dataset:palmprint, training... loss:0.74403\n",
      "[CSQ][216/2000][16:28:07] bit:1024, dataset:palmprint, training... loss:0.74393\n",
      "[CSQ][217/2000][16:28:15] bit:1024, dataset:palmprint, training... loss:0.74386\n",
      "[CSQ][218/2000][16:28:23] bit:1024, dataset:palmprint, training... loss:0.74423\n",
      "[CSQ][219/2000][16:28:32] bit:1024, dataset:palmprint, training... loss:0.74447\n",
      "[CSQ][220/2000][16:28:40] bit:1024, dataset:palmprint, training... loss:0.74379\n",
      "[CSQ][221/2000][16:28:48] bit:1024, dataset:palmprint, training... loss:0.74410\n",
      "[CSQ][222/2000][16:28:57] bit:1024, dataset:palmprint, training... loss:0.74248\n",
      "[CSQ][223/2000][16:29:05] bit:1024, dataset:palmprint, training... loss:0.74370\n",
      "[CSQ][224/2000][16:29:13] bit:1024, dataset:palmprint, training... loss:0.74421\n",
      "[CSQ][225/2000][16:29:22] bit:1024, dataset:palmprint, training... loss:0.74309\n",
      "[CSQ][226/2000][16:29:30] bit:1024, dataset:palmprint, training... loss:0.74283\n",
      "[CSQ][227/2000][16:29:38] bit:1024, dataset:palmprint, training... loss:0.74319\n",
      "[CSQ][228/2000][16:29:46] bit:1024, dataset:palmprint, training... loss:0.74311\n",
      "[CSQ][229/2000][16:29:55] bit:1024, dataset:palmprint, training... loss:0.74273\n",
      "[CSQ][230/2000][16:30:02] bit:1024, dataset:palmprint, training... loss:0.74184\n",
      "[CSQ][231/2000][16:30:11] bit:1024, dataset:palmprint, training... loss:0.74334\n",
      "[CSQ][232/2000][16:30:20] bit:1024, dataset:palmprint, training... loss:0.74209\n",
      "[CSQ][233/2000][16:30:28] bit:1024, dataset:palmprint, training... loss:0.74239\n",
      "[CSQ][234/2000][16:30:40] bit:1024, dataset:palmprint, training... loss:0.74240\n",
      "[CSQ][235/2000][16:30:48] bit:1024, dataset:palmprint, training... loss:0.74267\n",
      "[CSQ][236/2000][16:30:56] bit:1024, dataset:palmprint, training... loss:0.74250\n",
      "[CSQ][237/2000][16:31:03] bit:1024, dataset:palmprint, training... loss:0.74115\n",
      "[CSQ][238/2000][16:31:11] bit:1024, dataset:palmprint, training... loss:0.74191\n",
      "[CSQ][239/2000][16:31:19] bit:1024, dataset:palmprint, training... loss:0.74235\n",
      "[CSQ][240/2000][16:31:27] bit:1024, dataset:palmprint, training... loss:0.74162\n",
      "[CSQ][241/2000][16:31:35] bit:1024, dataset:palmprint, training... loss:0.74142\n",
      "[CSQ][242/2000][16:31:42] bit:1024, dataset:palmprint, training... loss:0.74100\n",
      "[CSQ][243/2000][16:31:50] bit:1024, dataset:palmprint, training... loss:0.74074\n",
      "[CSQ][244/2000][16:31:59] bit:1024, dataset:palmprint, training... loss:0.74226\n",
      "[CSQ][245/2000][16:32:07] bit:1024, dataset:palmprint, training... loss:0.74108\n",
      "[CSQ][246/2000][16:32:15] bit:1024, dataset:palmprint, training... loss:0.74209\n",
      "[CSQ][247/2000][16:32:24] bit:1024, dataset:palmprint, training... loss:0.74173\n",
      "[CSQ][248/2000][16:32:32] bit:1024, dataset:palmprint, training... loss:0.74100\n",
      "[CSQ][249/2000][16:32:40] bit:1024, dataset:palmprint, training... loss:0.74024\n",
      "[CSQ][250/2000][16:32:49] bit:1024, dataset:palmprint, training... loss:0.74075\n",
      "[CSQ][251/2000][16:32:57] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.016\n",
      "[CSQ] epoch:251, bit:1024, dataset:palmprint,eer:0.01600, Best eer: 0.01600\n",
      "\b loss:0.74040\n",
      "[CSQ][252/2000][16:34:11] bit:1024, dataset:palmprint, training... loss:0.74034\n",
      "[CSQ][253/2000][16:34:20] bit:1024, dataset:palmprint, training... loss:0.74043\n",
      "[CSQ][254/2000][16:34:28] bit:1024, dataset:palmprint, training... loss:0.74024\n",
      "[CSQ][255/2000][16:34:37] bit:1024, dataset:palmprint, training... loss:0.74146\n",
      "[CSQ][256/2000][16:34:45] bit:1024, dataset:palmprint, training... loss:0.74089\n",
      "[CSQ][257/2000][16:34:53] bit:1024, dataset:palmprint, training... loss:0.74012\n",
      "[CSQ][258/2000][16:35:01] bit:1024, dataset:palmprint, training... loss:0.73959\n",
      "[CSQ][259/2000][16:35:10] bit:1024, dataset:palmprint, training... loss:0.74080\n",
      "[CSQ][260/2000][16:35:18] bit:1024, dataset:palmprint, training... loss:0.73963\n",
      "[CSQ][261/2000][16:35:26] bit:1024, dataset:palmprint, training... loss:0.73961\n",
      "[CSQ][262/2000][16:35:35] bit:1024, dataset:palmprint, training... loss:0.73885\n",
      "[CSQ][263/2000][16:35:43] bit:1024, dataset:palmprint, training... loss:0.73984\n",
      "[CSQ][264/2000][16:35:51] bit:1024, dataset:palmprint, training... loss:0.73848\n",
      "[CSQ][265/2000][16:35:58] bit:1024, dataset:palmprint, training... loss:0.73969\n",
      "[CSQ][266/2000][16:36:07] bit:1024, dataset:palmprint, training... loss:0.73881\n",
      "[CSQ][267/2000][16:36:15] bit:1024, dataset:palmprint, training... loss:0.73898\n",
      "[CSQ][268/2000][16:36:23] bit:1024, dataset:palmprint, training... loss:0.73984\n",
      "[CSQ][269/2000][16:36:32] bit:1024, dataset:palmprint, training... loss:0.73817\n",
      "[CSQ][270/2000][16:36:39] bit:1024, dataset:palmprint, training... loss:0.73877\n",
      "[CSQ][271/2000][16:36:47] bit:1024, dataset:palmprint, training... loss:0.73884\n",
      "[CSQ][272/2000][16:36:56] bit:1024, dataset:palmprint, training... loss:0.73887\n",
      "[CSQ][273/2000][16:37:04] bit:1024, dataset:palmprint, training... loss:0.73920\n",
      "[CSQ][274/2000][16:37:12] bit:1024, dataset:palmprint, training... loss:0.73869\n",
      "[CSQ][275/2000][16:37:21] bit:1024, dataset:palmprint, training... loss:0.73821\n",
      "[CSQ][276/2000][16:37:29] bit:1024, dataset:palmprint, training... loss:0.73803\n",
      "[CSQ][277/2000][16:37:36] bit:1024, dataset:palmprint, training... loss:0.73855\n",
      "[CSQ][278/2000][16:37:45] bit:1024, dataset:palmprint, training... loss:0.73828\n",
      "[CSQ][279/2000][16:37:53] bit:1024, dataset:palmprint, training... loss:0.73842\n",
      "[CSQ][280/2000][16:38:01] bit:1024, dataset:palmprint, training... loss:0.73752\n",
      "[CSQ][281/2000][16:38:10] bit:1024, dataset:palmprint, training... loss:0.73767\n",
      "[CSQ][282/2000][16:38:18] bit:1024, dataset:palmprint, training... loss:0.73765\n",
      "[CSQ][283/2000][16:38:26] bit:1024, dataset:palmprint, training... loss:0.73713\n",
      "[CSQ][284/2000][16:38:34] bit:1024, dataset:palmprint, training... loss:0.73858\n",
      "[CSQ][285/2000][16:38:43] bit:1024, dataset:palmprint, training... loss:0.73760\n",
      "[CSQ][286/2000][16:38:50] bit:1024, dataset:palmprint, training... loss:0.73722\n",
      "[CSQ][287/2000][16:38:59] bit:1024, dataset:palmprint, training... loss:0.73756\n",
      "[CSQ][288/2000][16:39:07] bit:1024, dataset:palmprint, training... loss:0.73674\n",
      "[CSQ][289/2000][16:39:15] bit:1024, dataset:palmprint, training... loss:0.73699\n",
      "[CSQ][290/2000][16:39:23] bit:1024, dataset:palmprint, training... loss:0.73657\n",
      "[CSQ][291/2000][16:39:31] bit:1024, dataset:palmprint, training... loss:0.73672\n",
      "[CSQ][292/2000][16:39:39] bit:1024, dataset:palmprint, training... loss:0.73736\n",
      "[CSQ][293/2000][16:39:48] bit:1024, dataset:palmprint, training... loss:0.73644\n",
      "[CSQ][294/2000][16:39:56] bit:1024, dataset:palmprint, training... loss:0.73704\n",
      "[CSQ][295/2000][16:40:04] bit:1024, dataset:palmprint, training... loss:0.73680\n",
      "[CSQ][296/2000][16:40:12] bit:1024, dataset:palmprint, training... loss:0.73644\n",
      "[CSQ][297/2000][16:40:20] bit:1024, dataset:palmprint, training... loss:0.73622\n",
      "[CSQ][298/2000][16:40:28] bit:1024, dataset:palmprint, training... loss:0.73696\n",
      "[CSQ][299/2000][16:40:37] bit:1024, dataset:palmprint, training... loss:0.73613\n",
      "[CSQ][300/2000][16:40:45] bit:1024, dataset:palmprint, training... loss:0.73567\n",
      "[CSQ][301/2000][16:40:53] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.014666666666666666\n",
      "[CSQ] epoch:301, bit:1024, dataset:palmprint,eer:0.01467, Best eer: 0.01467\n",
      "\b loss:0.73565\n",
      "[CSQ][302/2000][16:42:13] bit:1024, dataset:palmprint, training... loss:0.73563\n",
      "[CSQ][303/2000][16:42:21] bit:1024, dataset:palmprint, training... loss:0.73575\n",
      "[CSQ][304/2000][16:42:28] bit:1024, dataset:palmprint, training... loss:0.73596\n",
      "[CSQ][305/2000][16:42:36] bit:1024, dataset:palmprint, training... loss:0.73497\n",
      "[CSQ][306/2000][16:42:44] bit:1024, dataset:palmprint, training... loss:0.73546\n",
      "[CSQ][307/2000][16:42:52] bit:1024, dataset:palmprint, training... loss:0.73581\n",
      "[CSQ][308/2000][16:43:00] bit:1024, dataset:palmprint, training... loss:0.73609\n",
      "[CSQ][309/2000][16:43:07] bit:1024, dataset:palmprint, training... loss:0.73535\n",
      "[CSQ][310/2000][16:43:16] bit:1024, dataset:palmprint, training... loss:0.73459\n",
      "[CSQ][311/2000][16:43:25] bit:1024, dataset:palmprint, training... loss:0.73488\n",
      "[CSQ][312/2000][16:43:33] bit:1024, dataset:palmprint, training... loss:0.73551\n",
      "[CSQ][313/2000][16:43:41] bit:1024, dataset:palmprint, training... loss:0.73411\n",
      "[CSQ][314/2000][16:43:49] bit:1024, dataset:palmprint, training... loss:0.73498\n",
      "[CSQ][315/2000][16:43:57] bit:1024, dataset:palmprint, training... loss:0.73503\n",
      "[CSQ][316/2000][16:44:06] bit:1024, dataset:palmprint, training... loss:0.73457\n",
      "[CSQ][317/2000][16:44:14] bit:1024, dataset:palmprint, training... loss:0.73393\n",
      "[CSQ][318/2000][16:44:22] bit:1024, dataset:palmprint, training... loss:0.73377\n",
      "[CSQ][319/2000][16:44:31] bit:1024, dataset:palmprint, training... loss:0.73483\n",
      "[CSQ][320/2000][16:44:39] bit:1024, dataset:palmprint, training... loss:0.73489\n",
      "[CSQ][321/2000][16:44:46] bit:1024, dataset:palmprint, training... loss:0.73439\n",
      "[CSQ][322/2000][16:44:55] bit:1024, dataset:palmprint, training... loss:0.73440\n",
      "[CSQ][323/2000][16:45:03] bit:1024, dataset:palmprint, training... loss:0.73419\n",
      "[CSQ][324/2000][16:45:11] bit:1024, dataset:palmprint, training... loss:0.73414\n",
      "[CSQ][325/2000][16:45:20] bit:1024, dataset:palmprint, training... loss:0.73382\n",
      "[CSQ][326/2000][16:45:28] bit:1024, dataset:palmprint, training... loss:0.73306\n",
      "[CSQ][327/2000][16:45:36] bit:1024, dataset:palmprint, training... loss:0.73475\n",
      "[CSQ][328/2000][16:45:44] bit:1024, dataset:palmprint, training... loss:0.73453\n",
      "[CSQ][329/2000][16:45:52] bit:1024, dataset:palmprint, training... loss:0.73364\n",
      "[CSQ][330/2000][16:46:01] bit:1024, dataset:palmprint, training... loss:0.73385\n",
      "[CSQ][331/2000][16:46:09] bit:1024, dataset:palmprint, training... loss:0.73318\n",
      "[CSQ][332/2000][16:46:17] bit:1024, dataset:palmprint, training... loss:0.73406\n",
      "[CSQ][333/2000][16:46:25] bit:1024, dataset:palmprint, training... loss:0.73256\n",
      "[CSQ][334/2000][16:46:34] bit:1024, dataset:palmprint, training... loss:0.73330\n",
      "[CSQ][335/2000][16:46:42] bit:1024, dataset:palmprint, training... loss:0.73295\n",
      "[CSQ][336/2000][16:46:50] bit:1024, dataset:palmprint, training... loss:0.73241\n",
      "[CSQ][337/2000][16:46:58] bit:1024, dataset:palmprint, training... loss:0.73375\n",
      "[CSQ][338/2000][16:47:06] bit:1024, dataset:palmprint, training... loss:0.73324\n",
      "[CSQ][339/2000][16:47:15] bit:1024, dataset:palmprint, training... loss:0.73267\n",
      "[CSQ][340/2000][16:47:23] bit:1024, dataset:palmprint, training... loss:0.73321\n",
      "[CSQ][341/2000][16:47:31] bit:1024, dataset:palmprint, training... loss:0.73230\n",
      "[CSQ][342/2000][16:47:39] bit:1024, dataset:palmprint, training... loss:0.73279\n",
      "[CSQ][343/2000][16:47:47] bit:1024, dataset:palmprint, training... loss:0.73170\n",
      "[CSQ][344/2000][16:47:55] bit:1024, dataset:palmprint, training... loss:0.73363\n",
      "[CSQ][345/2000][16:48:04] bit:1024, dataset:palmprint, training... loss:0.73264\n",
      "[CSQ][346/2000][16:48:12] bit:1024, dataset:palmprint, training... loss:0.73222\n",
      "[CSQ][347/2000][16:48:20] bit:1024, dataset:palmprint, training... loss:0.73300\n",
      "[CSQ][348/2000][16:48:28] bit:1024, dataset:palmprint, training... loss:0.73122\n",
      "[CSQ][349/2000][16:48:36] bit:1024, dataset:palmprint, training... loss:0.73206\n",
      "[CSQ][350/2000][16:48:44] bit:1024, dataset:palmprint, training... loss:0.73150\n",
      "[CSQ][351/2000][16:48:52] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.013333333333333334\n",
      "[CSQ] epoch:351, bit:1024, dataset:palmprint,eer:0.01333, Best eer: 0.01333\n",
      "\b loss:0.73079\n",
      "[CSQ][352/2000][16:50:07] bit:1024, dataset:palmprint, training... loss:0.73172\n",
      "[CSQ][353/2000][16:50:14] bit:1024, dataset:palmprint, training... loss:0.73066\n",
      "[CSQ][354/2000][16:50:23] bit:1024, dataset:palmprint, training... loss:0.73132\n",
      "[CSQ][355/2000][16:50:32] bit:1024, dataset:palmprint, training... loss:0.73291\n",
      "[CSQ][356/2000][16:50:40] bit:1024, dataset:palmprint, training... loss:0.73171\n",
      "[CSQ][357/2000][16:50:48] bit:1024, dataset:palmprint, training... loss:0.73043\n",
      "[CSQ][358/2000][16:50:57] bit:1024, dataset:palmprint, training... loss:0.73044\n",
      "[CSQ][359/2000][16:51:05] bit:1024, dataset:palmprint, training... loss:0.73090\n",
      "[CSQ][360/2000][16:51:14] bit:1024, dataset:palmprint, training... loss:0.73092\n",
      "[CSQ][361/2000][16:51:22] bit:1024, dataset:palmprint, training... loss:0.73148\n",
      "[CSQ][362/2000][16:51:30] bit:1024, dataset:palmprint, training... loss:0.72987\n",
      "[CSQ][363/2000][16:51:38] bit:1024, dataset:palmprint, training... loss:0.72979\n",
      "[CSQ][364/2000][16:51:47] bit:1024, dataset:palmprint, training... loss:0.73133\n",
      "[CSQ][365/2000][16:51:55] bit:1024, dataset:palmprint, training... loss:0.73137\n",
      "[CSQ][366/2000][16:52:03] bit:1024, dataset:palmprint, training... loss:0.73039\n",
      "[CSQ][367/2000][16:52:11] bit:1024, dataset:palmprint, training... loss:0.73020\n",
      "[CSQ][368/2000][16:52:19] bit:1024, dataset:palmprint, training... loss:0.73033\n",
      "[CSQ][369/2000][16:52:27] bit:1024, dataset:palmprint, training... loss:0.73100\n",
      "[CSQ][370/2000][16:52:35] bit:1024, dataset:palmprint, training... loss:0.72943\n",
      "[CSQ][371/2000][16:52:44] bit:1024, dataset:palmprint, training... loss:0.73047\n",
      "[CSQ][372/2000][16:52:52] bit:1024, dataset:palmprint, training... loss:0.72997\n",
      "[CSQ][373/2000][16:52:59] bit:1024, dataset:palmprint, training... loss:0.72926\n",
      "[CSQ][374/2000][16:53:08] bit:1024, dataset:palmprint, training... loss:0.72970\n",
      "[CSQ][375/2000][16:53:19] bit:1024, dataset:palmprint, training... loss:0.72975\n",
      "[CSQ][376/2000][16:53:27] bit:1024, dataset:palmprint, training... loss:0.72948\n",
      "[CSQ][377/2000][16:53:35] bit:1024, dataset:palmprint, training... loss:0.72846\n",
      "[CSQ][378/2000][16:53:42] bit:1024, dataset:palmprint, training... loss:0.72867\n",
      "[CSQ][379/2000][16:53:50] bit:1024, dataset:palmprint, training... loss:0.73020\n",
      "[CSQ][380/2000][16:53:58] bit:1024, dataset:palmprint, training... loss:0.72877\n",
      "[CSQ][381/2000][16:54:06] bit:1024, dataset:palmprint, training... loss:0.72888\n",
      "[CSQ][382/2000][16:54:14] bit:1024, dataset:palmprint, training... loss:0.72845\n",
      "[CSQ][383/2000][16:54:22] bit:1024, dataset:palmprint, training... loss:0.72961\n",
      "[CSQ][384/2000][16:54:30] bit:1024, dataset:palmprint, training... loss:0.72893\n",
      "[CSQ][385/2000][16:54:38] bit:1024, dataset:palmprint, training... loss:0.72869\n",
      "[CSQ][386/2000][16:54:47] bit:1024, dataset:palmprint, training... loss:0.72855\n",
      "[CSQ][387/2000][16:54:55] bit:1024, dataset:palmprint, training... loss:0.72772\n",
      "[CSQ][388/2000][16:55:03] bit:1024, dataset:palmprint, training... loss:0.72972\n",
      "[CSQ][389/2000][16:55:11] bit:1024, dataset:palmprint, training... loss:0.72760\n",
      "[CSQ][390/2000][16:55:19] bit:1024, dataset:palmprint, training... loss:0.72834\n",
      "[CSQ][391/2000][16:55:28] bit:1024, dataset:palmprint, training... loss:0.72809\n",
      "[CSQ][392/2000][16:55:36] bit:1024, dataset:palmprint, training... loss:0.72815\n",
      "[CSQ][393/2000][16:55:44] bit:1024, dataset:palmprint, training... loss:0.72818\n",
      "[CSQ][394/2000][16:55:51] bit:1024, dataset:palmprint, training... loss:0.72772\n",
      "[CSQ][395/2000][16:56:00] bit:1024, dataset:palmprint, training... loss:0.72943\n",
      "[CSQ][396/2000][16:56:08] bit:1024, dataset:palmprint, training... loss:0.72740\n",
      "[CSQ][397/2000][16:56:16] bit:1024, dataset:palmprint, training... loss:0.72755\n",
      "[CSQ][398/2000][16:56:25] bit:1024, dataset:palmprint, training... loss:0.72811\n",
      "[CSQ][399/2000][16:56:33] bit:1024, dataset:palmprint, training... loss:0.72731\n",
      "[CSQ][400/2000][16:56:40] bit:1024, dataset:palmprint, training... loss:0.72716\n",
      "[CSQ][401/2000][16:56:49] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.011\n",
      "[CSQ] epoch:401, bit:1024, dataset:palmprint,eer:0.01100, Best eer: 0.01100\n",
      "\b loss:0.72657\n",
      "[CSQ][402/2000][16:58:08] bit:1024, dataset:palmprint, training... loss:0.72662\n",
      "[CSQ][403/2000][16:58:16] bit:1024, dataset:palmprint, training... loss:0.72641\n",
      "[CSQ][404/2000][16:58:24] bit:1024, dataset:palmprint, training... loss:0.72631\n",
      "[CSQ][405/2000][16:58:33] bit:1024, dataset:palmprint, training... loss:0.72766\n",
      "[CSQ][406/2000][16:58:41] bit:1024, dataset:palmprint, training... loss:0.72809\n",
      "[CSQ][407/2000][16:58:49] bit:1024, dataset:palmprint, training... loss:0.72729\n",
      "[CSQ][408/2000][16:58:57] bit:1024, dataset:palmprint, training... loss:0.72633\n",
      "[CSQ][409/2000][16:59:05] bit:1024, dataset:palmprint, training... loss:0.72704\n",
      "[CSQ][410/2000][16:59:13] bit:1024, dataset:palmprint, training... loss:0.72625\n",
      "[CSQ][411/2000][16:59:21] bit:1024, dataset:palmprint, training... loss:0.72718\n",
      "[CSQ][412/2000][16:59:30] bit:1024, dataset:palmprint, training... loss:0.72714\n",
      "[CSQ][413/2000][16:59:38] bit:1024, dataset:palmprint, training... loss:0.72543\n",
      "[CSQ][414/2000][16:59:46] bit:1024, dataset:palmprint, training... loss:0.72648\n",
      "[CSQ][415/2000][16:59:55] bit:1024, dataset:palmprint, training... loss:0.72678\n",
      "[CSQ][416/2000][17:00:03] bit:1024, dataset:palmprint, training... loss:0.72697\n",
      "[CSQ][417/2000][17:00:10] bit:1024, dataset:palmprint, training... loss:0.72609\n",
      "[CSQ][418/2000][17:00:19] bit:1024, dataset:palmprint, training... loss:0.72578\n",
      "[CSQ][419/2000][17:00:27] bit:1024, dataset:palmprint, training... loss:0.72646\n",
      "[CSQ][420/2000][17:00:35] bit:1024, dataset:palmprint, training... loss:0.72673\n",
      "[CSQ][421/2000][17:00:44] bit:1024, dataset:palmprint, training... loss:0.72630\n",
      "[CSQ][422/2000][17:00:52] bit:1024, dataset:palmprint, training... loss:0.72569\n",
      "[CSQ][423/2000][17:01:00] bit:1024, dataset:palmprint, training... loss:0.72546\n",
      "[CSQ][424/2000][17:01:09] bit:1024, dataset:palmprint, training... loss:0.72437\n",
      "[CSQ][425/2000][17:01:17] bit:1024, dataset:palmprint, training... loss:0.72629\n",
      "[CSQ][426/2000][17:01:25] bit:1024, dataset:palmprint, training... loss:0.72485\n",
      "[CSQ][427/2000][17:01:33] bit:1024, dataset:palmprint, training... loss:0.72670\n",
      "[CSQ][428/2000][17:01:41] bit:1024, dataset:palmprint, training... loss:0.72602\n",
      "[CSQ][429/2000][17:01:51] bit:1024, dataset:palmprint, training... loss:0.72537\n",
      "[CSQ][430/2000][17:02:01] bit:1024, dataset:palmprint, training... loss:0.72502\n",
      "[CSQ][431/2000][17:02:16] bit:1024, dataset:palmprint, training... loss:0.72482\n",
      "[CSQ][432/2000][17:02:32] bit:1024, dataset:palmprint, training... loss:0.72298\n",
      "[CSQ][433/2000][17:02:48] bit:1024, dataset:palmprint, training... loss:0.72479\n",
      "[CSQ][434/2000][17:03:03] bit:1024, dataset:palmprint, training... loss:0.72391\n",
      "[CSQ][435/2000][17:03:18] bit:1024, dataset:palmprint, training... loss:0.72465\n",
      "[CSQ][436/2000][17:03:30] bit:1024, dataset:palmprint, training... loss:0.72375\n",
      "[CSQ][437/2000][17:03:38] bit:1024, dataset:palmprint, training... loss:0.72437\n",
      "[CSQ][438/2000][17:03:46] bit:1024, dataset:palmprint, training... loss:0.72426\n",
      "[CSQ][439/2000][17:03:53] bit:1024, dataset:palmprint, training... loss:0.72476\n",
      "[CSQ][440/2000][17:04:01] bit:1024, dataset:palmprint, training... loss:0.72383\n",
      "[CSQ][441/2000][17:04:09] bit:1024, dataset:palmprint, training... loss:0.72476\n",
      "[CSQ][442/2000][17:04:16] bit:1024, dataset:palmprint, training... loss:0.72329\n",
      "[CSQ][443/2000][17:04:24] bit:1024, dataset:palmprint, training... loss:0.72400\n",
      "[CSQ][444/2000][17:04:32] bit:1024, dataset:palmprint, training... loss:0.72419\n",
      "[CSQ][445/2000][17:04:39] bit:1024, dataset:palmprint, training... loss:0.72428\n",
      "[CSQ][446/2000][17:04:47] bit:1024, dataset:palmprint, training... loss:0.72480\n",
      "[CSQ][447/2000][17:04:55] bit:1024, dataset:palmprint, training... loss:0.72283\n",
      "[CSQ][448/2000][17:05:02] bit:1024, dataset:palmprint, training... loss:0.72268\n",
      "[CSQ][449/2000][17:05:10] bit:1024, dataset:palmprint, training... loss:0.72230\n",
      "[CSQ][450/2000][17:05:18] bit:1024, dataset:palmprint, training... loss:0.72290\n",
      "[CSQ][451/2000][17:05:26] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.011\n",
      "[CSQ] epoch:451, bit:1024, dataset:palmprint,eer:0.01100, Best eer: 0.01100\n",
      "\b loss:0.72225\n",
      "[CSQ][452/2000][17:07:14] bit:1024, dataset:palmprint, training... loss:0.72351\n",
      "[CSQ][453/2000][17:07:22] bit:1024, dataset:palmprint, training... loss:0.72302\n",
      "[CSQ][454/2000][17:07:30] bit:1024, dataset:palmprint, training... loss:0.72220\n",
      "[CSQ][455/2000][17:07:39] bit:1024, dataset:palmprint, training... loss:0.72303\n",
      "[CSQ][456/2000][17:07:48] bit:1024, dataset:palmprint, training... loss:0.72306\n",
      "[CSQ][457/2000][17:07:57] bit:1024, dataset:palmprint, training... loss:0.72184\n",
      "[CSQ][458/2000][17:08:05] bit:1024, dataset:palmprint, training... loss:0.72329\n",
      "[CSQ][459/2000][17:08:15] bit:1024, dataset:palmprint, training... loss:0.72191\n",
      "[CSQ][460/2000][17:08:24] bit:1024, dataset:palmprint, training... loss:0.72175\n",
      "[CSQ][461/2000][17:08:33] bit:1024, dataset:palmprint, training... loss:0.72321\n",
      "[CSQ][462/2000][17:08:41] bit:1024, dataset:palmprint, training... loss:0.72343\n",
      "[CSQ][463/2000][17:08:50] bit:1024, dataset:palmprint, training... loss:0.72201\n",
      "[CSQ][464/2000][17:08:59] bit:1024, dataset:palmprint, training... loss:0.72269\n",
      "[CSQ][465/2000][17:09:07] bit:1024, dataset:palmprint, training... loss:0.72265\n",
      "[CSQ][466/2000][17:09:16] bit:1024, dataset:palmprint, training... loss:0.72191\n",
      "[CSQ][467/2000][17:09:25] bit:1024, dataset:palmprint, training... loss:0.72141\n",
      "[CSQ][468/2000][17:09:33] bit:1024, dataset:palmprint, training... loss:0.72141\n",
      "[CSQ][469/2000][17:09:42] bit:1024, dataset:palmprint, training... loss:0.72204\n",
      "[CSQ][470/2000][17:09:50] bit:1024, dataset:palmprint, training... loss:0.72087\n",
      "[CSQ][471/2000][17:09:59] bit:1024, dataset:palmprint, training... loss:0.72095\n",
      "[CSQ][472/2000][17:10:08] bit:1024, dataset:palmprint, training... loss:0.72145\n",
      "[CSQ][473/2000][17:10:16] bit:1024, dataset:palmprint, training... loss:0.72194\n",
      "[CSQ][474/2000][17:10:25] bit:1024, dataset:palmprint, training... loss:0.72275\n",
      "[CSQ][475/2000][17:10:33] bit:1024, dataset:palmprint, training... loss:0.72283\n",
      "[CSQ][476/2000][17:10:41] bit:1024, dataset:palmprint, training... loss:0.72150\n",
      "[CSQ][477/2000][17:10:49] bit:1024, dataset:palmprint, training... loss:0.72092\n",
      "[CSQ][478/2000][17:10:59] bit:1024, dataset:palmprint, training... loss:0.72043\n",
      "[CSQ][479/2000][17:11:07] bit:1024, dataset:palmprint, training... loss:0.72054\n",
      "[CSQ][480/2000][17:11:16] bit:1024, dataset:palmprint, training... loss:0.71987\n",
      "[CSQ][481/2000][17:11:24] bit:1024, dataset:palmprint, training... loss:0.72055\n",
      "[CSQ][482/2000][17:11:33] bit:1024, dataset:palmprint, training... loss:0.72109\n",
      "[CSQ][483/2000][17:11:41] bit:1024, dataset:palmprint, training... loss:0.72010\n",
      "[CSQ][484/2000][17:11:49] bit:1024, dataset:palmprint, training... loss:0.72125\n",
      "[CSQ][485/2000][17:11:58] bit:1024, dataset:palmprint, training... loss:0.72100\n",
      "[CSQ][486/2000][17:12:06] bit:1024, dataset:palmprint, training... loss:0.72196\n",
      "[CSQ][487/2000][17:12:16] bit:1024, dataset:palmprint, training... loss:0.72054\n",
      "[CSQ][488/2000][17:12:24] bit:1024, dataset:palmprint, training... loss:0.72027\n",
      "[CSQ][489/2000][17:12:32] bit:1024, dataset:palmprint, training... loss:0.72120\n",
      "[CSQ][490/2000][17:12:40] bit:1024, dataset:palmprint, training... loss:0.71882\n",
      "[CSQ][491/2000][17:12:48] bit:1024, dataset:palmprint, training... loss:0.71884\n",
      "[CSQ][492/2000][17:12:57] bit:1024, dataset:palmprint, training... loss:0.72053\n",
      "[CSQ][493/2000][17:13:05] bit:1024, dataset:palmprint, training... loss:0.71936\n",
      "[CSQ][494/2000][17:13:15] bit:1024, dataset:palmprint, training... loss:0.72018\n",
      "[CSQ][495/2000][17:13:24] bit:1024, dataset:palmprint, training... loss:0.71843\n",
      "[CSQ][496/2000][17:13:32] bit:1024, dataset:palmprint, training... loss:0.72004\n",
      "[CSQ][497/2000][17:13:40] bit:1024, dataset:palmprint, training... loss:0.71944\n",
      "[CSQ][498/2000][17:13:48] bit:1024, dataset:palmprint, training... loss:0.71920\n",
      "[CSQ][499/2000][17:13:57] bit:1024, dataset:palmprint, training... loss:0.71923\n",
      "[CSQ][500/2000][17:14:05] bit:1024, dataset:palmprint, training... loss:0.71958\n",
      "[CSQ][501/2000][17:14:15] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.008666666666666666\n",
      "[CSQ] epoch:501, bit:1024, dataset:palmprint,eer:0.00867, Best eer: 0.00867\n",
      "\b loss:0.71957\n",
      "[CSQ][502/2000][17:15:32] bit:1024, dataset:palmprint, training... loss:0.71982\n",
      "[CSQ][503/2000][17:15:41] bit:1024, dataset:palmprint, training... loss:0.71867\n",
      "[CSQ][504/2000][17:15:50] bit:1024, dataset:palmprint, training... loss:0.71942\n",
      "[CSQ][505/2000][17:15:59] bit:1024, dataset:palmprint, training... loss:0.71860\n",
      "[CSQ][506/2000][17:16:08] bit:1024, dataset:palmprint, training... loss:0.71638\n",
      "[CSQ][507/2000][17:16:16] bit:1024, dataset:palmprint, training... loss:0.72024\n",
      "[CSQ][508/2000][17:16:26] bit:1024, dataset:palmprint, training... loss:0.71953\n",
      "[CSQ][509/2000][17:16:35] bit:1024, dataset:palmprint, training... loss:0.71789\n",
      "[CSQ][510/2000][17:16:44] bit:1024, dataset:palmprint, training... loss:0.71732\n",
      "[CSQ][511/2000][17:16:52] bit:1024, dataset:palmprint, training... loss:0.71800\n",
      "[CSQ][512/2000][17:17:01] bit:1024, dataset:palmprint, training... loss:0.71616\n",
      "[CSQ][513/2000][17:17:10] bit:1024, dataset:palmprint, training... loss:0.71911\n",
      "[CSQ][514/2000][17:17:19] bit:1024, dataset:palmprint, training... loss:0.71846\n",
      "[CSQ][515/2000][17:17:28] bit:1024, dataset:palmprint, training... loss:0.71814\n",
      "[CSQ][516/2000][17:17:37] bit:1024, dataset:palmprint, training... loss:0.71673\n",
      "[CSQ][517/2000][17:17:45] bit:1024, dataset:palmprint, training... loss:0.71753\n",
      "[CSQ][518/2000][17:17:54] bit:1024, dataset:palmprint, training... loss:0.71773\n",
      "[CSQ][519/2000][17:18:03] bit:1024, dataset:palmprint, training... loss:0.71799\n",
      "[CSQ][520/2000][17:18:12] bit:1024, dataset:palmprint, training... loss:0.71649\n",
      "[CSQ][521/2000][17:18:21] bit:1024, dataset:palmprint, training... loss:0.71670\n",
      "[CSQ][522/2000][17:18:30] bit:1024, dataset:palmprint, training... loss:0.71583\n",
      "[CSQ][523/2000][17:18:39] bit:1024, dataset:palmprint, training... loss:0.71771\n",
      "[CSQ][524/2000][17:18:47] bit:1024, dataset:palmprint, training... loss:0.71759\n",
      "[CSQ][525/2000][17:18:57] bit:1024, dataset:palmprint, training... loss:0.71712\n",
      "[CSQ][526/2000][17:19:06] bit:1024, dataset:palmprint, training... loss:0.71449\n",
      "[CSQ][527/2000][17:19:15] bit:1024, dataset:palmprint, training... loss:0.71604\n",
      "[CSQ][528/2000][17:19:24] bit:1024, dataset:palmprint, training... loss:0.71669\n",
      "[CSQ][529/2000][17:19:32] bit:1024, dataset:palmprint, training... loss:0.71672\n",
      "[CSQ][530/2000][17:19:42] bit:1024, dataset:palmprint, training... loss:0.71602\n",
      "[CSQ][531/2000][17:19:51] bit:1024, dataset:palmprint, training... loss:0.71732\n",
      "[CSQ][532/2000][17:20:00] bit:1024, dataset:palmprint, training... loss:0.71565\n",
      "[CSQ][533/2000][17:20:09] bit:1024, dataset:palmprint, training... loss:0.71616\n",
      "[CSQ][534/2000][17:20:16] bit:1024, dataset:palmprint, training... loss:0.71540\n",
      "[CSQ][535/2000][17:20:24] bit:1024, dataset:palmprint, training... loss:0.71619\n",
      "[CSQ][536/2000][17:20:32] bit:1024, dataset:palmprint, training... loss:0.71531\n",
      "[CSQ][537/2000][17:20:41] bit:1024, dataset:palmprint, training... loss:0.71635\n",
      "[CSQ][538/2000][17:20:49] bit:1024, dataset:palmprint, training... loss:0.71605\n",
      "[CSQ][539/2000][17:20:58] bit:1024, dataset:palmprint, training... loss:0.71608\n",
      "[CSQ][540/2000][17:21:06] bit:1024, dataset:palmprint, training... loss:0.71425\n",
      "[CSQ][541/2000][17:21:15] bit:1024, dataset:palmprint, training... loss:0.71551\n",
      "[CSQ][542/2000][17:21:23] bit:1024, dataset:palmprint, training... loss:0.71450\n",
      "[CSQ][543/2000][17:21:31] bit:1024, dataset:palmprint, training... loss:0.71595\n",
      "[CSQ][544/2000][17:21:40] bit:1024, dataset:palmprint, training... loss:0.71646\n",
      "[CSQ][545/2000][17:21:48] bit:1024, dataset:palmprint, training... loss:0.71561\n",
      "[CSQ][546/2000][17:21:56] bit:1024, dataset:palmprint, training... loss:0.71510\n",
      "[CSQ][547/2000][17:22:04] bit:1024, dataset:palmprint, training... loss:0.71485\n",
      "[CSQ][548/2000][17:22:14] bit:1024, dataset:palmprint, training... loss:0.71640\n",
      "[CSQ][549/2000][17:22:22] bit:1024, dataset:palmprint, training... loss:0.71429\n",
      "[CSQ][550/2000][17:22:31] bit:1024, dataset:palmprint, training... loss:0.71398\n",
      "[CSQ][551/2000][17:22:39] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.008333333333333333\n",
      "[CSQ] epoch:551, bit:1024, dataset:palmprint,eer:0.00833, Best eer: 0.00833\n",
      "\b loss:0.71380\n",
      "[CSQ][552/2000][17:23:56] bit:1024, dataset:palmprint, training... loss:0.71524\n",
      "[CSQ][553/2000][17:24:04] bit:1024, dataset:palmprint, training... loss:0.71442\n",
      "[CSQ][554/2000][17:24:12] bit:1024, dataset:palmprint, training... loss:0.71450\n",
      "[CSQ][555/2000][17:24:21] bit:1024, dataset:palmprint, training... loss:0.71511\n",
      "[CSQ][556/2000][17:24:29] bit:1024, dataset:palmprint, training... loss:0.71302\n",
      "[CSQ][557/2000][17:24:38] bit:1024, dataset:palmprint, training... loss:0.71569\n",
      "[CSQ][558/2000][17:24:46] bit:1024, dataset:palmprint, training... loss:0.71439\n",
      "[CSQ][559/2000][17:24:55] bit:1024, dataset:palmprint, training... loss:0.71467\n",
      "[CSQ][560/2000][17:25:04] bit:1024, dataset:palmprint, training... loss:0.71301\n",
      "[CSQ][561/2000][17:25:12] bit:1024, dataset:palmprint, training... loss:0.71441\n",
      "[CSQ][562/2000][17:25:20] bit:1024, dataset:palmprint, training... loss:0.71288\n",
      "[CSQ][563/2000][17:25:28] bit:1024, dataset:palmprint, training... loss:0.71401\n",
      "[CSQ][564/2000][17:25:37] bit:1024, dataset:palmprint, training... loss:0.71310\n",
      "[CSQ][565/2000][17:25:45] bit:1024, dataset:palmprint, training... loss:0.71208\n",
      "[CSQ][566/2000][17:25:54] bit:1024, dataset:palmprint, training... loss:0.71411\n",
      "[CSQ][567/2000][17:26:03] bit:1024, dataset:palmprint, training... loss:0.71318\n",
      "[CSQ][568/2000][17:26:11] bit:1024, dataset:palmprint, training... loss:0.71139\n",
      "[CSQ][569/2000][17:26:19] bit:1024, dataset:palmprint, training... loss:0.71327\n",
      "[CSQ][570/2000][17:26:27] bit:1024, dataset:palmprint, training... loss:0.71372\n",
      "[CSQ][571/2000][17:26:36] bit:1024, dataset:palmprint, training... loss:0.71315\n",
      "[CSQ][572/2000][17:26:44] bit:1024, dataset:palmprint, training... loss:0.71400\n",
      "[CSQ][573/2000][17:26:53] bit:1024, dataset:palmprint, training... loss:0.71442\n",
      "[CSQ][574/2000][17:27:02] bit:1024, dataset:palmprint, training... loss:0.71159\n",
      "[CSQ][575/2000][17:27:10] bit:1024, dataset:palmprint, training... loss:0.71232\n",
      "[CSQ][576/2000][17:27:19] bit:1024, dataset:palmprint, training... loss:0.71318\n",
      "[CSQ][577/2000][17:27:27] bit:1024, dataset:palmprint, training... loss:0.71250\n",
      "[CSQ][578/2000][17:27:36] bit:1024, dataset:palmprint, training... loss:0.71091\n",
      "[CSQ][579/2000][17:27:43] bit:1024, dataset:palmprint, training... loss:0.71324\n",
      "[CSQ][580/2000][17:27:52] bit:1024, dataset:palmprint, training... loss:0.71343\n",
      "[CSQ][581/2000][17:28:00] bit:1024, dataset:palmprint, training... loss:0.71357\n",
      "[CSQ][582/2000][17:28:10] bit:1024, dataset:palmprint, training... loss:0.71257\n",
      "[CSQ][583/2000][17:28:19] bit:1024, dataset:palmprint, training... loss:0.71326\n",
      "[CSQ][584/2000][17:28:27] bit:1024, dataset:palmprint, training... loss:0.71365\n",
      "[CSQ][585/2000][17:28:35] bit:1024, dataset:palmprint, training... loss:0.71164\n",
      "[CSQ][586/2000][17:28:43] bit:1024, dataset:palmprint, training... loss:0.71142\n",
      "[CSQ][587/2000][17:28:52] bit:1024, dataset:palmprint, training... loss:0.71201\n",
      "[CSQ][588/2000][17:29:00] bit:1024, dataset:palmprint, training... loss:0.71308\n",
      "[CSQ][589/2000][17:29:09] bit:1024, dataset:palmprint, training... loss:0.70969\n",
      "[CSQ][590/2000][17:29:17] bit:1024, dataset:palmprint, training... loss:0.71233\n",
      "[CSQ][591/2000][17:29:27] bit:1024, dataset:palmprint, training... loss:0.71219\n",
      "[CSQ][592/2000][17:29:35] bit:1024, dataset:palmprint, training... loss:0.71169\n",
      "[CSQ][593/2000][17:29:44] bit:1024, dataset:palmprint, training... loss:0.71074\n",
      "[CSQ][594/2000][17:29:52] bit:1024, dataset:palmprint, training... loss:0.71279\n",
      "[CSQ][595/2000][17:30:00] bit:1024, dataset:palmprint, training... loss:0.71075\n",
      "[CSQ][596/2000][17:30:09] bit:1024, dataset:palmprint, training... loss:0.71040\n",
      "[CSQ][597/2000][17:30:17] bit:1024, dataset:palmprint, training... loss:0.71047\n",
      "[CSQ][598/2000][17:30:26] bit:1024, dataset:palmprint, training... loss:0.71165\n",
      "[CSQ][599/2000][17:30:35] bit:1024, dataset:palmprint, training... loss:0.71091\n",
      "[CSQ][600/2000][17:30:43] bit:1024, dataset:palmprint, training... loss:0.71338\n",
      "[CSQ][601/2000][17:30:51] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.008333333333333333\n",
      "[CSQ] epoch:601, bit:1024, dataset:palmprint,eer:0.00833, Best eer: 0.00833\n",
      "\b loss:0.70999\n",
      "[CSQ][602/2000][17:31:57] bit:1024, dataset:palmprint, training... loss:0.71080\n",
      "[CSQ][603/2000][17:32:05] bit:1024, dataset:palmprint, training... loss:0.70935\n",
      "[CSQ][604/2000][17:32:13] bit:1024, dataset:palmprint, training... loss:0.71038\n",
      "[CSQ][605/2000][17:32:21] bit:1024, dataset:palmprint, training... loss:0.71130\n",
      "[CSQ][606/2000][17:32:29] bit:1024, dataset:palmprint, training... loss:0.71209\n",
      "[CSQ][607/2000][17:32:38] bit:1024, dataset:palmprint, training... loss:0.71064\n",
      "[CSQ][608/2000][17:32:46] bit:1024, dataset:palmprint, training... loss:0.71179\n",
      "[CSQ][609/2000][17:32:55] bit:1024, dataset:palmprint, training... loss:0.70968\n",
      "[CSQ][610/2000][17:33:05] bit:1024, dataset:palmprint, training... loss:0.71075\n",
      "[CSQ][611/2000][17:33:13] bit:1024, dataset:palmprint, training... loss:0.71108\n",
      "[CSQ][612/2000][17:33:21] bit:1024, dataset:palmprint, training... loss:0.70975\n",
      "[CSQ][613/2000][17:33:28] bit:1024, dataset:palmprint, training... loss:0.71034\n",
      "[CSQ][614/2000][17:33:37] bit:1024, dataset:palmprint, training... loss:0.70975\n",
      "[CSQ][615/2000][17:33:45] bit:1024, dataset:palmprint, training... loss:0.70933\n",
      "[CSQ][616/2000][17:33:53] bit:1024, dataset:palmprint, training... loss:0.70791\n",
      "[CSQ][617/2000][17:34:01] bit:1024, dataset:palmprint, training... loss:0.70950\n",
      "[CSQ][618/2000][17:34:10] bit:1024, dataset:palmprint, training... loss:0.70777\n",
      "[CSQ][619/2000][17:34:18] bit:1024, dataset:palmprint, training... loss:0.70882\n",
      "[CSQ][620/2000][17:34:27] bit:1024, dataset:palmprint, training... loss:0.70815\n",
      "[CSQ][621/2000][17:34:36] bit:1024, dataset:palmprint, training... loss:0.70916\n",
      "[CSQ][622/2000][17:34:44] bit:1024, dataset:palmprint, training... loss:0.70824\n",
      "[CSQ][623/2000][17:34:52] bit:1024, dataset:palmprint, training... loss:0.70929\n",
      "[CSQ][624/2000][17:34:59] bit:1024, dataset:palmprint, training... loss:0.70969\n",
      "[CSQ][625/2000][17:35:08] bit:1024, dataset:palmprint, training... loss:0.70900\n",
      "[CSQ][626/2000][17:35:16] bit:1024, dataset:palmprint, training... loss:0.70905\n",
      "[CSQ][627/2000][17:35:26] bit:1024, dataset:palmprint, training... loss:0.70954\n",
      "[CSQ][628/2000][17:35:35] bit:1024, dataset:palmprint, training... loss:0.70882\n",
      "[CSQ][629/2000][17:35:43] bit:1024, dataset:palmprint, training... loss:0.70775\n",
      "[CSQ][630/2000][17:35:51] bit:1024, dataset:palmprint, training... loss:0.70791\n",
      "[CSQ][631/2000][17:35:59] bit:1024, dataset:palmprint, training... loss:0.70911\n",
      "[CSQ][632/2000][17:36:08] bit:1024, dataset:palmprint, training... loss:0.70883\n",
      "[CSQ][633/2000][17:36:16] bit:1024, dataset:palmprint, training... loss:0.70815\n",
      "[CSQ][634/2000][17:36:26] bit:1024, dataset:palmprint, training... loss:0.70862\n",
      "[CSQ][635/2000][17:36:34] bit:1024, dataset:palmprint, training... loss:0.70898\n",
      "[CSQ][636/2000][17:36:42] bit:1024, dataset:palmprint, training... loss:0.70821\n",
      "[CSQ][637/2000][17:36:50] bit:1024, dataset:palmprint, training... loss:0.70839\n",
      "[CSQ][638/2000][17:36:58] bit:1024, dataset:palmprint, training... loss:0.70823\n",
      "[CSQ][639/2000][17:37:07] bit:1024, dataset:palmprint, training... loss:0.70731\n",
      "[CSQ][640/2000][17:37:15] bit:1024, dataset:palmprint, training... loss:0.70842\n",
      "[CSQ][641/2000][17:37:24] bit:1024, dataset:palmprint, training... loss:0.70729\n",
      "[CSQ][642/2000][17:37:32] bit:1024, dataset:palmprint, training... loss:0.70654\n",
      "[CSQ][643/2000][17:37:41] bit:1024, dataset:palmprint, training... loss:0.70715\n",
      "[CSQ][644/2000][17:37:49] bit:1024, dataset:palmprint, training... loss:0.70802\n",
      "[CSQ][645/2000][17:37:58] bit:1024, dataset:palmprint, training... loss:0.70626\n",
      "[CSQ][646/2000][17:38:07] bit:1024, dataset:palmprint, training... loss:0.70686\n",
      "[CSQ][647/2000][17:38:15] bit:1024, dataset:palmprint, training... loss:0.70702\n",
      "[CSQ][648/2000][17:38:24] bit:1024, dataset:palmprint, training... loss:0.70811\n",
      "[CSQ][649/2000][17:38:33] bit:1024, dataset:palmprint, training... loss:0.70502\n",
      "[CSQ][650/2000][17:38:41] bit:1024, dataset:palmprint, training... loss:0.70650\n",
      "[CSQ][651/2000][17:38:49] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.006\n",
      "[CSQ] epoch:651, bit:1024, dataset:palmprint,eer:0.00600, Best eer: 0.00600\n",
      "\b loss:0.70575\n",
      "[CSQ][652/2000][17:40:07] bit:1024, dataset:palmprint, training... loss:0.70740\n",
      "[CSQ][653/2000][17:40:15] bit:1024, dataset:palmprint, training... loss:0.70643\n",
      "[CSQ][654/2000][17:40:24] bit:1024, dataset:palmprint, training... loss:0.70667\n",
      "[CSQ][655/2000][17:40:32] bit:1024, dataset:palmprint, training... loss:0.70648\n",
      "[CSQ][656/2000][17:40:40] bit:1024, dataset:palmprint, training... loss:0.70776\n",
      "[CSQ][657/2000][17:40:49] bit:1024, dataset:palmprint, training... loss:0.70693\n",
      "[CSQ][658/2000][17:40:57] bit:1024, dataset:palmprint, training... loss:0.70676\n",
      "[CSQ][659/2000][17:41:06] bit:1024, dataset:palmprint, training... loss:0.70499\n",
      "[CSQ][660/2000][17:41:14] bit:1024, dataset:palmprint, training... loss:0.70533\n",
      "[CSQ][661/2000][17:41:23] bit:1024, dataset:palmprint, training... loss:0.70778\n",
      "[CSQ][662/2000][17:41:32] bit:1024, dataset:palmprint, training... loss:0.70669\n",
      "[CSQ][663/2000][17:41:40] bit:1024, dataset:palmprint, training... loss:0.70714\n",
      "[CSQ][664/2000][17:41:48] bit:1024, dataset:palmprint, training... loss:0.70582\n",
      "[CSQ][665/2000][17:41:57] bit:1024, dataset:palmprint, training... loss:0.70394\n",
      "[CSQ][666/2000][17:42:06] bit:1024, dataset:palmprint, training... loss:0.70686\n",
      "[CSQ][667/2000][17:42:14] bit:1024, dataset:palmprint, training... loss:0.70697\n",
      "[CSQ][668/2000][17:42:24] bit:1024, dataset:palmprint, training... loss:0.70634\n",
      "[CSQ][669/2000][17:42:32] bit:1024, dataset:palmprint, training... loss:0.70630\n",
      "[CSQ][670/2000][17:42:42] bit:1024, dataset:palmprint, training... loss:0.70550\n",
      "[CSQ][671/2000][17:42:50] bit:1024, dataset:palmprint, training... loss:0.70714\n",
      "[CSQ][672/2000][17:42:58] bit:1024, dataset:palmprint, training... loss:0.70551\n",
      "[CSQ][673/2000][17:43:07] bit:1024, dataset:palmprint, training... loss:0.70523\n",
      "[CSQ][674/2000][17:43:15] bit:1024, dataset:palmprint, training... loss:0.70565\n",
      "[CSQ][675/2000][17:43:23] bit:1024, dataset:palmprint, training... loss:0.70354\n",
      "[CSQ][676/2000][17:43:31] bit:1024, dataset:palmprint, training... loss:0.70396\n",
      "[CSQ][677/2000][17:43:40] bit:1024, dataset:palmprint, training... loss:0.70648\n",
      "[CSQ][678/2000][17:43:48] bit:1024, dataset:palmprint, training... loss:0.70488\n",
      "[CSQ][679/2000][17:43:58] bit:1024, dataset:palmprint, training... loss:0.70406\n",
      "[CSQ][680/2000][17:44:06] bit:1024, dataset:palmprint, training... loss:0.70540\n",
      "[CSQ][681/2000][17:44:15] bit:1024, dataset:palmprint, training... loss:0.70380\n",
      "[CSQ][682/2000][17:44:23] bit:1024, dataset:palmprint, training... loss:0.70431\n",
      "[CSQ][683/2000][17:44:32] bit:1024, dataset:palmprint, training... loss:0.70413\n",
      "[CSQ][684/2000][17:44:40] bit:1024, dataset:palmprint, training... loss:0.70409\n",
      "[CSQ][685/2000][17:44:48] bit:1024, dataset:palmprint, training... loss:0.70580\n",
      "[CSQ][686/2000][17:44:57] bit:1024, dataset:palmprint, training... loss:0.70348\n",
      "[CSQ][687/2000][17:45:05] bit:1024, dataset:palmprint, training... loss:0.70423\n",
      "[CSQ][688/2000][17:45:15] bit:1024, dataset:palmprint, training... loss:0.70531\n",
      "[CSQ][689/2000][17:45:23] bit:1024, dataset:palmprint, training... loss:0.70449\n",
      "[CSQ][690/2000][17:45:32] bit:1024, dataset:palmprint, training... loss:0.70483\n",
      "[CSQ][691/2000][17:45:40] bit:1024, dataset:palmprint, training... loss:0.70503\n",
      "[CSQ][692/2000][17:45:48] bit:1024, dataset:palmprint, training... loss:0.70246\n",
      "[CSQ][693/2000][17:45:56] bit:1024, dataset:palmprint, training... loss:0.70377\n",
      "[CSQ][694/2000][17:46:05] bit:1024, dataset:palmprint, training... loss:0.70473\n",
      "[CSQ][695/2000][17:46:13] bit:1024, dataset:palmprint, training... loss:0.70169\n",
      "[CSQ][696/2000][17:46:21] bit:1024, dataset:palmprint, training... loss:0.70564\n",
      "[CSQ][697/2000][17:46:28] bit:1024, dataset:palmprint, training... loss:0.70274\n",
      "[CSQ][698/2000][17:46:36] bit:1024, dataset:palmprint, training... loss:0.70483\n",
      "[CSQ][699/2000][17:46:45] bit:1024, dataset:palmprint, training... loss:0.70584\n",
      "[CSQ][700/2000][17:46:53] bit:1024, dataset:palmprint, training... loss:0.70269\n",
      "[CSQ][701/2000][17:47:03] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.006333333333333333\n",
      "[CSQ] epoch:701, bit:1024, dataset:palmprint,eer:0.00633, Best eer: 0.00600\n",
      "\b loss:0.70394\n",
      "[CSQ][702/2000][17:48:07] bit:1024, dataset:palmprint, training... loss:0.70298\n",
      "[CSQ][703/2000][17:48:15] bit:1024, dataset:palmprint, training... loss:0.70226\n",
      "[CSQ][704/2000][17:48:23] bit:1024, dataset:palmprint, training... loss:0.70304\n",
      "[CSQ][705/2000][17:48:32] bit:1024, dataset:palmprint, training... loss:0.70270\n",
      "[CSQ][706/2000][17:48:40] bit:1024, dataset:palmprint, training... loss:0.70108\n",
      "[CSQ][707/2000][17:48:48] bit:1024, dataset:palmprint, training... loss:0.70272\n",
      "[CSQ][708/2000][17:48:58] bit:1024, dataset:palmprint, training... loss:0.70256\n",
      "[CSQ][709/2000][17:49:06] bit:1024, dataset:palmprint, training... loss:0.70287\n",
      "[CSQ][710/2000][17:49:15] bit:1024, dataset:palmprint, training... loss:0.70250\n",
      "[CSQ][711/2000][17:49:23] bit:1024, dataset:palmprint, training... loss:0.70250\n",
      "[CSQ][712/2000][17:49:31] bit:1024, dataset:palmprint, training... loss:0.70338\n",
      "[CSQ][713/2000][17:49:40] bit:1024, dataset:palmprint, training... loss:0.70266\n",
      "[CSQ][714/2000][17:49:48] bit:1024, dataset:palmprint, training... loss:0.70217\n",
      "[CSQ][715/2000][17:49:57] bit:1024, dataset:palmprint, training... loss:0.70161\n",
      "[CSQ][716/2000][17:50:06] bit:1024, dataset:palmprint, training... loss:0.70205\n",
      "[CSQ][717/2000][17:50:14] bit:1024, dataset:palmprint, training... loss:0.70026\n",
      "[CSQ][718/2000][17:50:22] bit:1024, dataset:palmprint, training... loss:0.70157\n",
      "[CSQ][719/2000][17:50:30] bit:1024, dataset:palmprint, training... loss:0.70105\n",
      "[CSQ][720/2000][17:50:39] bit:1024, dataset:palmprint, training... loss:0.70271\n",
      "[CSQ][721/2000][17:50:47] bit:1024, dataset:palmprint, training... loss:0.70130\n",
      "[CSQ][722/2000][17:50:56] bit:1024, dataset:palmprint, training... loss:0.70210\n",
      "[CSQ][723/2000][17:51:05] bit:1024, dataset:palmprint, training... loss:0.70126\n",
      "[CSQ][724/2000][17:51:12] bit:1024, dataset:palmprint, training... loss:0.70057\n",
      "[CSQ][725/2000][17:51:20] bit:1024, dataset:palmprint, training... loss:0.69902\n",
      "[CSQ][726/2000][17:51:28] bit:1024, dataset:palmprint, training... loss:0.70265\n",
      "[CSQ][727/2000][17:51:37] bit:1024, dataset:palmprint, training... loss:0.70126\n",
      "[CSQ][728/2000][17:51:44] bit:1024, dataset:palmprint, training... loss:0.69965\n",
      "[CSQ][729/2000][17:51:53] bit:1024, dataset:palmprint, training... loss:0.70015\n",
      "[CSQ][730/2000][17:52:02] bit:1024, dataset:palmprint, training... loss:0.70151\n",
      "[CSQ][731/2000][17:52:10] bit:1024, dataset:palmprint, training... loss:0.69996\n",
      "[CSQ][732/2000][17:52:19] bit:1024, dataset:palmprint, training... loss:0.69993\n",
      "[CSQ][733/2000][17:52:27] bit:1024, dataset:palmprint, training... loss:0.70126\n",
      "[CSQ][734/2000][17:52:35] bit:1024, dataset:palmprint, training... loss:0.70006\n",
      "[CSQ][735/2000][17:52:42] bit:1024, dataset:palmprint, training... loss:0.69932\n",
      "[CSQ][736/2000][17:52:51] bit:1024, dataset:palmprint, training... loss:0.70072\n",
      "[CSQ][737/2000][17:52:59] bit:1024, dataset:palmprint, training... loss:0.70054\n",
      "[CSQ][738/2000][17:53:08] bit:1024, dataset:palmprint, training... loss:0.69957\n",
      "[CSQ][739/2000][17:53:16] bit:1024, dataset:palmprint, training... loss:0.69920\n",
      "[CSQ][740/2000][17:53:25] bit:1024, dataset:palmprint, training... loss:0.69986\n",
      "[CSQ][741/2000][17:53:33] bit:1024, dataset:palmprint, training... loss:0.70135\n",
      "[CSQ][742/2000][17:53:41] bit:1024, dataset:palmprint, training... loss:0.70000\n",
      "[CSQ][743/2000][17:53:49] bit:1024, dataset:palmprint, training... loss:0.69838\n",
      "[CSQ][744/2000][17:53:57] bit:1024, dataset:palmprint, training... loss:0.70103\n",
      "[CSQ][745/2000][17:54:06] bit:1024, dataset:palmprint, training... loss:0.69955\n",
      "[CSQ][746/2000][17:54:14] bit:1024, dataset:palmprint, training... loss:0.70247\n",
      "[CSQ][747/2000][17:54:23] bit:1024, dataset:palmprint, training... loss:0.69981\n",
      "[CSQ][748/2000][17:54:32] bit:1024, dataset:palmprint, training... loss:0.69991\n",
      "[CSQ][749/2000][17:54:40] bit:1024, dataset:palmprint, training... loss:0.69841\n",
      "[CSQ][750/2000][17:54:47] bit:1024, dataset:palmprint, training... loss:0.69901\n",
      "[CSQ][751/2000][17:54:55] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.005666666666666667\n",
      "[CSQ] epoch:751, bit:1024, dataset:palmprint,eer:0.00567, Best eer: 0.00567\n",
      "\b loss:0.70211\n",
      "[CSQ][752/2000][17:56:10] bit:1024, dataset:palmprint, training... loss:0.69712\n",
      "[CSQ][753/2000][17:56:19] bit:1024, dataset:palmprint, training... loss:0.69910\n",
      "[CSQ][754/2000][17:56:27] bit:1024, dataset:palmprint, training... loss:0.69910\n",
      "[CSQ][755/2000][17:56:35] bit:1024, dataset:palmprint, training... loss:0.69884\n",
      "[CSQ][756/2000][17:56:43] bit:1024, dataset:palmprint, training... loss:0.69936\n",
      "[CSQ][757/2000][17:56:51] bit:1024, dataset:palmprint, training... loss:0.69852\n",
      "[CSQ][758/2000][17:57:00] bit:1024, dataset:palmprint, training... loss:0.70084\n",
      "[CSQ][759/2000][17:57:08] bit:1024, dataset:palmprint, training... loss:0.69885\n",
      "[CSQ][760/2000][17:57:17] bit:1024, dataset:palmprint, training... loss:0.69943\n",
      "[CSQ][761/2000][17:57:26] bit:1024, dataset:palmprint, training... loss:0.69714\n",
      "[CSQ][762/2000][17:57:35] bit:1024, dataset:palmprint, training... loss:0.69742\n",
      "[CSQ][763/2000][17:57:42] bit:1024, dataset:palmprint, training... loss:0.69929\n",
      "[CSQ][764/2000][17:57:50] bit:1024, dataset:palmprint, training... loss:0.69715\n",
      "[CSQ][765/2000][17:57:59] bit:1024, dataset:palmprint, training... loss:0.69862\n",
      "[CSQ][766/2000][17:58:07] bit:1024, dataset:palmprint, training... loss:0.70017\n",
      "[CSQ][767/2000][17:58:16] bit:1024, dataset:palmprint, training... loss:0.69867\n",
      "[CSQ][768/2000][17:58:25] bit:1024, dataset:palmprint, training... loss:0.69791\n",
      "[CSQ][769/2000][17:58:34] bit:1024, dataset:palmprint, training... loss:0.69970\n",
      "[CSQ][770/2000][17:58:42] bit:1024, dataset:palmprint, training... loss:0.69904\n",
      "[CSQ][771/2000][17:58:50] bit:1024, dataset:palmprint, training... loss:0.69776\n",
      "[CSQ][772/2000][17:58:59] bit:1024, dataset:palmprint, training... loss:0.69820\n",
      "[CSQ][773/2000][17:59:06] bit:1024, dataset:palmprint, training... loss:0.70007\n",
      "[CSQ][774/2000][17:59:14] bit:1024, dataset:palmprint, training... loss:0.69834\n",
      "[CSQ][775/2000][17:59:21] bit:1024, dataset:palmprint, training... loss:0.69764\n",
      "[CSQ][776/2000][17:59:31] bit:1024, dataset:palmprint, training... loss:0.69897\n",
      "[CSQ][777/2000][17:59:39] bit:1024, dataset:palmprint, training... loss:0.69619\n",
      "[CSQ][778/2000][17:59:48] bit:1024, dataset:palmprint, training... loss:0.69865\n",
      "[CSQ][779/2000][17:59:57] bit:1024, dataset:palmprint, training... loss:0.69619\n",
      "[CSQ][780/2000][18:00:05] bit:1024, dataset:palmprint, training... loss:0.69695\n",
      "[CSQ][781/2000][18:00:13] bit:1024, dataset:palmprint, training... loss:0.69889\n",
      "[CSQ][782/2000][18:00:21] bit:1024, dataset:palmprint, training... loss:0.69603\n",
      "[CSQ][783/2000][18:00:30] bit:1024, dataset:palmprint, training... loss:0.69484\n",
      "[CSQ][784/2000][18:00:39] bit:1024, dataset:palmprint, training... loss:0.69609\n",
      "[CSQ][785/2000][18:00:47] bit:1024, dataset:palmprint, training... loss:0.69693\n",
      "[CSQ][786/2000][18:00:56] bit:1024, dataset:palmprint, training... loss:0.69668\n",
      "[CSQ][787/2000][18:01:04] bit:1024, dataset:palmprint, training... loss:0.69740\n",
      "[CSQ][788/2000][18:01:12] bit:1024, dataset:palmprint, training... loss:0.69787\n",
      "[CSQ][789/2000][18:01:20] bit:1024, dataset:palmprint, training... loss:0.69701\n",
      "[CSQ][790/2000][18:01:29] bit:1024, dataset:palmprint, training... loss:0.69743\n",
      "[CSQ][791/2000][18:01:38] bit:1024, dataset:palmprint, training... loss:0.69632\n",
      "[CSQ][792/2000][18:01:46] bit:1024, dataset:palmprint, training... loss:0.69679\n",
      "[CSQ][793/2000][18:01:54] bit:1024, dataset:palmprint, training... loss:0.69897\n",
      "[CSQ][794/2000][18:02:03] bit:1024, dataset:palmprint, training... loss:0.69682\n",
      "[CSQ][795/2000][18:02:11] bit:1024, dataset:palmprint, training... loss:0.69696\n",
      "[CSQ][796/2000][18:02:20] bit:1024, dataset:palmprint, training... loss:0.69782\n",
      "[CSQ][797/2000][18:02:29] bit:1024, dataset:palmprint, training... loss:0.69789\n",
      "[CSQ][798/2000][18:02:37] bit:1024, dataset:palmprint, training... loss:0.69629\n",
      "[CSQ][799/2000][18:02:46] bit:1024, dataset:palmprint, training... loss:0.69706\n",
      "[CSQ][800/2000][18:02:55] bit:1024, dataset:palmprint, training... loss:0.69597\n",
      "[CSQ][801/2000][18:03:03] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.005\n",
      "[CSQ] epoch:801, bit:1024, dataset:palmprint,eer:0.00500, Best eer: 0.00500\n",
      "\b loss:0.69313\n",
      "[CSQ][802/2000][18:04:18] bit:1024, dataset:palmprint, training... loss:0.69606\n",
      "[CSQ][803/2000][18:04:27] bit:1024, dataset:palmprint, training... loss:0.69635\n",
      "[CSQ][804/2000][18:04:36] bit:1024, dataset:palmprint, training... loss:0.69459\n",
      "[CSQ][805/2000][18:04:44] bit:1024, dataset:palmprint, training... loss:0.69620\n",
      "[CSQ][806/2000][18:04:53] bit:1024, dataset:palmprint, training... loss:0.69684\n",
      "[CSQ][807/2000][18:05:01] bit:1024, dataset:palmprint, training... loss:0.69546\n",
      "[CSQ][808/2000][18:05:09] bit:1024, dataset:palmprint, training... loss:0.69606\n",
      "[CSQ][809/2000][18:05:17] bit:1024, dataset:palmprint, training... loss:0.69537\n",
      "[CSQ][810/2000][18:05:26] bit:1024, dataset:palmprint, training... loss:0.69588\n",
      "[CSQ][811/2000][18:05:35] bit:1024, dataset:palmprint, training... loss:0.69610\n",
      "[CSQ][812/2000][18:05:43] bit:1024, dataset:palmprint, training... loss:0.69560\n",
      "[CSQ][813/2000][18:05:51] bit:1024, dataset:palmprint, training... loss:0.69641\n",
      "[CSQ][814/2000][18:05:59] bit:1024, dataset:palmprint, training... loss:0.69486\n",
      "[CSQ][815/2000][18:06:08] bit:1024, dataset:palmprint, training... loss:0.69582\n",
      "[CSQ][816/2000][18:06:15] bit:1024, dataset:palmprint, training... loss:0.69398\n",
      "[CSQ][817/2000][18:06:25] bit:1024, dataset:palmprint, training... loss:0.69546\n",
      "[CSQ][818/2000][18:06:34] bit:1024, dataset:palmprint, training... loss:0.69507\n",
      "[CSQ][819/2000][18:06:42] bit:1024, dataset:palmprint, training... loss:0.69521\n",
      "[CSQ][820/2000][18:06:50] bit:1024, dataset:palmprint, training... loss:0.69559\n",
      "[CSQ][821/2000][18:06:57] bit:1024, dataset:palmprint, training... loss:0.69360\n",
      "[CSQ][822/2000][18:07:06] bit:1024, dataset:palmprint, training... loss:0.69621\n",
      "[CSQ][823/2000][18:07:14] bit:1024, dataset:palmprint, training... loss:0.69378\n",
      "[CSQ][824/2000][18:07:22] bit:1024, dataset:palmprint, training... loss:0.69367\n",
      "[CSQ][825/2000][18:07:30] bit:1024, dataset:palmprint, training... loss:0.69322\n",
      "[CSQ][826/2000][18:07:39] bit:1024, dataset:palmprint, training... loss:0.69551\n",
      "[CSQ][827/2000][18:07:48] bit:1024, dataset:palmprint, training... loss:0.69328\n",
      "[CSQ][828/2000][18:07:56] bit:1024, dataset:palmprint, training... loss:0.69199\n",
      "[CSQ][829/2000][18:08:04] bit:1024, dataset:palmprint, training... loss:0.69249\n",
      "[CSQ][830/2000][18:08:12] bit:1024, dataset:palmprint, training... loss:0.69291\n",
      "[CSQ][831/2000][18:08:21] bit:1024, dataset:palmprint, training... loss:0.69315\n",
      "[CSQ][832/2000][18:08:28] bit:1024, dataset:palmprint, training... loss:0.69473\n",
      "[CSQ][833/2000][18:08:38] bit:1024, dataset:palmprint, training... loss:0.69495\n",
      "[CSQ][834/2000][18:08:46] bit:1024, dataset:palmprint, training... loss:0.69542\n",
      "[CSQ][835/2000][18:08:54] bit:1024, dataset:palmprint, training... loss:0.69432\n",
      "[CSQ][836/2000][18:09:02] bit:1024, dataset:palmprint, training... loss:0.69475\n",
      "[CSQ][837/2000][18:09:10] bit:1024, dataset:palmprint, training... loss:0.69278\n",
      "[CSQ][838/2000][18:09:19] bit:1024, dataset:palmprint, training... loss:0.69390\n",
      "[CSQ][839/2000][18:09:27] bit:1024, dataset:palmprint, training... loss:0.69192\n",
      "[CSQ][840/2000][18:09:35] bit:1024, dataset:palmprint, training... loss:0.69437\n",
      "[CSQ][841/2000][18:09:43] bit:1024, dataset:palmprint, training... loss:0.69433\n",
      "[CSQ][842/2000][18:09:52] bit:1024, dataset:palmprint, training... loss:0.69313\n",
      "[CSQ][843/2000][18:10:01] bit:1024, dataset:palmprint, training... loss:0.69392\n",
      "[CSQ][844/2000][18:10:09] bit:1024, dataset:palmprint, training... loss:0.69110\n",
      "[CSQ][845/2000][18:10:17] bit:1024, dataset:palmprint, training... loss:0.69244\n",
      "[CSQ][846/2000][18:10:25] bit:1024, dataset:palmprint, training... loss:0.69154\n",
      "[CSQ][847/2000][18:10:34] bit:1024, dataset:palmprint, training... loss:0.69519\n",
      "[CSQ][848/2000][18:10:41] bit:1024, dataset:palmprint, training... loss:0.69223\n",
      "[CSQ][849/2000][18:10:50] bit:1024, dataset:palmprint, training... loss:0.69281\n",
      "[CSQ][850/2000][18:10:59] bit:1024, dataset:palmprint, training... loss:0.69196\n",
      "[CSQ][851/2000][18:11:07] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.006333333333333333\n",
      "[CSQ] epoch:851, bit:1024, dataset:palmprint,eer:0.00633, Best eer: 0.00500\n",
      "\b loss:0.69344\n",
      "[CSQ][852/2000][18:12:09] bit:1024, dataset:palmprint, training... loss:0.69257\n",
      "[CSQ][853/2000][18:12:17] bit:1024, dataset:palmprint, training... loss:0.69253\n",
      "[CSQ][854/2000][18:12:25] bit:1024, dataset:palmprint, training... loss:0.69062\n",
      "[CSQ][855/2000][18:12:33] bit:1024, dataset:palmprint, training... loss:0.69410\n",
      "[CSQ][856/2000][18:12:41] bit:1024, dataset:palmprint, training... loss:0.69265\n",
      "[CSQ][857/2000][18:12:50] bit:1024, dataset:palmprint, training... loss:0.69159\n",
      "[CSQ][858/2000][18:12:57] bit:1024, dataset:palmprint, training... loss:0.69182\n",
      "[CSQ][859/2000][18:13:07] bit:1024, dataset:palmprint, training... loss:0.69068\n",
      "[CSQ][860/2000][18:13:16] bit:1024, dataset:palmprint, training... loss:0.69049\n",
      "[CSQ][861/2000][18:13:24] bit:1024, dataset:palmprint, training... loss:0.69294\n",
      "[CSQ][862/2000][18:13:32] bit:1024, dataset:palmprint, training... loss:0.69108\n",
      "[CSQ][863/2000][18:13:39] bit:1024, dataset:palmprint, training... loss:0.69259\n",
      "[CSQ][864/2000][18:13:48] bit:1024, dataset:palmprint, training... loss:0.69254\n",
      "[CSQ][865/2000][18:13:56] bit:1024, dataset:palmprint, training... loss:0.69192\n",
      "[CSQ][866/2000][18:14:05] bit:1024, dataset:palmprint, training... loss:0.69209\n",
      "[CSQ][867/2000][18:14:13] bit:1024, dataset:palmprint, training... loss:0.69024\n",
      "[CSQ][868/2000][18:14:22] bit:1024, dataset:palmprint, training... loss:0.69393\n",
      "[CSQ][869/2000][18:14:30] bit:1024, dataset:palmprint, training... loss:0.69061\n",
      "[CSQ][870/2000][18:14:38] bit:1024, dataset:palmprint, training... loss:0.69221\n",
      "[CSQ][871/2000][18:14:47] bit:1024, dataset:palmprint, training... loss:0.69322\n",
      "[CSQ][872/2000][18:14:54] bit:1024, dataset:palmprint, training... loss:0.69162\n",
      "[CSQ][873/2000][18:15:04] bit:1024, dataset:palmprint, training... loss:0.69031\n",
      "[CSQ][874/2000][18:15:12] bit:1024, dataset:palmprint, training... loss:0.69148\n",
      "[CSQ][875/2000][18:15:20] bit:1024, dataset:palmprint, training... loss:0.69100\n",
      "[CSQ][876/2000][18:15:29] bit:1024, dataset:palmprint, training... loss:0.69265\n",
      "[CSQ][877/2000][18:15:37] bit:1024, dataset:palmprint, training... loss:0.68868\n",
      "[CSQ][878/2000][18:15:45] bit:1024, dataset:palmprint, training... loss:0.68908\n",
      "[CSQ][879/2000][18:15:53] bit:1024, dataset:palmprint, training... loss:0.69433\n",
      "[CSQ][880/2000][18:16:02] bit:1024, dataset:palmprint, training... loss:0.69240\n",
      "[CSQ][881/2000][18:16:09] bit:1024, dataset:palmprint, training... loss:0.68995\n",
      "[CSQ][882/2000][18:16:19] bit:1024, dataset:palmprint, training... loss:0.68973\n",
      "[CSQ][883/2000][18:16:27] bit:1024, dataset:palmprint, training... loss:0.69213\n",
      "[CSQ][884/2000][18:16:35] bit:1024, dataset:palmprint, training... loss:0.69037\n",
      "[CSQ][885/2000][18:16:43] bit:1024, dataset:palmprint, training... loss:0.68998\n",
      "[CSQ][886/2000][18:16:52] bit:1024, dataset:palmprint, training... loss:0.69109\n",
      "[CSQ][887/2000][18:17:00] bit:1024, dataset:palmprint, training... loss:0.68940\n",
      "[CSQ][888/2000][18:17:08] bit:1024, dataset:palmprint, training... loss:0.68892\n",
      "[CSQ][889/2000][18:17:17] bit:1024, dataset:palmprint, training... loss:0.69034\n",
      "[CSQ][890/2000][18:17:24] bit:1024, dataset:palmprint, training... loss:0.68778\n",
      "[CSQ][891/2000][18:17:33] bit:1024, dataset:palmprint, training... loss:0.69054\n",
      "[CSQ][892/2000][18:17:42] bit:1024, dataset:palmprint, training... loss:0.69024\n",
      "[CSQ][893/2000][18:17:51] bit:1024, dataset:palmprint, training... loss:0.69034\n",
      "[CSQ][894/2000][18:17:59] bit:1024, dataset:palmprint, training... loss:0.68941\n",
      "[CSQ][895/2000][18:18:06] bit:1024, dataset:palmprint, training... loss:0.68796\n",
      "[CSQ][896/2000][18:18:15] bit:1024, dataset:palmprint, training... loss:0.68979\n",
      "[CSQ][897/2000][18:18:23] bit:1024, dataset:palmprint, training... loss:0.69169\n",
      "[CSQ][898/2000][18:18:32] bit:1024, dataset:palmprint, training... loss:0.68979\n",
      "[CSQ][899/2000][18:18:40] bit:1024, dataset:palmprint, training... loss:0.69048\n",
      "[CSQ][900/2000][18:18:49] bit:1024, dataset:palmprint, training... loss:0.68990\n",
      "[CSQ][901/2000][18:18:57] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.003\n",
      "[CSQ] epoch:901, bit:1024, dataset:palmprint,eer:0.00300, Best eer: 0.00300\n",
      "\b loss:0.68888\n",
      "[CSQ][902/2000][18:20:10] bit:1024, dataset:palmprint, training... loss:0.68934\n",
      "[CSQ][903/2000][18:20:17] bit:1024, dataset:palmprint, training... loss:0.69114\n",
      "[CSQ][904/2000][18:20:27] bit:1024, dataset:palmprint, training... loss:0.68920\n",
      "[CSQ][905/2000][18:20:35] bit:1024, dataset:palmprint, training... loss:0.69073\n",
      "[CSQ][906/2000][18:20:44] bit:1024, dataset:palmprint, training... loss:0.68994\n",
      "[CSQ][907/2000][18:20:52] bit:1024, dataset:palmprint, training... loss:0.68878\n",
      "[CSQ][908/2000][18:21:00] bit:1024, dataset:palmprint, training... loss:0.68887\n",
      "[CSQ][909/2000][18:21:09] bit:1024, dataset:palmprint, training... loss:0.68847\n",
      "[CSQ][910/2000][18:21:17] bit:1024, dataset:palmprint, training... loss:0.69093\n",
      "[CSQ][911/2000][18:21:26] bit:1024, dataset:palmprint, training... loss:0.68703\n",
      "[CSQ][912/2000][18:21:34] bit:1024, dataset:palmprint, training... loss:0.68767\n",
      "[CSQ][913/2000][18:21:42] bit:1024, dataset:palmprint, training... loss:0.68757\n",
      "[CSQ][914/2000][18:21:50] bit:1024, dataset:palmprint, training... loss:0.68785\n",
      "[CSQ][915/2000][18:21:58] bit:1024, dataset:palmprint, training... loss:0.68896\n",
      "[CSQ][916/2000][18:22:07] bit:1024, dataset:palmprint, training... loss:0.68993\n",
      "[CSQ][917/2000][18:22:14] bit:1024, dataset:palmprint, training... loss:0.68993\n",
      "[CSQ][918/2000][18:22:23] bit:1024, dataset:palmprint, training... loss:0.68895\n",
      "[CSQ][919/2000][18:22:31] bit:1024, dataset:palmprint, training... loss:0.68965\n",
      "[CSQ][920/2000][18:22:40] bit:1024, dataset:palmprint, training... loss:0.68746\n",
      "[CSQ][921/2000][18:22:49] bit:1024, dataset:palmprint, training... loss:0.68709\n",
      "[CSQ][922/2000][18:22:57] bit:1024, dataset:palmprint, training... loss:0.68819\n",
      "[CSQ][923/2000][18:23:05] bit:1024, dataset:palmprint, training... loss:0.68749\n",
      "[CSQ][924/2000][18:23:13] bit:1024, dataset:palmprint, training... loss:0.68750\n",
      "[CSQ][925/2000][18:23:21] bit:1024, dataset:palmprint, training... loss:0.68694\n",
      "[CSQ][926/2000][18:23:29] bit:1024, dataset:palmprint, training... loss:0.68911\n",
      "[CSQ][927/2000][18:23:38] bit:1024, dataset:palmprint, training... loss:0.68884\n",
      "[CSQ][928/2000][18:23:46] bit:1024, dataset:palmprint, training... loss:0.68800\n",
      "[CSQ][929/2000][18:23:55] bit:1024, dataset:palmprint, training... loss:0.68910\n",
      "[CSQ][930/2000][18:24:04] bit:1024, dataset:palmprint, training... loss:0.68733\n",
      "[CSQ][931/2000][18:24:12] bit:1024, dataset:palmprint, training... loss:0.68707\n",
      "[CSQ][932/2000][18:24:20] bit:1024, dataset:palmprint, training... loss:0.68941\n",
      "[CSQ][933/2000][18:24:28] bit:1024, dataset:palmprint, training... loss:0.68807\n",
      "[CSQ][934/2000][18:24:37] bit:1024, dataset:palmprint, training... loss:0.68778\n",
      "[CSQ][935/2000][18:24:44] bit:1024, dataset:palmprint, training... loss:0.68736\n",
      "[CSQ][936/2000][18:24:52] bit:1024, dataset:palmprint, training... loss:0.68688\n",
      "[CSQ][937/2000][18:25:00] bit:1024, dataset:palmprint, training... loss:0.68648\n",
      "[CSQ][938/2000][18:25:09] bit:1024, dataset:palmprint, training... loss:0.68623\n",
      "[CSQ][939/2000][18:25:18] bit:1024, dataset:palmprint, training... loss:0.68671\n",
      "[CSQ][940/2000][18:25:26] bit:1024, dataset:palmprint, training... loss:0.68817\n",
      "[CSQ][941/2000][18:25:34] bit:1024, dataset:palmprint, training... loss:0.68541\n",
      "[CSQ][942/2000][18:25:42] bit:1024, dataset:palmprint, training... loss:0.68722\n",
      "[CSQ][943/2000][18:25:51] bit:1024, dataset:palmprint, training... loss:0.68433\n",
      "[CSQ][944/2000][18:25:59] bit:1024, dataset:palmprint, training... loss:0.68930\n",
      "[CSQ][945/2000][18:26:08] bit:1024, dataset:palmprint, training... loss:0.68853\n",
      "[CSQ][946/2000][18:26:17] bit:1024, dataset:palmprint, training... loss:0.68472\n",
      "[CSQ][947/2000][18:26:25] bit:1024, dataset:palmprint, training... loss:0.68540\n",
      "[CSQ][948/2000][18:26:33] bit:1024, dataset:palmprint, training... loss:0.68596\n",
      "[CSQ][949/2000][18:26:41] bit:1024, dataset:palmprint, training... loss:0.68824\n",
      "[CSQ][950/2000][18:26:49] bit:1024, dataset:palmprint, training... loss:0.68912\n",
      "[CSQ][951/2000][18:26:57] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0036666666666666666\n",
      "[CSQ] epoch:951, bit:1024, dataset:palmprint,eer:0.00367, Best eer: 0.00300\n",
      "\b loss:0.68635\n",
      "[CSQ][952/2000][18:28:00] bit:1024, dataset:palmprint, training... loss:0.68580\n",
      "[CSQ][953/2000][18:28:08] bit:1024, dataset:palmprint, training... loss:0.68613\n",
      "[CSQ][954/2000][18:28:17] bit:1024, dataset:palmprint, training... loss:0.68650\n",
      "[CSQ][955/2000][18:28:26] bit:1024, dataset:palmprint, training... loss:0.68686\n",
      "[CSQ][956/2000][18:28:34] bit:1024, dataset:palmprint, training... loss:0.68506\n",
      "[CSQ][957/2000][18:28:42] bit:1024, dataset:palmprint, training... loss:0.68714\n",
      "[CSQ][958/2000][18:28:50] bit:1024, dataset:palmprint, training... loss:0.68550\n",
      "[CSQ][959/2000][18:29:00] bit:1024, dataset:palmprint, training... loss:0.68828\n",
      "[CSQ][960/2000][18:29:08] bit:1024, dataset:palmprint, training... loss:0.68735\n",
      "[CSQ][961/2000][18:29:17] bit:1024, dataset:palmprint, training... loss:0.68401\n",
      "[CSQ][962/2000][18:29:25] bit:1024, dataset:palmprint, training... loss:0.68717\n",
      "[CSQ][963/2000][18:29:34] bit:1024, dataset:palmprint, training... loss:0.68759\n",
      "[CSQ][964/2000][18:29:42] bit:1024, dataset:palmprint, training... loss:0.68789\n",
      "[CSQ][965/2000][18:29:50] bit:1024, dataset:palmprint, training... loss:0.68689\n",
      "[CSQ][966/2000][18:29:58] bit:1024, dataset:palmprint, training... loss:0.68890\n",
      "[CSQ][967/2000][18:30:06] bit:1024, dataset:palmprint, training... loss:0.68620\n",
      "[CSQ][968/2000][18:30:15] bit:1024, dataset:palmprint, training... loss:0.68472\n",
      "[CSQ][969/2000][18:30:23] bit:1024, dataset:palmprint, training... loss:0.68422\n",
      "[CSQ][970/2000][18:30:32] bit:1024, dataset:palmprint, training... loss:0.68428\n",
      "[CSQ][971/2000][18:30:40] bit:1024, dataset:palmprint, training... loss:0.68528\n",
      "[CSQ][972/2000][18:30:48] bit:1024, dataset:palmprint, training... loss:0.68438\n",
      "[CSQ][973/2000][18:30:56] bit:1024, dataset:palmprint, training... loss:0.68445\n",
      "[CSQ][974/2000][18:31:04] bit:1024, dataset:palmprint, training... loss:0.68391\n",
      "[CSQ][975/2000][18:31:13] bit:1024, dataset:palmprint, training... loss:0.68434\n",
      "[CSQ][976/2000][18:31:21] bit:1024, dataset:palmprint, training... loss:0.68590\n",
      "[CSQ][977/2000][18:31:30] bit:1024, dataset:palmprint, training... loss:0.68342\n",
      "[CSQ][978/2000][18:31:38] bit:1024, dataset:palmprint, training... loss:0.68575\n",
      "[CSQ][979/2000][18:31:46] bit:1024, dataset:palmprint, training... loss:0.68706\n",
      "[CSQ][980/2000][18:31:54] bit:1024, dataset:palmprint, training... loss:0.68434\n",
      "[CSQ][981/2000][18:32:02] bit:1024, dataset:palmprint, training... loss:0.68261\n",
      "[CSQ][982/2000][18:32:11] bit:1024, dataset:palmprint, training... loss:0.68619\n",
      "[CSQ][983/2000][18:32:18] bit:1024, dataset:palmprint, training... loss:0.68419\n",
      "[CSQ][984/2000][18:32:27] bit:1024, dataset:palmprint, training... loss:0.68528\n",
      "[CSQ][985/2000][18:32:36] bit:1024, dataset:palmprint, training... loss:0.68665\n",
      "[CSQ][986/2000][18:32:44] bit:1024, dataset:palmprint, training... loss:0.68361\n",
      "[CSQ][987/2000][18:32:53] bit:1024, dataset:palmprint, training... loss:0.68492\n",
      "[CSQ][988/2000][18:33:00] bit:1024, dataset:palmprint, training... loss:0.68486\n",
      "[CSQ][989/2000][18:33:09] bit:1024, dataset:palmprint, training... loss:0.68445\n",
      "[CSQ][990/2000][18:33:17] bit:1024, dataset:palmprint, training... loss:0.68296\n",
      "[CSQ][991/2000][18:33:26] bit:1024, dataset:palmprint, training... loss:0.68352\n",
      "[CSQ][992/2000][18:33:33] bit:1024, dataset:palmprint, training... loss:0.68479\n",
      "[CSQ][993/2000][18:33:42] bit:1024, dataset:palmprint, training... loss:0.68418\n",
      "[CSQ][994/2000][18:33:51] bit:1024, dataset:palmprint, training... loss:0.68469\n",
      "[CSQ][995/2000][18:33:59] bit:1024, dataset:palmprint, training... loss:0.68386\n",
      "[CSQ][996/2000][18:34:07] bit:1024, dataset:palmprint, training... loss:0.68453\n",
      "[CSQ][997/2000][18:34:15] bit:1024, dataset:palmprint, training... loss:0.68619\n",
      "[CSQ][998/2000][18:34:23] bit:1024, dataset:palmprint, training... loss:0.68346\n",
      "[CSQ][999/2000][18:34:31] bit:1024, dataset:palmprint, training... loss:0.68226\n",
      "[CSQ][1000/2000][18:34:40] bit:1024, dataset:palmprint, training... loss:0.68415\n",
      "[CSQ][1001/2000][18:34:48] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0036666666666666666\n",
      "[CSQ] epoch:1001, bit:1024, dataset:palmprint,eer:0.00367, Best eer: 0.00300\n",
      "\b loss:0.68577\n",
      "[CSQ][1002/2000][18:35:53] bit:1024, dataset:palmprint, training... loss:0.68296\n",
      "[CSQ][1003/2000][18:36:02] bit:1024, dataset:palmprint, training... loss:0.68396\n",
      "[CSQ][1004/2000][18:36:10] bit:1024, dataset:palmprint, training... loss:0.68355\n",
      "[CSQ][1005/2000][18:36:18] bit:1024, dataset:palmprint, training... loss:0.68570\n",
      "[CSQ][1006/2000][18:36:26] bit:1024, dataset:palmprint, training... loss:0.68427\n",
      "[CSQ][1007/2000][18:36:34] bit:1024, dataset:palmprint, training... loss:0.68351\n",
      "[CSQ][1008/2000][18:36:42] bit:1024, dataset:palmprint, training... loss:0.68383\n",
      "[CSQ][1009/2000][18:36:51] bit:1024, dataset:palmprint, training... loss:0.68307\n",
      "[CSQ][1010/2000][18:36:59] bit:1024, dataset:palmprint, training... loss:0.68517\n",
      "[CSQ][1011/2000][18:37:08] bit:1024, dataset:palmprint, training... loss:0.68333\n",
      "[CSQ][1012/2000][18:37:18] bit:1024, dataset:palmprint, training... loss:0.68160\n",
      "[CSQ][1013/2000][18:37:26] bit:1024, dataset:palmprint, training... loss:0.68081\n",
      "[CSQ][1014/2000][18:37:33] bit:1024, dataset:palmprint, training... loss:0.68344\n",
      "[CSQ][1015/2000][18:37:41] bit:1024, dataset:palmprint, training... loss:0.68302\n",
      "[CSQ][1016/2000][18:37:48] bit:1024, dataset:palmprint, training... loss:0.68228\n",
      "[CSQ][1017/2000][18:37:58] bit:1024, dataset:palmprint, training... loss:0.68123\n",
      "[CSQ][1018/2000][18:38:07] bit:1024, dataset:palmprint, training... loss:0.68376\n",
      "[CSQ][1019/2000][18:38:15] bit:1024, dataset:palmprint, training... loss:0.68255\n",
      "[CSQ][1020/2000][18:38:23] bit:1024, dataset:palmprint, training... loss:0.68510\n",
      "[CSQ][1021/2000][18:38:31] bit:1024, dataset:palmprint, training... loss:0.68128\n",
      "[CSQ][1022/2000][18:38:40] bit:1024, dataset:palmprint, training... loss:0.68097\n",
      "[CSQ][1023/2000][18:38:48] bit:1024, dataset:palmprint, training... loss:0.68125\n",
      "[CSQ][1024/2000][18:38:57] bit:1024, dataset:palmprint, training... loss:0.68305\n",
      "[CSQ][1025/2000][18:39:05] bit:1024, dataset:palmprint, training... loss:0.68237\n",
      "[CSQ][1026/2000][18:39:13] bit:1024, dataset:palmprint, training... loss:0.68075\n",
      "[CSQ][1027/2000][18:39:21] bit:1024, dataset:palmprint, training... loss:0.68185\n",
      "[CSQ][1028/2000][18:39:28] bit:1024, dataset:palmprint, training... loss:0.68222\n",
      "[CSQ][1029/2000][18:39:37] bit:1024, dataset:palmprint, training... loss:0.68184\n",
      "[CSQ][1030/2000][18:39:45] bit:1024, dataset:palmprint, training... loss:0.68320\n",
      "[CSQ][1031/2000][18:39:54] bit:1024, dataset:palmprint, training... loss:0.68395\n",
      "[CSQ][1032/2000][18:40:03] bit:1024, dataset:palmprint, training... loss:0.68166\n",
      "[CSQ][1033/2000][18:40:11] bit:1024, dataset:palmprint, training... loss:0.68150\n",
      "[CSQ][1034/2000][18:40:19] bit:1024, dataset:palmprint, training... loss:0.68161\n",
      "[CSQ][1035/2000][18:40:27] bit:1024, dataset:palmprint, training... loss:0.68382\n",
      "[CSQ][1036/2000][18:40:36] bit:1024, dataset:palmprint, training... loss:0.68242\n",
      "[CSQ][1037/2000][18:40:43] bit:1024, dataset:palmprint, training... loss:0.68030\n",
      "[CSQ][1038/2000][18:40:52] bit:1024, dataset:palmprint, training... loss:0.68327\n",
      "[CSQ][1039/2000][18:41:00] bit:1024, dataset:palmprint, training... loss:0.68305\n",
      "[CSQ][1040/2000][18:41:09] bit:1024, dataset:palmprint, training... loss:0.68220\n",
      "[CSQ][1041/2000][18:41:17] bit:1024, dataset:palmprint, training... loss:0.68135\n",
      "[CSQ][1042/2000][18:41:26] bit:1024, dataset:palmprint, training... loss:0.68123\n",
      "[CSQ][1043/2000][18:41:34] bit:1024, dataset:palmprint, training... loss:0.68094\n",
      "[CSQ][1044/2000][18:41:42] bit:1024, dataset:palmprint, training... loss:0.68439\n",
      "[CSQ][1045/2000][18:41:51] bit:1024, dataset:palmprint, training... loss:0.68323\n",
      "[CSQ][1046/2000][18:42:00] bit:1024, dataset:palmprint, training... loss:0.67982\n",
      "[CSQ][1047/2000][18:42:08] bit:1024, dataset:palmprint, training... loss:0.68341\n",
      "[CSQ][1048/2000][18:42:16] bit:1024, dataset:palmprint, training... loss:0.68048\n",
      "[CSQ][1049/2000][18:42:25] bit:1024, dataset:palmprint, training... loss:0.68221\n",
      "[CSQ][1050/2000][18:42:33] bit:1024, dataset:palmprint, training... loss:0.68033\n",
      "[CSQ][1051/2000][18:42:41] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0033333333333333335\n",
      "[CSQ] epoch:1051, bit:1024, dataset:palmprint,eer:0.00333, Best eer: 0.00300\n",
      "\b loss:0.68067\n",
      "[CSQ][1052/2000][18:43:42] bit:1024, dataset:palmprint, training... loss:0.68125\n",
      "[CSQ][1053/2000][18:43:50] bit:1024, dataset:palmprint, training... loss:0.68034\n",
      "[CSQ][1054/2000][18:43:58] bit:1024, dataset:palmprint, training... loss:0.68200\n",
      "[CSQ][1055/2000][18:44:06] bit:1024, dataset:palmprint, training... loss:0.68267\n",
      "[CSQ][1056/2000][18:44:15] bit:1024, dataset:palmprint, training... loss:0.68233\n",
      "[CSQ][1057/2000][18:44:22] bit:1024, dataset:palmprint, training... loss:0.68252\n",
      "[CSQ][1058/2000][18:44:31] bit:1024, dataset:palmprint, training... loss:0.68158\n",
      "[CSQ][1059/2000][18:44:39] bit:1024, dataset:palmprint, training... loss:0.67962\n",
      "[CSQ][1060/2000][18:44:48] bit:1024, dataset:palmprint, training... loss:0.68081\n",
      "[CSQ][1061/2000][18:44:56] bit:1024, dataset:palmprint, training... loss:0.68001\n",
      "[CSQ][1062/2000][18:45:04] bit:1024, dataset:palmprint, training... loss:0.67932\n",
      "[CSQ][1063/2000][18:45:13] bit:1024, dataset:palmprint, training... loss:0.68109\n",
      "[CSQ][1064/2000][18:45:20] bit:1024, dataset:palmprint, training... loss:0.67836\n",
      "[CSQ][1065/2000][18:45:30] bit:1024, dataset:palmprint, training... loss:0.67919\n",
      "[CSQ][1066/2000][18:45:38] bit:1024, dataset:palmprint, training... loss:0.68047\n",
      "[CSQ][1067/2000][18:45:47] bit:1024, dataset:palmprint, training... loss:0.68032\n",
      "[CSQ][1068/2000][18:45:55] bit:1024, dataset:palmprint, training... loss:0.67941\n",
      "[CSQ][1069/2000][18:46:03] bit:1024, dataset:palmprint, training... loss:0.67930\n",
      "[CSQ][1070/2000][18:46:11] bit:1024, dataset:palmprint, training... loss:0.67972\n",
      "[CSQ][1071/2000][18:46:19] bit:1024, dataset:palmprint, training... loss:0.67894\n",
      "[CSQ][1072/2000][18:46:28] bit:1024, dataset:palmprint, training... loss:0.67946\n",
      "[CSQ][1073/2000][18:46:37] bit:1024, dataset:palmprint, training... loss:0.68002\n",
      "[CSQ][1074/2000][18:46:45] bit:1024, dataset:palmprint, training... loss:0.67930\n",
      "[CSQ][1075/2000][18:46:53] bit:1024, dataset:palmprint, training... loss:0.68107\n",
      "[CSQ][1076/2000][18:47:02] bit:1024, dataset:palmprint, training... loss:0.67909\n",
      "[CSQ][1077/2000][18:47:10] bit:1024, dataset:palmprint, training... loss:0.68211\n",
      "[CSQ][1078/2000][18:47:18] bit:1024, dataset:palmprint, training... loss:0.68132\n",
      "[CSQ][1079/2000][18:47:27] bit:1024, dataset:palmprint, training... loss:0.67907\n",
      "[CSQ][1080/2000][18:47:35] bit:1024, dataset:palmprint, training... loss:0.67859\n",
      "[CSQ][1081/2000][18:47:44] bit:1024, dataset:palmprint, training... loss:0.67923\n",
      "[CSQ][1082/2000][18:47:52] bit:1024, dataset:palmprint, training... loss:0.67926\n",
      "[CSQ][1083/2000][18:48:00] bit:1024, dataset:palmprint, training... loss:0.67953\n",
      "[CSQ][1084/2000][18:48:08] bit:1024, dataset:palmprint, training... loss:0.67980\n",
      "[CSQ][1085/2000][18:48:16] bit:1024, dataset:palmprint, training... loss:0.67932\n",
      "[CSQ][1086/2000][18:48:25] bit:1024, dataset:palmprint, training... loss:0.68015\n",
      "[CSQ][1087/2000][18:48:33] bit:1024, dataset:palmprint, training... loss:0.67903\n",
      "[CSQ][1088/2000][18:48:42] bit:1024, dataset:palmprint, training... loss:0.67936\n",
      "[CSQ][1089/2000][18:48:50] bit:1024, dataset:palmprint, training... loss:0.67606\n",
      "[CSQ][1090/2000][18:48:59] bit:1024, dataset:palmprint, training... loss:0.67904\n",
      "[CSQ][1091/2000][18:49:07] bit:1024, dataset:palmprint, training... loss:0.67891\n",
      "[CSQ][1092/2000][18:49:14] bit:1024, dataset:palmprint, training... loss:0.67887\n",
      "[CSQ][1093/2000][18:49:23] bit:1024, dataset:palmprint, training... loss:0.67996\n",
      "[CSQ][1094/2000][18:49:30] bit:1024, dataset:palmprint, training... loss:0.67998\n",
      "[CSQ][1095/2000][18:49:39] bit:1024, dataset:palmprint, training... loss:0.67745\n",
      "[CSQ][1096/2000][18:49:47] bit:1024, dataset:palmprint, training... loss:0.67999\n",
      "[CSQ][1097/2000][18:49:56] bit:1024, dataset:palmprint, training... loss:0.67994\n",
      "[CSQ][1098/2000][18:50:04] bit:1024, dataset:palmprint, training... loss:0.67807\n",
      "[CSQ][1099/2000][18:50:12] bit:1024, dataset:palmprint, training... loss:0.67947\n",
      "[CSQ][1100/2000][18:50:20] bit:1024, dataset:palmprint, training... loss:0.67914\n",
      "[CSQ][1101/2000][18:50:27] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.0033333333333333335\n",
      "[CSQ] epoch:1101, bit:1024, dataset:palmprint,eer:0.00333, Best eer: 0.00300\n",
      "\b loss:0.67754\n",
      "[CSQ][1102/2000][18:51:27] bit:1024, dataset:palmprint, training... loss:0.67996\n",
      "[CSQ][1103/2000][18:51:35] bit:1024, dataset:palmprint, training... loss:0.67698\n",
      "[CSQ][1104/2000][18:51:43] bit:1024, dataset:palmprint, training... loss:0.67820\n",
      "[CSQ][1105/2000][18:51:51] bit:1024, dataset:palmprint, training... loss:0.67886\n",
      "[CSQ][1106/2000][18:52:00] bit:1024, dataset:palmprint, training... loss:0.67915\n",
      "[CSQ][1107/2000][18:52:07] bit:1024, dataset:palmprint, training... loss:0.67964\n",
      "[CSQ][1108/2000][18:52:17] bit:1024, dataset:palmprint, training... loss:0.67882\n",
      "[CSQ][1109/2000][18:52:25] bit:1024, dataset:palmprint, training... loss:0.68070\n",
      "[CSQ][1110/2000][18:52:33] bit:1024, dataset:palmprint, training... loss:0.67850\n",
      "[CSQ][1111/2000][18:52:42] bit:1024, dataset:palmprint, training... loss:0.67743\n",
      "[CSQ][1112/2000][18:52:49] bit:1024, dataset:palmprint, training... loss:0.67725\n",
      "[CSQ][1113/2000][18:52:57] bit:1024, dataset:palmprint, training... loss:0.67800\n",
      "[CSQ][1114/2000][18:53:05] bit:1024, dataset:palmprint, training... loss:0.67827\n",
      "[CSQ][1115/2000][18:53:14] bit:1024, dataset:palmprint, training... loss:0.67726\n",
      "[CSQ][1116/2000][18:53:22] bit:1024, dataset:palmprint, training... loss:0.67674\n",
      "[CSQ][1117/2000][18:53:31] bit:1024, dataset:palmprint, training... loss:0.67761\n",
      "[CSQ][1118/2000][18:53:39] bit:1024, dataset:palmprint, training... loss:0.67731\n",
      "[CSQ][1119/2000][18:53:47] bit:1024, dataset:palmprint, training... loss:0.67919\n",
      "[CSQ][1120/2000][18:53:57] bit:1024, dataset:palmprint, training... loss:0.67599\n",
      "[CSQ][1121/2000][18:54:04] bit:1024, dataset:palmprint, training... loss:0.67825\n",
      "[CSQ][1122/2000][18:54:13] bit:1024, dataset:palmprint, training... loss:0.67680\n",
      "[CSQ][1123/2000][18:54:22] bit:1024, dataset:palmprint, training... loss:0.67758\n",
      "[CSQ][1124/2000][18:54:30] bit:1024, dataset:palmprint, training... loss:0.67861\n",
      "[CSQ][1125/2000][18:54:39] bit:1024, dataset:palmprint, training... loss:0.67826\n",
      "[CSQ][1126/2000][18:54:47] bit:1024, dataset:palmprint, training... loss:0.67732\n",
      "[CSQ][1127/2000][18:54:55] bit:1024, dataset:palmprint, training... loss:0.67784\n",
      "[CSQ][1128/2000][18:55:02] bit:1024, dataset:palmprint, training... loss:0.67972\n",
      "[CSQ][1129/2000][18:55:11] bit:1024, dataset:palmprint, training... loss:0.67741\n",
      "[CSQ][1130/2000][18:55:19] bit:1024, dataset:palmprint, training... loss:0.67990\n",
      "[CSQ][1131/2000][18:55:28] bit:1024, dataset:palmprint, training... loss:0.67839\n",
      "[CSQ][1132/2000][18:55:36] bit:1024, dataset:palmprint, training... loss:0.67740\n",
      "[CSQ][1133/2000][18:55:45] bit:1024, dataset:palmprint, training... loss:0.67762\n",
      "[CSQ][1134/2000][18:55:53] bit:1024, dataset:palmprint, training... loss:0.67731\n",
      "[CSQ][1135/2000][18:56:01] bit:1024, dataset:palmprint, training... loss:0.67650\n",
      "[CSQ][1136/2000][18:56:10] bit:1024, dataset:palmprint, training... loss:0.67895\n",
      "[CSQ][1137/2000][18:56:17] bit:1024, dataset:palmprint, training... loss:0.67663\n",
      "[CSQ][1138/2000][18:56:26] bit:1024, dataset:palmprint, training... loss:0.67866\n",
      "[CSQ][1139/2000][18:56:34] bit:1024, dataset:palmprint, training... loss:0.67516\n",
      "[CSQ][1140/2000][18:56:43] bit:1024, dataset:palmprint, training... loss:0.67610\n",
      "[CSQ][1141/2000][18:56:52] bit:1024, dataset:palmprint, training... loss:0.67722\n",
      "[CSQ][1142/2000][18:57:00] bit:1024, dataset:palmprint, training... loss:0.67643\n",
      "[CSQ][1143/2000][18:57:08] bit:1024, dataset:palmprint, training... loss:0.67655\n",
      "[CSQ][1144/2000][18:57:16] bit:1024, dataset:palmprint, training... loss:0.67831\n",
      "[CSQ][1145/2000][18:57:24] bit:1024, dataset:palmprint, training... loss:0.67880\n",
      "[CSQ][1146/2000][18:57:32] bit:1024, dataset:palmprint, training... loss:0.67720\n",
      "[CSQ][1147/2000][18:57:40] bit:1024, dataset:palmprint, training... loss:0.67543\n",
      "[CSQ][1148/2000][18:57:48] bit:1024, dataset:palmprint, training... loss:0.67865\n",
      "[CSQ][1149/2000][18:57:57] bit:1024, dataset:palmprint, training... loss:0.67666\n",
      "[CSQ][1150/2000][18:58:05] bit:1024, dataset:palmprint, training... loss:0.67629\n",
      "[CSQ][1151/2000][18:58:14] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.003\n",
      "[CSQ] epoch:1151, bit:1024, dataset:palmprint,eer:0.00300, Best eer: 0.00300\n",
      "\b loss:0.67685\n",
      "[CSQ][1152/2000][18:59:12] bit:1024, dataset:palmprint, training... loss:0.67774\n",
      "[CSQ][1153/2000][18:59:20] bit:1024, dataset:palmprint, training... loss:0.67463\n",
      "[CSQ][1154/2000][18:59:28] bit:1024, dataset:palmprint, training... loss:0.67675\n",
      "[CSQ][1155/2000][18:59:37] bit:1024, dataset:palmprint, training... loss:0.67571\n",
      "[CSQ][1156/2000][18:59:45] bit:1024, dataset:palmprint, training... loss:0.67478\n",
      "[CSQ][1157/2000][18:59:54] bit:1024, dataset:palmprint, training... loss:0.67729\n",
      "[CSQ][1158/2000][19:00:02] bit:1024, dataset:palmprint, training... loss:0.67772\n",
      "[CSQ][1159/2000][19:00:11] bit:1024, dataset:palmprint, training... loss:0.67504\n",
      "[CSQ][1160/2000][19:00:19] bit:1024, dataset:palmprint, training... loss:0.67549\n",
      "[CSQ][1161/2000][19:00:27] bit:1024, dataset:palmprint, training... loss:0.67773\n",
      "[CSQ][1162/2000][19:00:35] bit:1024, dataset:palmprint, training... loss:0.67608\n",
      "[CSQ][1163/2000][19:00:43] bit:1024, dataset:palmprint, training... loss:0.67666\n",
      "[CSQ][1164/2000][19:00:52] bit:1024, dataset:palmprint, training... loss:0.67310\n",
      "[CSQ][1165/2000][19:01:00] bit:1024, dataset:palmprint, training... loss:0.67423\n",
      "[CSQ][1166/2000][19:01:08] bit:1024, dataset:palmprint, training... loss:0.67677\n",
      "[CSQ][1167/2000][19:01:17] bit:1024, dataset:palmprint, training... loss:0.67580\n",
      "[CSQ][1168/2000][19:01:26] bit:1024, dataset:palmprint, training... loss:0.67397\n",
      "[CSQ][1169/2000][19:01:34] bit:1024, dataset:palmprint, training... loss:0.67468\n",
      "[CSQ][1170/2000][19:01:42] bit:1024, dataset:palmprint, training... loss:0.67702\n",
      "[CSQ][1171/2000][19:01:51] bit:1024, dataset:palmprint, training... loss:0.67360\n",
      "[CSQ][1172/2000][19:01:59] bit:1024, dataset:palmprint, training... loss:0.67635\n",
      "[CSQ][1173/2000][19:02:08] bit:1024, dataset:palmprint, training... loss:0.67864\n",
      "[CSQ][1174/2000][19:02:16] bit:1024, dataset:palmprint, training... loss:0.67456\n",
      "[CSQ][1175/2000][19:02:24] bit:1024, dataset:palmprint, training... loss:0.67510\n",
      "[CSQ][1176/2000][19:02:32] bit:1024, dataset:palmprint, training... loss:0.67540\n",
      "[CSQ][1177/2000][19:02:40] bit:1024, dataset:palmprint, training... loss:0.67511\n",
      "[CSQ][1178/2000][19:02:49] bit:1024, dataset:palmprint, training... loss:0.67427\n",
      "[CSQ][1179/2000][19:02:57] bit:1024, dataset:palmprint, training... loss:0.67404\n",
      "[CSQ][1180/2000][19:03:05] bit:1024, dataset:palmprint, training... loss:0.67606\n",
      "[CSQ][1181/2000][19:03:12] bit:1024, dataset:palmprint, training... loss:0.67170\n",
      "[CSQ][1182/2000][19:03:20] bit:1024, dataset:palmprint, training... loss:0.67817\n",
      "[CSQ][1183/2000][19:03:28] bit:1024, dataset:palmprint, training... loss:0.67439\n",
      "[CSQ][1184/2000][19:03:36] bit:1024, dataset:palmprint, training... loss:0.67281\n",
      "[CSQ][1185/2000][19:03:45] bit:1024, dataset:palmprint, training... loss:0.67295\n",
      "[CSQ][1186/2000][19:03:53] bit:1024, dataset:palmprint, training... loss:0.67649\n",
      "[CSQ][1187/2000][19:04:01] bit:1024, dataset:palmprint, training... loss:0.67511\n",
      "[CSQ][1188/2000][19:04:09] bit:1024, dataset:palmprint, training... loss:0.67510\n",
      "[CSQ][1189/2000][19:04:18] bit:1024, dataset:palmprint, training... loss:0.67491\n",
      "[CSQ][1190/2000][19:04:25] bit:1024, dataset:palmprint, training... loss:0.67479\n",
      "[CSQ][1191/2000][19:04:34] bit:1024, dataset:palmprint, training... loss:0.67391\n",
      "[CSQ][1192/2000][19:04:43] bit:1024, dataset:palmprint, training... loss:0.67627\n",
      "[CSQ][1193/2000][19:04:52] bit:1024, dataset:palmprint, training... loss:0.67649\n",
      "[CSQ][1194/2000][19:05:01] bit:1024, dataset:palmprint, training... loss:0.67697\n",
      "[CSQ][1195/2000][19:05:09] bit:1024, dataset:palmprint, training... loss:0.67563\n",
      "[CSQ][1196/2000][19:05:18] bit:1024, dataset:palmprint, training... loss:0.67536\n",
      "[CSQ][1197/2000][19:05:26] bit:1024, dataset:palmprint, training... loss:0.67557\n",
      "[CSQ][1198/2000][19:05:36] bit:1024, dataset:palmprint, training... loss:0.67416\n",
      "[CSQ][1199/2000][19:05:44] bit:1024, dataset:palmprint, training... loss:0.67489\n",
      "[CSQ][1200/2000][19:05:53] bit:1024, dataset:palmprint, training... loss:0.67234\n",
      "[CSQ][1201/2000][19:06:02] bit:1024, dataset:palmprint, training....- feats shape: (2000, 1024)\n",
      "- GT shape: (2000, 500)\n",
      "0.004\n",
      "[CSQ] epoch:1201, bit:1024, dataset:palmprint,eer:0.00400, Best eer: 0.00300\n",
      "\b loss:0.67287\n",
      "[CSQ][1202/2000][19:07:08] bit:1024, dataset:palmprint, training... loss:0.67500\n",
      "[CSQ][1203/2000][19:07:16] bit:1024, dataset:palmprint, training... loss:0.67459\n",
      "[CSQ][1204/2000][19:07:24] bit:1024, dataset:palmprint, training... loss:0.67288\n",
      "[CSQ][1205/2000][19:07:32] bit:1024, dataset:palmprint, training... loss:0.67618\n",
      "[CSQ][1206/2000][19:07:40] bit:1024, dataset:palmprint, training... loss:0.67401\n",
      "[CSQ][1207/2000][19:07:50] bit:1024, dataset:palmprint, training... loss:0.67209\n",
      "[CSQ][1208/2000][19:07:58] bit:1024, dataset:palmprint, training... loss:0.67579\n",
      "[CSQ][1209/2000][19:08:07] bit:1024, dataset:palmprint, training... loss:0.67330\n",
      "[CSQ][1210/2000][19:08:15] bit:1024, dataset:palmprint, training... loss:0.67373\n",
      "[CSQ][1211/2000][19:08:24] bit:1024, dataset:palmprint, training...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_274233/4263127081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Best_eer = 1.0\n",
    "\n",
    "config[\"epoch\"] = 2000\n",
    "for epoch in range(config[\"epoch\"]):\n",
    "\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "    print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "        config[\"info\"], epoch + 1, config[\"epoch\"], current_time, bit, config[\"dataset\"]), end=\"\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_idx, img in enumerate(train_loader):\n",
    "        image = img[0].to(device, dtype=torch.float)\n",
    "        label = img[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        u = model(image)\n",
    "\n",
    "        loss = criterion(u, label.float(), 0, config)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    if epoch % 50 ==0:\n",
    "        eer = test(model,test_loader)\n",
    "        if eer < Best_eer:\n",
    "            Best_eer = eer\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': Best_eer,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, True, filename='GCN1channelhashing.pth.tar', remark='GCN1channelhashing')\n",
    "        print(\"%s epoch:%d, bit:%d, dataset:%s,eer:%.5f, Best eer: %.5f\" % (\n",
    "            config[\"info\"], epoch + 1, bit, config[\"dataset\"], eer, Best_eer))\n",
    "    print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))##loss:0.625\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c207d-c655-4440-a022-7b52fb8f09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if epoch % 50 ==0:\n",
    "# save_checkpoint({\n",
    "#     'epoch': epoch + 1,\n",
    "#     'state_dict': net.state_dict(),\n",
    "#     'best_prec1': 0,\n",
    "#     'optimizer' : optimizer.state_dict(),\n",
    "# }, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5106d6-538f-49b0-8727-639819292ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, img in tqdm.tqdm(enumerate(train_loader)):\n",
    "#     print((img[0].size(), img[1].size()))\n",
    "#     break\n",
    "    \n",
    "# for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "#     print((img[0].size(), img[1].size()))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b1563-8fd3-4c32-b642-e8e5e0797abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07f76d-3767-4c30-a673-4ade0f99d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img[0].permute(0, 3, 1, 2).size())\n",
    "# print(img[0].permute(0, 3, 1, 2).double().dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac54206-2374-4eca-89a9-9dedd82149ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b584df4-e536-405f-b2a8-aecca260adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = model(img[0].permute(0, 3, 1, 2).float())\n",
    "# print(outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ae5d-1f0e-4125-9030-2d3e7a03eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best_gcn2wayca.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4c16a-6902-4aec-8243-638a3d87d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a085d-7905-4d95-a92b-dca883df4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=False)  # ,prefetch_factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20120f-d310-4dc6-8b6c-3f00a94f442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break\n",
    "print(img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2253928-e81e-4878-b0a0-09cde7302ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### HELPER FUNCTION FOR FEATURE EXTRACTION\n",
    "# # print(model)\n",
    "\n",
    "# def get_features(name):\n",
    "#     def hook(model, input, output):\n",
    "#         features[name] = output.detach()\n",
    "#     return hook\n",
    "# ##### REGISTER HOOK\n",
    "# model.model_print[16].register_forward_hook(get_features('feats'))\n",
    "# # model.fc1.register_forward_hook(get_features('feats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1841c-fd4b-4c4e-b867-77a297778675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(net,test_loader):\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    FEATS = []\n",
    "    GT = []\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            output = net(rbn)\n",
    "            FEATS.append(output.cpu().numpy())\n",
    "#             FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "    GCNFEATS = np.concatenate(FEATS)\n",
    "    GT = np.concatenate(GT)\n",
    "    print('- feats shape:', GCNFEATS.shape)\n",
    "    print('- GT shape:', GT.shape)\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    def cossim(a,b):\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    pred_scores = []\n",
    "    gt_label = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(3000)):\n",
    "        for j in range(i+1,3000):\n",
    "            # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "            a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "            pred_scores.append(a)\n",
    "            gt_label.append(i//6 == j//6)\n",
    "\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    gt_label = np.array(gt_label)\n",
    "\n",
    "    Gen = pred_scores[gt_label]\n",
    "    Imp = pred_scores[gt_label==False]\n",
    "    Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "#     import seaborn as sns\n",
    "#     sns.distplot(Gen,  kde=False, label='Gen')\n",
    "#     # df =gapminder[gapminder.continent == 'Americas']\n",
    "#     sns.distplot(Imp,  kde=False,label='Imp')\n",
    "#     # Plot formatting\n",
    "#     plt.legend(prop={'size': 12})\n",
    "#     plt.title('Life Expectancy of Two Continents')\n",
    "#     plt.xlabel('Life Exp (years)')\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "    from pyeer.eer_info import get_eer_stats\n",
    "    from pyeer.report import generate_eer_report, export_error_rates\n",
    "    from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "    # Calculating stats for classifier A\n",
    "    stats_a = get_eer_stats(Gen, Imp)\n",
    "    print(stats_a.eer)\n",
    "\n",
    "    return stats_a.eer\n",
    "\n",
    "test(net,test_loader)\n",
    "##### INSPECT FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b1964-473b-4444-b003-4fcf3cd1242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# np.save('FEATS_gcn2wayca.npy', GCNFEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8c159-8bc4-4f96-869a-98be084c706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATSRES18 = np.load('FEATS_res18.npy')\n",
    "# print(FEATSRES18.shape)\n",
    "# FEATSRES18 = np.squeeze(FEATSRES18);\n",
    "# print(FEATSRES18.shape)\n",
    "# # # NewFEATS = np.dstack((FEATS,FEATSRES18))\n",
    "# # FEATS = np.concatenate((FEATS,FEATSRES18), axis=1)\n",
    "# # print(NewFEATS.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc60cae-2740-45df-9277-817c10f4fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "pred_scores = []\n",
    "gt_label = []\n",
    "\n",
    "for i in tqdm.tqdm(range(3000)):\n",
    "    for j in range(i+1,3000):\n",
    "        # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "        a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "#         b = cossim(FEATSRES18[i,:],FEATSRES18[j,:])\n",
    "#         pred_scores.append((1.8*a+0.2*b)/2)\n",
    "        pred_scores.append(a)\n",
    "#         pred_scores.append(b)\n",
    "        gt_label.append(i//6 == j//6)\n",
    "        \n",
    "\n",
    "# for i in tqdm.tqdm(range(600)):\n",
    "#         # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "#         pred_scores.append(cossim(FEATS[i,:],FEATS[]))\n",
    "#         # gt_label.append(i//12 == j//12)\n",
    "pred_scores = np.array(pred_scores)\n",
    "gt_label = np.array(gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbcf98-18e0-4fda-8bfd-53925629bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Gen = pred_scores[gt_label]\n",
    "Imp = pred_scores[gt_label==False]\n",
    "Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.distplot(Gen,  kde=False, label='Gen')\n",
    "# df =gapminder[gapminder.continent == 'Americas']\n",
    "sns.distplot(Imp,  kde=False,label='Imp')\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.title('Life Expectancy of Two Continents')\n",
    "plt.xlabel('Life Exp (years)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from pyeer.report import generate_eer_report, export_error_rates\n",
    "from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "# Calculating stats for classifier A\n",
    "stats_a = get_eer_stats(Gen, Imp)\n",
    "print(stats_a.eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673f170-9bc8-499b-a6c2-29209e84ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)#, positive_label\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "\n",
    "eer = compute_eer(gt_label, pred_scores)\n",
    "print('The equal error rate is {:.3f}'.format(eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3bd27-046c-42d5-a20d-07a9a4343902",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Gen \n",
    "del Imp\n",
    "del pred_scores\n",
    "del gt_label\n",
    "del GCNFEATS\n",
    "del GT\n",
    "del test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
