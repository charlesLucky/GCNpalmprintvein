{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c20581d-f5c4-47a1-8b6e-9289655b0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from utils import accuracy, AverageMeter, save_checkpoint, visualize_graph, get_parameters_size\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from net_factory import get_network_fn\n",
    "\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# del test_loader\n",
    "import tqdm\n",
    "# test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6983cbb4-bc52-4e7d-9ebf-a700dd4ba4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GCN MNIST Training')\n",
    "\n",
    "parser.add_argument('--epochs', default=50, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--pretrained', default='', type=str, metavar='PATH',\n",
    "                    help='path to pretrained checkpoint (default: none)')\n",
    "parser.add_argument('--gpu', default=0, type=int,\n",
    "                    metavar='N', help='GPU device ID (default: -1)')\n",
    "parser.add_argument('--dataset_dir', default='../../MNIST', type=str, metavar='PATH',\n",
    "                    help='path to dataset (default: ../MNIST)')\n",
    "parser.add_argument('--comment', default='', type=str, metavar='INFO',\n",
    "                    help='Extra description for tensorboard')\n",
    "parser.add_argument('--model', default='gcn', type=str, metavar='NETWORK',\n",
    "                    help='Network to train')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "use_cuda = (args.gpu >= 0) and torch.cuda.is_available()\n",
    "best_prec1 = 0\n",
    "writer = SummaryWriter(comment='_'+args.model+'_'+args.comment)\n",
    "iteration = 0\n",
    "\n",
    "# # from loaddataset import load_data\n",
    "# from loaddataset import load_data\n",
    "\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n",
    "# test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cdaa3e-43a8-4a65-b846-dbc1f66065e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-nas/Gabor_CNN_PyTorch/gcn/layers/GConv.py:67: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  for i in range(x.size(1)):\n",
      "/root/autodl-nas/Gabor_CNN_PyTorch/demo/net_factory.py:194: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return x + self.pe[:, :, :x.size(2), :x.size(3)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model\n",
    "model = get_network_fn(name='GCN2wayHashingsimple')#GCNCNN\n",
    "# print(model)\n",
    "\n",
    "# Try to visulize the model\n",
    "try:\n",
    "\tvisualize_graph(model, writer, input_size=(1, 5, 128, 128))\n",
    "except:\n",
    "\tprint('\\nNetwork Visualization Failed! But the training procedure continue.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acf49d1-1fd2-4255-ad2d-759efd90d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(torch.rand(5, 5, 128, 128))\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce89666-4993-4a4a-9625-722d15d45121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best_gcn2waycqhashing.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained,map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n",
    "# print(checkpoint['best_prec1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29785785-5148-4172-abf0-46105050db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSQLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(CSQLoss, self).__init__()\n",
    "        self.is_single_label = config[\"dataset\"] not in {\"nuswide_21\", \"nuswide_21_m\", \"coco\"}\n",
    "        self.hash_targets = self.get_hash_targets(config[\"n_class\"], bit).to(config[\"device\"])\n",
    "        self.multi_label_random_center = torch.randint(2, (bit,)).float().to(config[\"device\"])\n",
    "        self.criterion = torch.nn.BCELoss().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        u = u.tanh()\n",
    "        hash_center = self.label2center(y)\n",
    "        center_loss = self.criterion(0.5 * (u + 1), 0.5 * (hash_center + 1))\n",
    "\n",
    "        Q_loss = (u.abs() - 1).pow(2).mean()\n",
    "        return center_loss + config[\"lambda\"] * Q_loss\n",
    "\n",
    "    def label2center(self, y):\n",
    "        if self.is_single_label:\n",
    "            hash_center = self.hash_targets[y.argmax(axis=1)]\n",
    "        else:\n",
    "            # to get sign no need to use mean, use sum here\n",
    "            center_sum = y @ self.hash_targets\n",
    "            random_center = self.multi_label_random_center.repeat(center_sum.shape[0], 1)\n",
    "            center_sum[center_sum == 0] = random_center[center_sum == 0]\n",
    "            hash_center = 2 * (center_sum > 0).float() - 1\n",
    "        return hash_center\n",
    "\n",
    "    # use algorithm 1 to generate hash centers\n",
    "    def get_hash_targets(self, n_class, bit):\n",
    "        H_K = hadamard(bit)\n",
    "        H_2K = np.concatenate((H_K, -H_K), 0)\n",
    "        hash_targets = torch.from_numpy(H_2K[:n_class]).float()\n",
    "\n",
    "        if H_2K.shape[0] < n_class:\n",
    "            hash_targets.resize_(n_class, bit)\n",
    "            for k in range(20):\n",
    "                for index in range(H_2K.shape[0], n_class):\n",
    "                    ones = torch.ones(bit)\n",
    "                    # Bernouli distribution\n",
    "                    sa = random.sample(list(range(bit)), bit // 2)\n",
    "                    ones[sa] = -1\n",
    "                    hash_targets[index] = ones\n",
    "                # to find average/min  pairwise distance\n",
    "                c = []\n",
    "                for i in range(n_class):\n",
    "                    for j in range(n_class):\n",
    "                        if i < j:\n",
    "                            TF = sum(hash_targets[i] != hash_targets[j])\n",
    "                            c.append(TF)\n",
    "                c = np.array(c)\n",
    "\n",
    "                # choose min(c) in the range of K/4 to K/3\n",
    "                # see in https://github.com/yuanli2333/Hadamard-Matrix-for-hashing/issues/1\n",
    "                # but it is hard when bit is  small\n",
    "                if c.min() > bit / 4 and c.mean() >= bit / 2:\n",
    "                    print(c.min(), c.mean())\n",
    "                    break\n",
    "        return hash_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010b059e-a572-4e84-b2c8-5f5e17d8f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model.fc = nn.Linear(num_ftrs, 450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b1d92b-24b6-4edb-b68e-fba7bc38aef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# read image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "ms_polyu_path = 'dataset/MS_PolyU/'\n",
    "casia_path = 'dataset/CASIA-Multi-Spectral-PalmprintV1/images/'\n",
    "\n",
    "r_img_path = ms_polyu_path + 'Red_ind/'\n",
    "b_img_path =  ms_polyu_path + 'Blue_ind/'\n",
    "n_img_path =  ms_polyu_path + 'NIR_ind/'\n",
    "g_img_path =  ms_polyu_path + 'Green_ind/'\n",
    "\n",
    "################ DATASET CLASS\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] \n",
    "# one_hot_embedding(1, 10)\n",
    "def part_init(istrain=True):\n",
    "    r_list = []\n",
    "    b_list = []\n",
    "    vein_list = []\n",
    "    prints_list = []\n",
    "    labels = []\n",
    "    \n",
    "        # split all data into train, test data\n",
    "    train_ratio = 0.9\n",
    "    train_num = int(500 * train_ratio)\n",
    "    print(\"split train users:\",train_num)\n",
    "    if istrain:\n",
    "        for i in tqdm.tqdm(range(train_num)):\n",
    "            for j in range(12):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "                g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "                b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "                n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "                rb = r_normed - b_normed * 0.5\n",
    "                rb =  (rb * 128+128).astype(np.uint8)\n",
    "\n",
    "                imgprint = np.dstack((r_img,g_img,b_img))\n",
    "                imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "                vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                labels.append(one_hot_embedding(i, train_num))\n",
    "#                 labels.append(i)\n",
    "    else:\n",
    "        for i in tqdm.tqdm(range(train_num,500)):\n",
    "            for j in range(12):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "                g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "                b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "                n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "                rb = r_normed - b_normed * 0.5\n",
    "                rb =  (rb * 128+128).astype(np.uint8)\n",
    "                imgprint = np.dstack((r_img,g_img,b_img))\n",
    "                imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "                vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "#                 labels.append(one_hot_embedding(i, train_num))\n",
    "                labels.append(i)\n",
    "\n",
    "\n",
    "\n",
    "    # return np.array(r_list), np.array(b_list), np.array(n_list), np.array(labels),np.array(r_list_test), np.array(b_list_test), np.array(n_list_test), np.array(labels_test)\n",
    "    return  vein_list,prints_list, labels\n",
    "\n",
    "# r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "class load_data(Dataset):\n",
    "    \"\"\"Loads the Data.\"\"\"\n",
    "    def __init__(self, training=True):\n",
    "\n",
    "        self.training = training\n",
    "#         r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomPerspective(),\n",
    "        transforms.RandomAffine(30),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        self.transform_test = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        if self.training:\n",
    "            print('\\n...... Train files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels= part_init(istrain=True)\n",
    "            print('\\nTrain files loaded ......\\n')\n",
    "        else:\n",
    "            print('\\n...... Test files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels = part_init(istrain=False)\n",
    "            print('\\nTest files loaded ......\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vein_list)\n",
    "\n",
    "         \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.training:\n",
    "            prints_img = self.transform(self.prints_list[idx])\n",
    "            vein_img = self.transform(self.vein_list[idx])\n",
    "        else:\n",
    "            prints_img = self.transform_test(self.prints_list[idx])\n",
    "            vein_img = self.transform_test(self.vein_list[idx])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        n_img = np.dstack((prints_img[0,:,:],prints_img[1,:,:],prints_img[2,:,:],vein_img[0,:,:],vein_img[1,:,:]))\n",
    "        \n",
    "        return n_img,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33159f71-22b4-4950-9972-b2cfa3d3277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda': 0.1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[CSQ]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 64, 'net': 'w', 'dataset': 'palmprint', 'n_class': 450, 'epoch': 500, 'test_map': 10, 'device': device(type='cuda', index=0), 'bit_list': [1024]}\n"
     ]
    }
   ],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"lambda\": 0.1,\n",
    "        \"optimizer\": {\"type\": optim.RMSprop, \"optim_params\": {\"lr\": 1e-5, \"weight_decay\": 10 ** -5}},\n",
    "        \"info\": \"[CSQ]\",\n",
    "        \"resize_size\": 256,\n",
    "        \"crop_size\": 224,\n",
    "        \"batch_size\": 64,\n",
    "        # \"net\": AlexNet,\n",
    "        \"net\": \"w\",\n",
    "        \"dataset\": \"palmprint\",\n",
    "        \"n_class\":450,\n",
    "        # \"dataset\": \"imagenet\",\n",
    "        # \"dataset\": \"coco\",\n",
    "        # \"dataset\": \"nuswide_21\",\n",
    "        # \"dataset\": \"nuswide_21_m\",\n",
    "        \"epoch\": 500,\n",
    "        \"test_map\": 10,\n",
    "        # \"device\":torch.device(\"cpu\"),\n",
    "        \"device\": torch.device(\"cuda:0\"),\n",
    "        \"bit_list\": [1024],\n",
    "    }\n",
    "#     config = config_dataset(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "print(config)\n",
    "bit = 1024\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d7fd0b-2f15-4bd8-91ab-f1e224e7e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...... Train files loading\n",
      "\n",
      "split train users: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [01:16<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train files loaded ......\n",
      "\n",
      "\n",
      "...... Test files loading\n",
      "\n",
      "split train users: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 16.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test files loaded ......\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset = get_data(config)\n",
    "batch_size = 300\n",
    "train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,prefetch_factor=8)  # ,prefetch_factor=2\n",
    "test_loader = DataLoader(load_data(training=False), batch_size=64, shuffle=False)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6306800-6cbc-456e-bea1-66480629db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 32\n",
    "dataset_loader = train_loader\n",
    "num_train = 4000\n",
    "num_test = 2000\n",
    "num_dataset = 6000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62da0f6d-5f5e-418d-9c04-28ed7f96ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ae9993-d8d2-44b0-bae1-4458ba730ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "# from network import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.linalg import hadamard  # direct import  hadamrd matrix from scipy\n",
    "import random\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "\n",
    "class CSQLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(CSQLoss, self).__init__()\n",
    "        self.is_single_label = config[\"dataset\"] not in {\"nuswide_21\", \"nuswide_21_m\", \"coco\"}\n",
    "        self.hash_targets = self.get_hash_targets(config[\"n_class\"], bit).to(config[\"device\"])\n",
    "        self.multi_label_random_center = torch.randint(2, (bit,)).float().to(config[\"device\"])\n",
    "        self.criterion = torch.nn.BCELoss().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        u = u.tanh()\n",
    "        hash_center = self.label2center(y)\n",
    "        center_loss = self.criterion(0.5 * (u + 1), 0.5 * (hash_center + 1))\n",
    "\n",
    "        Q_loss = (u.abs() - 1).pow(2).mean()\n",
    "        return center_loss + config[\"lambda\"] * Q_loss\n",
    "\n",
    "    def label2center(self, y):\n",
    "        if self.is_single_label:\n",
    "            hash_center = self.hash_targets[y.argmax(axis=1)]\n",
    "        else:\n",
    "            # to get sign no need to use mean, use sum here\n",
    "            center_sum = y @ self.hash_targets\n",
    "            random_center = self.multi_label_random_center.repeat(center_sum.shape[0], 1)\n",
    "            center_sum[center_sum == 0] = random_center[center_sum == 0]\n",
    "            hash_center = 2 * (center_sum > 0).float() - 1\n",
    "        return hash_center\n",
    "\n",
    "    # use algorithm 1 to generate hash centers\n",
    "    def get_hash_targets(self, n_class, bit):\n",
    "        H_K = hadamard(bit)\n",
    "        H_2K = np.concatenate((H_K, -H_K), 0)\n",
    "        hash_targets = torch.from_numpy(H_2K[:n_class]).float()\n",
    "\n",
    "        if H_2K.shape[0] < n_class:\n",
    "            hash_targets.resize_(n_class, bit)\n",
    "            for k in range(20):\n",
    "                for index in range(H_2K.shape[0], n_class):\n",
    "                    ones = torch.ones(bit)\n",
    "                    # Bernouli distribution\n",
    "                    sa = random.sample(list(range(bit)), bit // 2)\n",
    "                    ones[sa] = -1\n",
    "                    hash_targets[index] = ones\n",
    "                # to find average/min  pairwise distance\n",
    "                c = []\n",
    "                for i in range(n_class):\n",
    "                    for j in range(n_class):\n",
    "                        if i < j:\n",
    "                            TF = sum(hash_targets[i] != hash_targets[j])\n",
    "                            c.append(TF)\n",
    "                c = np.array(c)\n",
    "\n",
    "                # choose min(c) in the range of K/4 to K/3\n",
    "                # see in https://github.com/yuanli2333/Hadamard-Matrix-for-hashing/issues/1\n",
    "                # but it is hard when bit is  small\n",
    "                if c.min() > bit / 4 and c.mean() >= bit / 2:\n",
    "                    print(c.min(), c.mean())\n",
    "                    break\n",
    "        return hash_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fbdca84-303c-405a-b2b7-31d138e9298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4d94-f3ec-4d50-915e-3e60310f83f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92f556f-0fd8-4248-894e-db92eb2062cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_train\"] = num_train\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = config[\"optimizer\"][\"type\"](model.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
    "\n",
    "criterion = CSQLoss(config, bit)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=args.momentum, weight_decay=3e-05)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0136acba-d04b-4a4e-b76a-fa22ea3bf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37e5dc6d-0958-4215-ba97-1a473024789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99435958 0.10606145]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "A = np.random.randn(1,2)*10\n",
    "# print(A)\n",
    "# print(normalized(A,0))\n",
    "print(normalized(A,1))# ok verified\n",
    "\n",
    "\n",
    "def test(net,test_loader):\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    FEATS = []\n",
    "    GT = []\n",
    "    net.eval()\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            output = net(rbn)\n",
    "            FEATS.append(output.cpu().numpy())\n",
    "#             FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "    GCNFEATS = np.concatenate(FEATS)\n",
    "    GT = np.concatenate(GT)\n",
    "    GCNFEATS = normalized(GCNFEATS,1)\n",
    "    print('- feats shape:', GCNFEATS.shape)\n",
    "    print('- GT shape:', GT.shape)\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    def cossim(a,b):\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    pred_scores = []\n",
    "    gt_label = []\n",
    "\n",
    "    for i in range(600):\n",
    "        for j in range(i+1,600):\n",
    "            # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "            a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "            pred_scores.append(a)\n",
    "            gt_label.append(i//12 == j//12)\n",
    "\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    gt_label = np.array(gt_label)\n",
    "\n",
    "    Gen = pred_scores[gt_label]\n",
    "    Imp = pred_scores[gt_label==False]\n",
    "    Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "#     import seaborn as sns\n",
    "#     sns.distplot(Gen,  kde=False, label='Gen')\n",
    "#     # df =gapminder[gapminder.continent == 'Americas']\n",
    "#     sns.distplot(Imp,  kde=False,label='Imp')\n",
    "#     # Plot formatting\n",
    "#     plt.legend(prop={'size': 12})\n",
    "#     plt.title('Life Expectancy of Two Continents')\n",
    "#     plt.xlabel('Life Exp (years)')\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "    from pyeer.eer_info import get_eer_stats\n",
    "    from pyeer.report import generate_eer_report, export_error_rates\n",
    "    from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "    # Calculating stats for classifier A\n",
    "    stats_a = get_eer_stats(Gen, Imp)\n",
    "    print(stats_a.eer)\n",
    "\n",
    "    return stats_a.eer\n",
    "\n",
    "##### INSPECT FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53117228-f6d8-4b16-b4b1-5cc939b42d58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSQ][ 1/2000][17:06:50] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.10272727272727272\n",
      "[CSQ] epoch:1, bit:1024, dataset:palmprint,eer:0.10273, Best eer: 0.10273\n",
      "\b loss:1.29369\n",
      "[CSQ][ 2/2000][17:07:33] bit:1024, dataset:palmprint, training... loss:1.11183\n",
      "[CSQ][ 3/2000][17:07:48] bit:1024, dataset:palmprint, training... loss:1.07621\n",
      "[CSQ][ 4/2000][17:08:03] bit:1024, dataset:palmprint, training... loss:1.04885\n",
      "[CSQ][ 5/2000][17:08:19] bit:1024, dataset:palmprint, training... loss:1.02559\n",
      "[CSQ][ 6/2000][17:08:33] bit:1024, dataset:palmprint, training... loss:1.00490\n",
      "[CSQ][ 7/2000][17:08:48] bit:1024, dataset:palmprint, training... loss:0.98826\n",
      "[CSQ][ 8/2000][17:09:03] bit:1024, dataset:palmprint, training... loss:0.97134\n",
      "[CSQ][ 9/2000][17:09:18] bit:1024, dataset:palmprint, training... loss:0.95691\n",
      "[CSQ][10/2000][17:09:33] bit:1024, dataset:palmprint, training... loss:0.94376\n",
      "[CSQ][11/2000][17:09:48] bit:1024, dataset:palmprint, training... loss:0.93170\n",
      "[CSQ][12/2000][17:10:03] bit:1024, dataset:palmprint, training... loss:0.91968\n",
      "[CSQ][13/2000][17:10:18] bit:1024, dataset:palmprint, training... loss:0.90704\n",
      "[CSQ][14/2000][17:10:32] bit:1024, dataset:palmprint, training... loss:0.89805\n",
      "[CSQ][15/2000][17:10:48] bit:1024, dataset:palmprint, training... loss:0.88835\n",
      "[CSQ][16/2000][17:11:03] bit:1024, dataset:palmprint, training... loss:0.88023\n",
      "[CSQ][17/2000][17:11:18] bit:1024, dataset:palmprint, training... loss:0.87238\n",
      "[CSQ][18/2000][17:11:33] bit:1024, dataset:palmprint, training... loss:0.86367\n",
      "[CSQ][19/2000][17:11:48] bit:1024, dataset:palmprint, training... loss:0.85683\n",
      "[CSQ][20/2000][17:12:03] bit:1024, dataset:palmprint, training... loss:0.85066\n",
      "[CSQ][21/2000][17:12:18] bit:1024, dataset:palmprint, training... loss:0.84506\n",
      "[CSQ][22/2000][17:12:33] bit:1024, dataset:palmprint, training... loss:0.83924\n",
      "[CSQ][23/2000][17:12:47] bit:1024, dataset:palmprint, training... loss:0.83382\n",
      "[CSQ][24/2000][17:13:03] bit:1024, dataset:palmprint, training... loss:0.82975\n",
      "[CSQ][25/2000][17:13:18] bit:1024, dataset:palmprint, training... loss:0.82431\n",
      "[CSQ][26/2000][17:13:33] bit:1024, dataset:palmprint, training... loss:0.82068\n",
      "[CSQ][27/2000][17:13:48] bit:1024, dataset:palmprint, training... loss:0.81685\n",
      "[CSQ][28/2000][17:14:03] bit:1024, dataset:palmprint, training... loss:0.81467\n",
      "[CSQ][29/2000][17:14:18] bit:1024, dataset:palmprint, training... loss:0.81115\n",
      "[CSQ][30/2000][17:14:33] bit:1024, dataset:palmprint, training... loss:0.80800\n",
      "[CSQ][31/2000][17:14:48] bit:1024, dataset:palmprint, training... loss:0.80491\n",
      "[CSQ][32/2000][17:15:02] bit:1024, dataset:palmprint, training... loss:0.80330\n",
      "[CSQ][33/2000][17:15:16] bit:1024, dataset:palmprint, training... loss:0.80006\n",
      "[CSQ][34/2000][17:15:30] bit:1024, dataset:palmprint, training... loss:0.79802\n",
      "[CSQ][35/2000][17:15:45] bit:1024, dataset:palmprint, training... loss:0.79702\n",
      "[CSQ][36/2000][17:16:00] bit:1024, dataset:palmprint, training... loss:0.79510\n",
      "[CSQ][37/2000][17:16:15] bit:1024, dataset:palmprint, training... loss:0.79313\n",
      "[CSQ][38/2000][17:16:30] bit:1024, dataset:palmprint, training... loss:0.79035\n",
      "[CSQ][39/2000][17:16:45] bit:1024, dataset:palmprint, training... loss:0.78943\n",
      "[CSQ][40/2000][17:17:00] bit:1024, dataset:palmprint, training... loss:0.78874\n",
      "[CSQ][41/2000][17:17:15] bit:1024, dataset:palmprint, training... loss:0.78578\n",
      "[CSQ][42/2000][17:17:30] bit:1024, dataset:palmprint, training... loss:0.78438\n",
      "[CSQ][43/2000][17:17:45] bit:1024, dataset:palmprint, training... loss:0.78391\n",
      "[CSQ][44/2000][17:18:00] bit:1024, dataset:palmprint, training... loss:0.78272\n",
      "[CSQ][45/2000][17:18:15] bit:1024, dataset:palmprint, training... loss:0.78144\n",
      "[CSQ][46/2000][17:18:29] bit:1024, dataset:palmprint, training... loss:0.78019\n",
      "[CSQ][47/2000][17:18:45] bit:1024, dataset:palmprint, training... loss:0.77929\n",
      "[CSQ][48/2000][17:18:59] bit:1024, dataset:palmprint, training... loss:0.77902\n",
      "[CSQ][49/2000][17:19:14] bit:1024, dataset:palmprint, training... loss:0.77763\n",
      "[CSQ][50/2000][17:19:30] bit:1024, dataset:palmprint, training... loss:0.77611\n",
      "[CSQ][51/2000][17:19:44] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.07636363636363637\n",
      "[CSQ] epoch:51, bit:1024, dataset:palmprint,eer:0.07636, Best eer: 0.07636\n",
      "\b loss:0.77478\n",
      "[CSQ][52/2000][17:20:31] bit:1024, dataset:palmprint, training... loss:0.77469\n",
      "[CSQ][53/2000][17:20:47] bit:1024, dataset:palmprint, training... loss:0.77371\n",
      "[CSQ][54/2000][17:21:02] bit:1024, dataset:palmprint, training... loss:0.77272\n",
      "[CSQ][55/2000][17:21:17] bit:1024, dataset:palmprint, training... loss:0.77181\n",
      "[CSQ][56/2000][17:21:31] bit:1024, dataset:palmprint, training... loss:0.77173\n",
      "[CSQ][57/2000][17:21:46] bit:1024, dataset:palmprint, training... loss:0.77105\n",
      "[CSQ][58/2000][17:22:02] bit:1024, dataset:palmprint, training... loss:0.76845\n",
      "[CSQ][59/2000][17:22:17] bit:1024, dataset:palmprint, training... loss:0.76732\n",
      "[CSQ][60/2000][17:22:31] bit:1024, dataset:palmprint, training... loss:0.76784\n",
      "[CSQ][61/2000][17:22:45] bit:1024, dataset:palmprint, training... loss:0.76740\n",
      "[CSQ][62/2000][17:23:00] bit:1024, dataset:palmprint, training... loss:0.76614\n",
      "[CSQ][63/2000][17:23:14] bit:1024, dataset:palmprint, training... loss:0.76598\n",
      "[CSQ][64/2000][17:23:29] bit:1024, dataset:palmprint, training... loss:0.76395\n",
      "[CSQ][65/2000][17:23:43] bit:1024, dataset:palmprint, training... loss:0.76367\n",
      "[CSQ][66/2000][17:23:57] bit:1024, dataset:palmprint, training... loss:0.76364\n",
      "[CSQ][67/2000][17:24:12] bit:1024, dataset:palmprint, training... loss:0.76196\n",
      "[CSQ][68/2000][17:24:27] bit:1024, dataset:palmprint, training... loss:0.76161\n",
      "[CSQ][69/2000][17:24:42] bit:1024, dataset:palmprint, training... loss:0.75930\n",
      "[CSQ][70/2000][17:24:57] bit:1024, dataset:palmprint, training... loss:0.76020\n",
      "[CSQ][71/2000][17:25:12] bit:1024, dataset:palmprint, training... loss:0.76015\n",
      "[CSQ][72/2000][17:25:27] bit:1024, dataset:palmprint, training... loss:0.75924\n",
      "[CSQ][73/2000][17:25:42] bit:1024, dataset:palmprint, training... loss:0.75830\n",
      "[CSQ][74/2000][17:25:57] bit:1024, dataset:palmprint, training... loss:0.75644\n",
      "[CSQ][75/2000][17:26:11] bit:1024, dataset:palmprint, training... loss:0.75540\n",
      "[CSQ][76/2000][17:26:26] bit:1024, dataset:palmprint, training... loss:0.75600\n",
      "[CSQ][77/2000][17:26:42] bit:1024, dataset:palmprint, training... loss:0.75469\n",
      "[CSQ][78/2000][17:26:57] bit:1024, dataset:palmprint, training... loss:0.75392\n",
      "[CSQ][79/2000][17:27:12] bit:1024, dataset:palmprint, training... loss:0.75308\n",
      "[CSQ][80/2000][17:27:27] bit:1024, dataset:palmprint, training... loss:0.75386\n",
      "[CSQ][81/2000][17:27:42] bit:1024, dataset:palmprint, training... loss:0.75279\n",
      "[CSQ][82/2000][17:27:58] bit:1024, dataset:palmprint, training... loss:0.75123\n",
      "[CSQ][83/2000][17:28:13] bit:1024, dataset:palmprint, training... loss:0.75055\n",
      "[CSQ][84/2000][17:28:28] bit:1024, dataset:palmprint, training... loss:0.75092\n",
      "[CSQ][85/2000][17:28:43] bit:1024, dataset:palmprint, training... loss:0.74901\n",
      "[CSQ][86/2000][17:28:58] bit:1024, dataset:palmprint, training... loss:0.74866\n",
      "[CSQ][87/2000][17:29:14] bit:1024, dataset:palmprint, training... loss:0.74869\n",
      "[CSQ][88/2000][17:29:29] bit:1024, dataset:palmprint, training... loss:0.74672\n",
      "[CSQ][89/2000][17:29:44] bit:1024, dataset:palmprint, training... loss:0.74679\n",
      "[CSQ][90/2000][17:29:58] bit:1024, dataset:palmprint, training... loss:0.74695\n",
      "[CSQ][91/2000][17:30:13] bit:1024, dataset:palmprint, training... loss:0.74487\n",
      "[CSQ][92/2000][17:30:29] bit:1024, dataset:palmprint, training... loss:0.74550\n",
      "[CSQ][93/2000][17:30:43] bit:1024, dataset:palmprint, training... loss:0.74450\n",
      "[CSQ][94/2000][17:30:59] bit:1024, dataset:palmprint, training... loss:0.74433\n",
      "[CSQ][95/2000][17:31:13] bit:1024, dataset:palmprint, training... loss:0.74315\n",
      "[CSQ][96/2000][17:31:28] bit:1024, dataset:palmprint, training... loss:0.74128\n",
      "[CSQ][97/2000][17:31:42] bit:1024, dataset:palmprint, training... loss:0.74124\n",
      "[CSQ][98/2000][17:31:57] bit:1024, dataset:palmprint, training... loss:0.74139\n",
      "[CSQ][99/2000][17:32:12] bit:1024, dataset:palmprint, training... loss:0.74074\n",
      "[CSQ][100/2000][17:32:28] bit:1024, dataset:palmprint, training... loss:0.74052\n",
      "[CSQ][101/2000][17:32:43] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.04242424242424243\n",
      "[CSQ] epoch:101, bit:1024, dataset:palmprint,eer:0.04242, Best eer: 0.04242\n",
      "\b loss:0.73490\n",
      "[CSQ][102/2000][17:33:31] bit:1024, dataset:palmprint, training... loss:0.73444\n",
      "[CSQ][103/2000][17:33:45] bit:1024, dataset:palmprint, training... loss:0.73462\n",
      "[CSQ][104/2000][17:34:00] bit:1024, dataset:palmprint, training... loss:0.73548\n",
      "[CSQ][105/2000][17:34:15] bit:1024, dataset:palmprint, training... loss:0.73416\n",
      "[CSQ][106/2000][17:34:30] bit:1024, dataset:palmprint, training... loss:0.73413\n",
      "[CSQ][107/2000][17:34:44] bit:1024, dataset:palmprint, training... loss:0.73349\n",
      "[CSQ][108/2000][17:34:59] bit:1024, dataset:palmprint, training... loss:0.73336\n",
      "[CSQ][109/2000][17:35:14] bit:1024, dataset:palmprint, training... loss:0.73246\n",
      "[CSQ][110/2000][17:35:29] bit:1024, dataset:palmprint, training... loss:0.73150\n",
      "[CSQ][111/2000][17:35:44] bit:1024, dataset:palmprint, training... loss:0.73120\n",
      "[CSQ][112/2000][17:35:58] bit:1024, dataset:palmprint, training... loss:0.73035\n",
      "[CSQ][113/2000][17:36:14] bit:1024, dataset:palmprint, training... loss:0.73024\n",
      "[CSQ][114/2000][17:36:28] bit:1024, dataset:palmprint, training... loss:0.72917\n",
      "[CSQ][115/2000][17:36:43] bit:1024, dataset:palmprint, training... loss:0.72956\n",
      "[CSQ][116/2000][17:36:57] bit:1024, dataset:palmprint, training... loss:0.72901\n",
      "[CSQ][117/2000][17:37:13] bit:1024, dataset:palmprint, training... loss:0.72807\n",
      "[CSQ][118/2000][17:37:28] bit:1024, dataset:palmprint, training... loss:0.72722\n",
      "[CSQ][119/2000][17:37:43] bit:1024, dataset:palmprint, training... loss:0.72728\n",
      "[CSQ][120/2000][17:37:58] bit:1024, dataset:palmprint, training... loss:0.72686\n",
      "[CSQ][121/2000][17:38:13] bit:1024, dataset:palmprint, training... loss:0.72478\n",
      "[CSQ][122/2000][17:38:28] bit:1024, dataset:palmprint, training... loss:0.72490\n",
      "[CSQ][123/2000][17:38:43] bit:1024, dataset:palmprint, training... loss:0.72453\n",
      "[CSQ][124/2000][17:38:58] bit:1024, dataset:palmprint, training... loss:0.72462\n",
      "[CSQ][125/2000][17:39:13] bit:1024, dataset:palmprint, training... loss:0.72296\n",
      "[CSQ][126/2000][17:39:27] bit:1024, dataset:palmprint, training... loss:0.72322\n",
      "[CSQ][127/2000][17:39:41] bit:1024, dataset:palmprint, training... loss:0.72215\n",
      "[CSQ][128/2000][17:39:56] bit:1024, dataset:palmprint, training... loss:0.72096\n",
      "[CSQ][129/2000][17:40:11] bit:1024, dataset:palmprint, training... loss:0.72153\n",
      "[CSQ][130/2000][17:40:25] bit:1024, dataset:palmprint, training... loss:0.71963\n",
      "[CSQ][131/2000][17:40:40] bit:1024, dataset:palmprint, training... loss:0.71954\n",
      "[CSQ][132/2000][17:40:55] bit:1024, dataset:palmprint, training... loss:0.71929\n",
      "[CSQ][133/2000][17:41:11] bit:1024, dataset:palmprint, training... loss:0.71943\n",
      "[CSQ][134/2000][17:41:27] bit:1024, dataset:palmprint, training... loss:0.71885\n",
      "[CSQ][135/2000][17:41:42] bit:1024, dataset:palmprint, training... loss:0.71752\n",
      "[CSQ][136/2000][17:41:57] bit:1024, dataset:palmprint, training... loss:0.71624\n",
      "[CSQ][137/2000][17:42:12] bit:1024, dataset:palmprint, training... loss:0.71781\n",
      "[CSQ][138/2000][17:42:28] bit:1024, dataset:palmprint, training... loss:0.71538\n",
      "[CSQ][139/2000][17:42:44] bit:1024, dataset:palmprint, training... loss:0.71488\n",
      "[CSQ][140/2000][17:43:00] bit:1024, dataset:palmprint, training... loss:0.71539\n",
      "[CSQ][141/2000][17:43:15] bit:1024, dataset:palmprint, training... loss:0.71412\n",
      "[CSQ][142/2000][17:43:30] bit:1024, dataset:palmprint, training... loss:0.71387\n",
      "[CSQ][143/2000][17:43:45] bit:1024, dataset:palmprint, training... loss:0.71320\n",
      "[CSQ][144/2000][17:44:01] bit:1024, dataset:palmprint, training... loss:0.71304\n",
      "[CSQ][145/2000][17:44:16] bit:1024, dataset:palmprint, training... loss:0.71179\n",
      "[CSQ][146/2000][17:44:32] bit:1024, dataset:palmprint, training... loss:0.71119\n",
      "[CSQ][147/2000][17:44:47] bit:1024, dataset:palmprint, training... loss:0.71067\n",
      "[CSQ][148/2000][17:45:02] bit:1024, dataset:palmprint, training... loss:0.71074\n",
      "[CSQ][149/2000][17:45:18] bit:1024, dataset:palmprint, training... loss:0.71027\n",
      "[CSQ][150/2000][17:45:33] bit:1024, dataset:palmprint, training... loss:0.70938\n",
      "[CSQ][151/2000][17:45:48] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.03666666666666667\n",
      "[CSQ] epoch:151, bit:1024, dataset:palmprint,eer:0.03667, Best eer: 0.03667\n",
      "\b loss:0.70871\n",
      "[CSQ][152/2000][17:46:35] bit:1024, dataset:palmprint, training... loss:0.70758\n",
      "[CSQ][153/2000][17:46:50] bit:1024, dataset:palmprint, training... loss:0.70757\n",
      "[CSQ][154/2000][17:47:05] bit:1024, dataset:palmprint, training... loss:0.70837\n",
      "[CSQ][155/2000][17:47:20] bit:1024, dataset:palmprint, training... loss:0.70667\n",
      "[CSQ][156/2000][17:47:34] bit:1024, dataset:palmprint, training... loss:0.70686\n",
      "[CSQ][157/2000][17:47:48] bit:1024, dataset:palmprint, training... loss:0.70470\n",
      "[CSQ][158/2000][17:48:03] bit:1024, dataset:palmprint, training... loss:0.70481\n",
      "[CSQ][159/2000][17:48:17] bit:1024, dataset:palmprint, training... loss:0.70428\n",
      "[CSQ][160/2000][17:48:31] bit:1024, dataset:palmprint, training... loss:0.70460\n",
      "[CSQ][161/2000][17:48:46] bit:1024, dataset:palmprint, training... loss:0.70417\n",
      "[CSQ][162/2000][17:49:01] bit:1024, dataset:palmprint, training... loss:0.70372\n",
      "[CSQ][163/2000][17:49:16] bit:1024, dataset:palmprint, training... loss:0.70319\n",
      "[CSQ][164/2000][17:49:30] bit:1024, dataset:palmprint, training... loss:0.70286\n",
      "[CSQ][165/2000][17:49:45] bit:1024, dataset:palmprint, training... loss:0.70161\n",
      "[CSQ][166/2000][17:50:00] bit:1024, dataset:palmprint, training... loss:0.70215\n",
      "[CSQ][167/2000][17:50:14] bit:1024, dataset:palmprint, training... loss:0.70076\n",
      "[CSQ][168/2000][17:50:29] bit:1024, dataset:palmprint, training... loss:0.70150\n",
      "[CSQ][169/2000][17:50:45] bit:1024, dataset:palmprint, training... loss:0.70028\n",
      "[CSQ][170/2000][17:50:59] bit:1024, dataset:palmprint, training... loss:0.69951\n",
      "[CSQ][171/2000][17:51:14] bit:1024, dataset:palmprint, training... loss:0.69943\n",
      "[CSQ][172/2000][17:51:28] bit:1024, dataset:palmprint, training... loss:0.69811\n",
      "[CSQ][173/2000][17:51:43] bit:1024, dataset:palmprint, training... loss:0.69882\n",
      "[CSQ][174/2000][17:51:58] bit:1024, dataset:palmprint, training... loss:0.69730\n",
      "[CSQ][175/2000][17:52:12] bit:1024, dataset:palmprint, training... loss:0.69732\n",
      "[CSQ][176/2000][17:52:27] bit:1024, dataset:palmprint, training... loss:0.69721\n",
      "[CSQ][177/2000][17:52:41] bit:1024, dataset:palmprint, training... loss:0.69588\n",
      "[CSQ][178/2000][17:52:56] bit:1024, dataset:palmprint, training... loss:0.69636\n",
      "[CSQ][179/2000][17:53:11] bit:1024, dataset:palmprint, training... loss:0.69494\n",
      "[CSQ][180/2000][17:53:26] bit:1024, dataset:palmprint, training... loss:0.69432\n",
      "[CSQ][181/2000][17:53:41] bit:1024, dataset:palmprint, training... loss:0.69385\n",
      "[CSQ][182/2000][17:53:56] bit:1024, dataset:palmprint, training... loss:0.69264\n",
      "[CSQ][183/2000][17:54:11] bit:1024, dataset:palmprint, training... loss:0.69227\n",
      "[CSQ][184/2000][17:54:26] bit:1024, dataset:palmprint, training... loss:0.69354\n",
      "[CSQ][185/2000][17:54:40] bit:1024, dataset:palmprint, training... loss:0.69246\n",
      "[CSQ][186/2000][17:54:55] bit:1024, dataset:palmprint, training... loss:0.69050\n",
      "[CSQ][187/2000][17:55:10] bit:1024, dataset:palmprint, training... loss:0.69033\n",
      "[CSQ][188/2000][17:55:24] bit:1024, dataset:palmprint, training... loss:0.68996\n",
      "[CSQ][189/2000][17:55:38] bit:1024, dataset:palmprint, training... loss:0.69021\n",
      "[CSQ][190/2000][17:55:52] bit:1024, dataset:palmprint, training... loss:0.68827\n",
      "[CSQ][191/2000][17:56:06] bit:1024, dataset:palmprint, training... loss:0.68882\n",
      "[CSQ][192/2000][17:56:22] bit:1024, dataset:palmprint, training... loss:0.68887\n",
      "[CSQ][193/2000][17:56:36] bit:1024, dataset:palmprint, training... loss:0.68850\n",
      "[CSQ][194/2000][17:56:51] bit:1024, dataset:palmprint, training... loss:0.68739\n",
      "[CSQ][195/2000][17:57:06] bit:1024, dataset:palmprint, training... loss:0.68609\n",
      "[CSQ][196/2000][17:57:21] bit:1024, dataset:palmprint, training... loss:0.68691\n",
      "[CSQ][197/2000][17:57:35] bit:1024, dataset:palmprint, training... loss:0.68776\n",
      "[CSQ][198/2000][17:57:50] bit:1024, dataset:palmprint, training... loss:0.68557\n",
      "[CSQ][199/2000][17:58:05] bit:1024, dataset:palmprint, training... loss:0.68479\n",
      "[CSQ][200/2000][17:58:20] bit:1024, dataset:palmprint, training... loss:0.68488\n",
      "[CSQ][201/2000][17:58:34] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.03363636363636364\n",
      "[CSQ] epoch:201, bit:1024, dataset:palmprint,eer:0.03364, Best eer: 0.03364\n",
      "\b loss:0.68143\n",
      "[CSQ][202/2000][17:59:21] bit:1024, dataset:palmprint, training... loss:0.68202\n",
      "[CSQ][203/2000][17:59:36] bit:1024, dataset:palmprint, training... loss:0.68192\n",
      "[CSQ][204/2000][17:59:50] bit:1024, dataset:palmprint, training... loss:0.68048\n",
      "[CSQ][205/2000][18:00:05] bit:1024, dataset:palmprint, training... loss:0.67989\n",
      "[CSQ][206/2000][18:00:20] bit:1024, dataset:palmprint, training... loss:0.68093\n",
      "[CSQ][207/2000][18:00:35] bit:1024, dataset:palmprint, training... loss:0.68025\n",
      "[CSQ][208/2000][18:00:49] bit:1024, dataset:palmprint, training... loss:0.67915\n",
      "[CSQ][209/2000][18:01:04] bit:1024, dataset:palmprint, training... loss:0.67971\n",
      "[CSQ][210/2000][18:01:19] bit:1024, dataset:palmprint, training... loss:0.67815\n",
      "[CSQ][211/2000][18:01:34] bit:1024, dataset:palmprint, training... loss:0.68000\n",
      "[CSQ][212/2000][18:01:48] bit:1024, dataset:palmprint, training... loss:0.68020\n",
      "[CSQ][213/2000][18:02:03] bit:1024, dataset:palmprint, training... loss:0.67644\n",
      "[CSQ][214/2000][18:02:18] bit:1024, dataset:palmprint, training... loss:0.67636\n",
      "[CSQ][215/2000][18:02:34] bit:1024, dataset:palmprint, training... loss:0.67694\n",
      "[CSQ][216/2000][18:02:49] bit:1024, dataset:palmprint, training... loss:0.67600\n",
      "[CSQ][217/2000][18:03:03] bit:1024, dataset:palmprint, training... loss:0.67644\n",
      "[CSQ][218/2000][18:03:18] bit:1024, dataset:palmprint, training... loss:0.67518\n",
      "[CSQ][219/2000][18:03:32] bit:1024, dataset:palmprint, training... loss:0.67424\n",
      "[CSQ][220/2000][18:03:46] bit:1024, dataset:palmprint, training... loss:0.67540\n",
      "[CSQ][221/2000][18:04:00] bit:1024, dataset:palmprint, training... loss:0.67326\n",
      "[CSQ][222/2000][18:04:15] bit:1024, dataset:palmprint, training... loss:0.67631\n",
      "[CSQ][223/2000][18:04:29] bit:1024, dataset:palmprint, training... loss:0.67219\n",
      "[CSQ][224/2000][18:04:44] bit:1024, dataset:palmprint, training... loss:0.67198\n",
      "[CSQ][225/2000][18:05:00] bit:1024, dataset:palmprint, training... loss:0.67200\n",
      "[CSQ][226/2000][18:05:15] bit:1024, dataset:palmprint, training... loss:0.67267\n",
      "[CSQ][227/2000][18:05:30] bit:1024, dataset:palmprint, training... loss:0.67264\n",
      "[CSQ][228/2000][18:05:44] bit:1024, dataset:palmprint, training... loss:0.67119\n",
      "[CSQ][229/2000][18:05:59] bit:1024, dataset:palmprint, training... loss:0.67103\n",
      "[CSQ][230/2000][18:06:14] bit:1024, dataset:palmprint, training... loss:0.67095\n",
      "[CSQ][231/2000][18:06:29] bit:1024, dataset:palmprint, training... loss:0.66988\n",
      "[CSQ][232/2000][18:06:43] bit:1024, dataset:palmprint, training... loss:0.66775\n",
      "[CSQ][233/2000][18:06:57] bit:1024, dataset:palmprint, training... loss:0.66894\n",
      "[CSQ][234/2000][18:07:12] bit:1024, dataset:palmprint, training... loss:0.66840\n",
      "[CSQ][235/2000][18:07:27] bit:1024, dataset:palmprint, training... loss:0.66904\n",
      "[CSQ][236/2000][18:07:42] bit:1024, dataset:palmprint, training... loss:0.66756\n",
      "[CSQ][237/2000][18:07:57] bit:1024, dataset:palmprint, training... loss:0.66722\n",
      "[CSQ][238/2000][18:08:11] bit:1024, dataset:palmprint, training... loss:0.66705\n",
      "[CSQ][239/2000][18:08:26] bit:1024, dataset:palmprint, training... loss:0.66607\n",
      "[CSQ][240/2000][18:08:41] bit:1024, dataset:palmprint, training... loss:0.66560\n",
      "[CSQ][241/2000][18:08:55] bit:1024, dataset:palmprint, training... loss:0.66682\n",
      "[CSQ][242/2000][18:09:10] bit:1024, dataset:palmprint, training... loss:0.66649\n",
      "[CSQ][243/2000][18:09:25] bit:1024, dataset:palmprint, training... loss:0.66477\n",
      "[CSQ][244/2000][18:09:40] bit:1024, dataset:palmprint, training... loss:0.66497\n",
      "[CSQ][245/2000][18:09:55] bit:1024, dataset:palmprint, training... loss:0.66439\n",
      "[CSQ][246/2000][18:10:09] bit:1024, dataset:palmprint, training... loss:0.66417\n",
      "[CSQ][247/2000][18:10:24] bit:1024, dataset:palmprint, training... loss:0.66315\n",
      "[CSQ][248/2000][18:10:39] bit:1024, dataset:palmprint, training... loss:0.66368\n",
      "[CSQ][249/2000][18:10:53] bit:1024, dataset:palmprint, training... loss:0.66188\n",
      "[CSQ][250/2000][18:11:08] bit:1024, dataset:palmprint, training... loss:0.66166\n",
      "[CSQ][251/2000][18:11:22] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.03303030303030303\n",
      "[CSQ] epoch:251, bit:1024, dataset:palmprint,eer:0.03303, Best eer: 0.03303\n",
      "\b loss:0.66181\n",
      "[CSQ][252/2000][18:12:11] bit:1024, dataset:palmprint, training... loss:0.66133\n",
      "[CSQ][253/2000][18:12:25] bit:1024, dataset:palmprint, training... loss:0.66149\n",
      "[CSQ][254/2000][18:12:40] bit:1024, dataset:palmprint, training... loss:0.66072\n",
      "[CSQ][255/2000][18:12:55] bit:1024, dataset:palmprint, training... loss:0.66147\n",
      "[CSQ][256/2000][18:13:10] bit:1024, dataset:palmprint, training... loss:0.65846\n",
      "[CSQ][257/2000][18:13:24] bit:1024, dataset:palmprint, training... loss:0.65932\n",
      "[CSQ][258/2000][18:13:39] bit:1024, dataset:palmprint, training... loss:0.66077\n",
      "[CSQ][259/2000][18:13:54] bit:1024, dataset:palmprint, training... loss:0.65970\n",
      "[CSQ][260/2000][18:14:09] bit:1024, dataset:palmprint, training... loss:0.65937\n",
      "[CSQ][261/2000][18:14:23] bit:1024, dataset:palmprint, training... loss:0.65808\n",
      "[CSQ][262/2000][18:14:38] bit:1024, dataset:palmprint, training... loss:0.65850\n",
      "[CSQ][263/2000][18:14:53] bit:1024, dataset:palmprint, training... loss:0.65746\n",
      "[CSQ][264/2000][18:15:07] bit:1024, dataset:palmprint, training... loss:0.65612\n",
      "[CSQ][265/2000][18:15:22] bit:1024, dataset:palmprint, training... loss:0.65555\n",
      "[CSQ][266/2000][18:15:37] bit:1024, dataset:palmprint, training... loss:0.65531\n",
      "[CSQ][267/2000][18:15:52] bit:1024, dataset:palmprint, training... loss:0.65435\n",
      "[CSQ][268/2000][18:16:07] bit:1024, dataset:palmprint, training... loss:0.65515\n",
      "[CSQ][269/2000][18:16:22] bit:1024, dataset:palmprint, training... loss:0.65480\n",
      "[CSQ][270/2000][18:16:37] bit:1024, dataset:palmprint, training... loss:0.65524\n",
      "[CSQ][271/2000][18:16:52] bit:1024, dataset:palmprint, training... loss:0.65510\n",
      "[CSQ][272/2000][18:17:06] bit:1024, dataset:palmprint, training... loss:0.65500\n",
      "[CSQ][273/2000][18:17:22] bit:1024, dataset:palmprint, training... loss:0.65425\n",
      "[CSQ][274/2000][18:17:37] bit:1024, dataset:palmprint, training... loss:0.65418\n",
      "[CSQ][275/2000][18:17:51] bit:1024, dataset:palmprint, training... loss:0.65230\n",
      "[CSQ][276/2000][18:18:05] bit:1024, dataset:palmprint, training... loss:0.65292\n",
      "[CSQ][277/2000][18:18:21] bit:1024, dataset:palmprint, training... loss:0.65117\n",
      "[CSQ][278/2000][18:18:36] bit:1024, dataset:palmprint, training... loss:0.65171\n",
      "[CSQ][279/2000][18:18:50] bit:1024, dataset:palmprint, training... loss:0.65148\n",
      "[CSQ][280/2000][18:19:05] bit:1024, dataset:palmprint, training... loss:0.65036\n",
      "[CSQ][281/2000][18:19:19] bit:1024, dataset:palmprint, training... loss:0.65152\n",
      "[CSQ][282/2000][18:19:33] bit:1024, dataset:palmprint, training... loss:0.65078\n",
      "[CSQ][283/2000][18:19:47] bit:1024, dataset:palmprint, training... loss:0.65039\n",
      "[CSQ][284/2000][18:20:01] bit:1024, dataset:palmprint, training... loss:0.65121\n",
      "[CSQ][285/2000][18:20:15] bit:1024, dataset:palmprint, training... loss:0.64965\n",
      "[CSQ][286/2000][18:20:30] bit:1024, dataset:palmprint, training... loss:0.64737\n",
      "[CSQ][287/2000][18:20:44] bit:1024, dataset:palmprint, training... loss:0.64932\n",
      "[CSQ][288/2000][18:20:59] bit:1024, dataset:palmprint, training... loss:0.64694\n",
      "[CSQ][289/2000][18:21:14] bit:1024, dataset:palmprint, training... loss:0.64739\n",
      "[CSQ][290/2000][18:21:28] bit:1024, dataset:palmprint, training... loss:0.64717\n",
      "[CSQ][291/2000][18:21:43] bit:1024, dataset:palmprint, training... loss:0.64637\n",
      "[CSQ][292/2000][18:21:57] bit:1024, dataset:palmprint, training... loss:0.64724\n",
      "[CSQ][293/2000][18:22:13] bit:1024, dataset:palmprint, training... loss:0.64557\n",
      "[CSQ][294/2000][18:22:28] bit:1024, dataset:palmprint, training... loss:0.64544\n",
      "[CSQ][295/2000][18:22:43] bit:1024, dataset:palmprint, training... loss:0.64661\n",
      "[CSQ][296/2000][18:22:58] bit:1024, dataset:palmprint, training... loss:0.64527\n",
      "[CSQ][297/2000][18:23:13] bit:1024, dataset:palmprint, training... loss:0.64246\n",
      "[CSQ][298/2000][18:23:28] bit:1024, dataset:palmprint, training... loss:0.64438\n",
      "[CSQ][299/2000][18:23:43] bit:1024, dataset:palmprint, training... loss:0.64329\n",
      "[CSQ][300/2000][18:23:58] bit:1024, dataset:palmprint, training... loss:0.64248\n",
      "[CSQ][301/2000][18:24:12] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.030303030303030304\n",
      "[CSQ] epoch:301, bit:1024, dataset:palmprint,eer:0.03030, Best eer: 0.03030\n",
      "\b loss:0.64118\n",
      "[CSQ][302/2000][18:24:58] bit:1024, dataset:palmprint, training... loss:0.64070\n",
      "[CSQ][303/2000][18:25:13] bit:1024, dataset:palmprint, training... loss:0.64179\n",
      "[CSQ][304/2000][18:25:27] bit:1024, dataset:palmprint, training... loss:0.63873\n",
      "[CSQ][305/2000][18:25:42] bit:1024, dataset:palmprint, training... loss:0.64030\n",
      "[CSQ][306/2000][18:25:57] bit:1024, dataset:palmprint, training... loss:0.64017\n",
      "[CSQ][307/2000][18:26:12] bit:1024, dataset:palmprint, training... loss:0.63916\n",
      "[CSQ][308/2000][18:26:26] bit:1024, dataset:palmprint, training... loss:0.63941\n",
      "[CSQ][309/2000][18:26:40] bit:1024, dataset:palmprint, training... loss:0.63873\n",
      "[CSQ][310/2000][18:26:55] bit:1024, dataset:palmprint, training... loss:0.63904\n",
      "[CSQ][311/2000][18:27:10] bit:1024, dataset:palmprint, training... loss:0.63736\n",
      "[CSQ][312/2000][18:27:24] bit:1024, dataset:palmprint, training... loss:0.64024\n",
      "[CSQ][313/2000][18:27:38] bit:1024, dataset:palmprint, training... loss:0.63815\n",
      "[CSQ][314/2000][18:27:52] bit:1024, dataset:palmprint, training... loss:0.63688\n",
      "[CSQ][315/2000][18:28:06] bit:1024, dataset:palmprint, training... loss:0.63658\n",
      "[CSQ][316/2000][18:28:21] bit:1024, dataset:palmprint, training... loss:0.63665\n",
      "[CSQ][317/2000][18:28:36] bit:1024, dataset:palmprint, training... loss:0.63683\n",
      "[CSQ][318/2000][18:28:50] bit:1024, dataset:palmprint, training... loss:0.63757\n",
      "[CSQ][319/2000][18:29:05] bit:1024, dataset:palmprint, training... loss:0.63494\n",
      "[CSQ][320/2000][18:29:20] bit:1024, dataset:palmprint, training... loss:0.63490\n",
      "[CSQ][321/2000][18:29:35] bit:1024, dataset:palmprint, training... loss:0.63686\n",
      "[CSQ][322/2000][18:29:49] bit:1024, dataset:palmprint, training... loss:0.63479\n",
      "[CSQ][323/2000][18:30:05] bit:1024, dataset:palmprint, training... loss:0.63661\n",
      "[CSQ][324/2000][18:30:19] bit:1024, dataset:palmprint, training... loss:0.63271\n",
      "[CSQ][325/2000][18:30:35] bit:1024, dataset:palmprint, training... loss:0.63355\n",
      "[CSQ][326/2000][18:30:49] bit:1024, dataset:palmprint, training... loss:0.63383\n",
      "[CSQ][327/2000][18:31:03] bit:1024, dataset:palmprint, training... loss:0.63266\n",
      "[CSQ][328/2000][18:31:19] bit:1024, dataset:palmprint, training... loss:0.63201\n",
      "[CSQ][329/2000][18:31:33] bit:1024, dataset:palmprint, training... loss:0.63311\n",
      "[CSQ][330/2000][18:31:47] bit:1024, dataset:palmprint, training... loss:0.63179\n",
      "[CSQ][331/2000][18:32:01] bit:1024, dataset:palmprint, training... loss:0.63289\n",
      "[CSQ][332/2000][18:32:17] bit:1024, dataset:palmprint, training... loss:0.63294\n",
      "[CSQ][333/2000][18:32:32] bit:1024, dataset:palmprint, training... loss:0.63003\n",
      "[CSQ][334/2000][18:32:47] bit:1024, dataset:palmprint, training... loss:0.63197\n",
      "[CSQ][335/2000][18:33:01] bit:1024, dataset:palmprint, training... loss:0.63240\n",
      "[CSQ][336/2000][18:33:15] bit:1024, dataset:palmprint, training... loss:0.62854\n",
      "[CSQ][337/2000][18:33:31] bit:1024, dataset:palmprint, training... loss:0.63136\n",
      "[CSQ][338/2000][18:33:45] bit:1024, dataset:palmprint, training... loss:0.62912\n",
      "[CSQ][339/2000][18:34:00] bit:1024, dataset:palmprint, training... loss:0.62831\n",
      "[CSQ][340/2000][18:34:14] bit:1024, dataset:palmprint, training... loss:0.62715\n",
      "[CSQ][341/2000][18:34:30] bit:1024, dataset:palmprint, training... loss:0.62640\n",
      "[CSQ][342/2000][18:34:45] bit:1024, dataset:palmprint, training... loss:0.62822\n",
      "[CSQ][343/2000][18:34:59] bit:1024, dataset:palmprint, training... loss:0.62738\n",
      "[CSQ][344/2000][18:35:14] bit:1024, dataset:palmprint, training... loss:0.62872\n",
      "[CSQ][345/2000][18:35:28] bit:1024, dataset:palmprint, training... loss:0.62620\n",
      "[CSQ][346/2000][18:35:42] bit:1024, dataset:palmprint, training... loss:0.62540\n",
      "[CSQ][347/2000][18:35:57] bit:1024, dataset:palmprint, training... loss:0.62750\n",
      "[CSQ][348/2000][18:36:11] bit:1024, dataset:palmprint, training... loss:0.62573\n",
      "[CSQ][349/2000][18:36:25] bit:1024, dataset:palmprint, training... loss:0.62583\n",
      "[CSQ][350/2000][18:36:41] bit:1024, dataset:palmprint, training... loss:0.62534\n",
      "[CSQ][351/2000][18:36:55] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.029696969696969697\n",
      "[CSQ] epoch:351, bit:1024, dataset:palmprint,eer:0.02970, Best eer: 0.02970\n",
      "\b loss:0.62640\n",
      "[CSQ][352/2000][18:37:46] bit:1024, dataset:palmprint, training... loss:0.62288\n",
      "[CSQ][353/2000][18:38:01] bit:1024, dataset:palmprint, training... loss:0.62445\n",
      "[CSQ][354/2000][18:38:15] bit:1024, dataset:palmprint, training... loss:0.62335\n",
      "[CSQ][355/2000][18:38:29] bit:1024, dataset:palmprint, training... loss:0.62473\n",
      "[CSQ][356/2000][18:38:44] bit:1024, dataset:palmprint, training... loss:0.62255\n",
      "[CSQ][357/2000][18:38:58] bit:1024, dataset:palmprint, training... loss:0.62419\n",
      "[CSQ][358/2000][18:39:13] bit:1024, dataset:palmprint, training... loss:0.62398\n",
      "[CSQ][359/2000][18:39:28] bit:1024, dataset:palmprint, training... loss:0.62322\n",
      "[CSQ][360/2000][18:39:43] bit:1024, dataset:palmprint, training... loss:0.62291\n",
      "[CSQ][361/2000][18:39:58] bit:1024, dataset:palmprint, training... loss:0.62197\n",
      "[CSQ][362/2000][18:40:12] bit:1024, dataset:palmprint, training... loss:0.62104\n",
      "[CSQ][363/2000][18:40:27] bit:1024, dataset:palmprint, training... loss:0.62203\n",
      "[CSQ][364/2000][18:40:41] bit:1024, dataset:palmprint, training... loss:0.61923\n",
      "[CSQ][365/2000][18:40:57] bit:1024, dataset:palmprint, training... loss:0.62061\n",
      "[CSQ][366/2000][18:41:11] bit:1024, dataset:palmprint, training... loss:0.61954\n",
      "[CSQ][367/2000][18:41:26] bit:1024, dataset:palmprint, training... loss:0.61961\n",
      "[CSQ][368/2000][18:41:41] bit:1024, dataset:palmprint, training... loss:0.62081\n",
      "[CSQ][369/2000][18:41:56] bit:1024, dataset:palmprint, training... loss:0.61923\n",
      "[CSQ][370/2000][18:42:10] bit:1024, dataset:palmprint, training... loss:0.61801\n",
      "[CSQ][371/2000][18:42:25] bit:1024, dataset:palmprint, training... loss:0.61931\n",
      "[CSQ][372/2000][18:42:40] bit:1024, dataset:palmprint, training... loss:0.61866\n",
      "[CSQ][373/2000][18:42:55] bit:1024, dataset:palmprint, training... loss:0.61850\n",
      "[CSQ][374/2000][18:43:09] bit:1024, dataset:palmprint, training... loss:0.61834\n",
      "[CSQ][375/2000][18:43:24] bit:1024, dataset:palmprint, training... loss:0.61917\n",
      "[CSQ][376/2000][18:43:37] bit:1024, dataset:palmprint, training... loss:0.61713\n",
      "[CSQ][377/2000][18:43:51] bit:1024, dataset:palmprint, training... loss:0.61625\n",
      "[CSQ][378/2000][18:44:06] bit:1024, dataset:palmprint, training... loss:0.61430\n",
      "[CSQ][379/2000][18:44:21] bit:1024, dataset:palmprint, training... loss:0.61633\n",
      "[CSQ][380/2000][18:44:35] bit:1024, dataset:palmprint, training... loss:0.61416\n",
      "[CSQ][381/2000][18:44:50] bit:1024, dataset:palmprint, training... loss:0.61591\n",
      "[CSQ][382/2000][18:45:04] bit:1024, dataset:palmprint, training... loss:0.61438\n",
      "[CSQ][383/2000][18:45:19] bit:1024, dataset:palmprint, training... loss:0.61542\n",
      "[CSQ][384/2000][18:45:34] bit:1024, dataset:palmprint, training... loss:0.61499\n",
      "[CSQ][385/2000][18:45:49] bit:1024, dataset:palmprint, training... loss:0.61474\n",
      "[CSQ][386/2000][18:46:03] bit:1024, dataset:palmprint, training... loss:0.61315\n",
      "[CSQ][387/2000][18:46:18] bit:1024, dataset:palmprint, training... loss:0.61429\n",
      "[CSQ][388/2000][18:46:33] bit:1024, dataset:palmprint, training... loss:0.61184\n",
      "[CSQ][389/2000][18:46:47] bit:1024, dataset:palmprint, training... loss:0.61186\n",
      "[CSQ][390/2000][18:47:02] bit:1024, dataset:palmprint, training... loss:0.61319\n",
      "[CSQ][391/2000][18:47:17] bit:1024, dataset:palmprint, training... loss:0.61277\n",
      "[CSQ][392/2000][18:47:32] bit:1024, dataset:palmprint, training... loss:0.61021\n",
      "[CSQ][393/2000][18:47:47] bit:1024, dataset:palmprint, training... loss:0.61157\n",
      "[CSQ][394/2000][18:48:01] bit:1024, dataset:palmprint, training... loss:0.60983\n",
      "[CSQ][395/2000][18:48:15] bit:1024, dataset:palmprint, training... loss:0.61040\n",
      "[CSQ][396/2000][18:48:31] bit:1024, dataset:palmprint, training... loss:0.61029\n",
      "[CSQ][397/2000][18:48:46] bit:1024, dataset:palmprint, training... loss:0.60980\n",
      "[CSQ][398/2000][18:49:00] bit:1024, dataset:palmprint, training... loss:0.60914\n",
      "[CSQ][399/2000][18:49:15] bit:1024, dataset:palmprint, training... loss:0.60847\n",
      "[CSQ][400/2000][18:49:29] bit:1024, dataset:palmprint, training... loss:0.60917\n",
      "[CSQ][401/2000][18:49:45] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.027575757575757576\n",
      "[CSQ] epoch:401, bit:1024, dataset:palmprint,eer:0.02758, Best eer: 0.02758\n",
      "\b loss:0.60816\n",
      "[CSQ][402/2000][18:50:39] bit:1024, dataset:palmprint, training... loss:0.60607\n",
      "[CSQ][403/2000][18:50:53] bit:1024, dataset:palmprint, training... loss:0.60770\n",
      "[CSQ][404/2000][18:51:07] bit:1024, dataset:palmprint, training... loss:0.60622\n",
      "[CSQ][405/2000][18:51:21] bit:1024, dataset:palmprint, training... loss:0.60530\n",
      "[CSQ][406/2000][18:51:36] bit:1024, dataset:palmprint, training... loss:0.60735\n",
      "[CSQ][407/2000][18:51:50] bit:1024, dataset:palmprint, training... loss:0.60687\n",
      "[CSQ][408/2000][18:52:06] bit:1024, dataset:palmprint, training... loss:0.60861\n",
      "[CSQ][409/2000][18:52:21] bit:1024, dataset:palmprint, training... loss:0.60719\n",
      "[CSQ][410/2000][18:52:35] bit:1024, dataset:palmprint, training... loss:0.60675\n",
      "[CSQ][411/2000][18:52:50] bit:1024, dataset:palmprint, training... loss:0.60500\n",
      "[CSQ][412/2000][18:53:04] bit:1024, dataset:palmprint, training... loss:0.60527\n",
      "[CSQ][413/2000][18:53:19] bit:1024, dataset:palmprint, training... loss:0.60557\n",
      "[CSQ][414/2000][18:53:34] bit:1024, dataset:palmprint, training... loss:0.60609\n",
      "[CSQ][415/2000][18:53:48] bit:1024, dataset:palmprint, training... loss:0.60423\n",
      "[CSQ][416/2000][18:54:03] bit:1024, dataset:palmprint, training... loss:0.60486\n",
      "[CSQ][417/2000][18:54:18] bit:1024, dataset:palmprint, training... loss:0.60511\n",
      "[CSQ][418/2000][18:54:33] bit:1024, dataset:palmprint, training... loss:0.60231\n",
      "[CSQ][419/2000][18:54:48] bit:1024, dataset:palmprint, training... loss:0.60200\n",
      "[CSQ][420/2000][18:55:02] bit:1024, dataset:palmprint, training... loss:0.60242\n",
      "[CSQ][421/2000][18:55:17] bit:1024, dataset:palmprint, training... loss:0.60182\n",
      "[CSQ][422/2000][18:55:32] bit:1024, dataset:palmprint, training... loss:0.60268\n",
      "[CSQ][423/2000][18:55:47] bit:1024, dataset:palmprint, training... loss:0.59970\n",
      "[CSQ][424/2000][18:56:02] bit:1024, dataset:palmprint, training... loss:0.60080\n",
      "[CSQ][425/2000][18:56:16] bit:1024, dataset:palmprint, training... loss:0.60123\n",
      "[CSQ][426/2000][18:56:32] bit:1024, dataset:palmprint, training... loss:0.59981\n",
      "[CSQ][427/2000][18:56:47] bit:1024, dataset:palmprint, training... loss:0.60259\n",
      "[CSQ][428/2000][18:57:02] bit:1024, dataset:palmprint, training... loss:0.60186\n",
      "[CSQ][429/2000][18:57:16] bit:1024, dataset:palmprint, training... loss:0.59982\n",
      "[CSQ][430/2000][18:57:31] bit:1024, dataset:palmprint, training... loss:0.60039\n",
      "[CSQ][431/2000][18:57:46] bit:1024, dataset:palmprint, training... loss:0.60074\n",
      "[CSQ][432/2000][18:58:01] bit:1024, dataset:palmprint, training... loss:0.59840\n",
      "[CSQ][433/2000][18:58:15] bit:1024, dataset:palmprint, training... loss:0.59888\n",
      "[CSQ][434/2000][18:58:30] bit:1024, dataset:palmprint, training... loss:0.59660\n",
      "[CSQ][435/2000][18:58:44] bit:1024, dataset:palmprint, training... loss:0.59889\n",
      "[CSQ][436/2000][18:58:59] bit:1024, dataset:palmprint, training... loss:0.59783\n",
      "[CSQ][437/2000][18:59:13] bit:1024, dataset:palmprint, training... loss:0.59733\n",
      "[CSQ][438/2000][18:59:28] bit:1024, dataset:palmprint, training... loss:0.59610\n",
      "[CSQ][439/2000][18:59:43] bit:1024, dataset:palmprint, training... loss:0.59706\n",
      "[CSQ][440/2000][18:59:57] bit:1024, dataset:palmprint, training... loss:0.59701\n",
      "[CSQ][441/2000][19:00:12] bit:1024, dataset:palmprint, training... loss:0.59850\n",
      "[CSQ][442/2000][19:00:26] bit:1024, dataset:palmprint, training... loss:0.59781\n",
      "[CSQ][443/2000][19:00:41] bit:1024, dataset:palmprint, training... loss:0.59562\n",
      "[CSQ][444/2000][19:00:56] bit:1024, dataset:palmprint, training... loss:0.59493\n",
      "[CSQ][445/2000][19:01:11] bit:1024, dataset:palmprint, training... loss:0.59466\n",
      "[CSQ][446/2000][19:01:26] bit:1024, dataset:palmprint, training... loss:0.59667\n",
      "[CSQ][447/2000][19:01:40] bit:1024, dataset:palmprint, training... loss:0.59651\n",
      "[CSQ][448/2000][19:01:55] bit:1024, dataset:palmprint, training... loss:0.59407\n",
      "[CSQ][449/2000][19:02:09] bit:1024, dataset:palmprint, training... loss:0.59457\n",
      "[CSQ][450/2000][19:02:24] bit:1024, dataset:palmprint, training... loss:0.59463\n",
      "[CSQ][451/2000][19:02:39] bit:1024, dataset:palmprint, training....- feats shape: (600, 1024)\n",
      "- GT shape: (600,)\n",
      "0.026060606060606062\n",
      "[CSQ] epoch:451, bit:1024, dataset:palmprint,eer:0.02606, Best eer: 0.02606\n",
      "\b loss:0.59472\n",
      "[CSQ][452/2000][19:03:24] bit:1024, dataset:palmprint, training... loss:0.59569\n",
      "[CSQ][453/2000][19:03:39] bit:1024, dataset:palmprint, training... loss:0.59237\n",
      "[CSQ][454/2000][19:03:55] bit:1024, dataset:palmprint, training... loss:0.59124\n",
      "[CSQ][455/2000][19:04:09] bit:1024, dataset:palmprint, training... loss:0.59099\n",
      "[CSQ][456/2000][19:04:24] bit:1024, dataset:palmprint, training... loss:0.59360\n",
      "[CSQ][457/2000][19:04:39] bit:1024, dataset:palmprint, training... loss:0.59375\n",
      "[CSQ][458/2000][19:04:54] bit:1024, dataset:palmprint, training... loss:0.59105\n",
      "[CSQ][459/2000][19:05:09] bit:1024, dataset:palmprint, training... loss:0.59210\n",
      "[CSQ][460/2000][19:05:24] bit:1024, dataset:palmprint, training... loss:0.59263\n",
      "[CSQ][461/2000][19:05:40] bit:1024, dataset:palmprint, training... loss:0.59331\n",
      "[CSQ][462/2000][19:05:55] bit:1024, dataset:palmprint, training... loss:0.59026\n",
      "[CSQ][463/2000][19:06:10] bit:1024, dataset:palmprint, training... loss:0.59126\n",
      "[CSQ][464/2000][19:06:25] bit:1024, dataset:palmprint, training... loss:0.58905\n",
      "[CSQ][465/2000][19:06:40] bit:1024, dataset:palmprint, training... loss:0.59124\n",
      "[CSQ][466/2000][19:06:55] bit:1024, dataset:palmprint, training... loss:0.58826\n",
      "[CSQ][467/2000][19:07:10] bit:1024, dataset:palmprint, training... loss:0.58923\n",
      "[CSQ][468/2000][19:07:25] bit:1024, dataset:palmprint, training... loss:0.59170\n",
      "[CSQ][469/2000][19:07:40] bit:1024, dataset:palmprint, training... loss:0.58933\n",
      "[CSQ][470/2000][19:07:56] bit:1024, dataset:palmprint, training... loss:0.58876\n",
      "[CSQ][471/2000][19:08:12] bit:1024, dataset:palmprint, training... loss:0.59039\n",
      "[CSQ][472/2000][19:08:27] bit:1024, dataset:palmprint, training... loss:0.58849\n",
      "[CSQ][473/2000][19:08:42] bit:1024, dataset:palmprint, training... loss:0.58771\n",
      "[CSQ][474/2000][19:08:56] bit:1024, dataset:palmprint, training... loss:0.58829\n",
      "[CSQ][475/2000][19:09:11] bit:1024, dataset:palmprint, training... loss:0.58731\n",
      "[CSQ][476/2000][19:09:25] bit:1024, dataset:palmprint, training... loss:0.58641\n",
      "[CSQ][477/2000][19:09:39] bit:1024, dataset:palmprint, training... loss:0.58643\n",
      "[CSQ][478/2000][19:09:54] bit:1024, dataset:palmprint, training...."
     ]
    }
   ],
   "source": [
    "Best_eer = 1.0\n",
    "\n",
    "config[\"epoch\"] = 2000\n",
    "for epoch in range(config[\"epoch\"]):\n",
    "\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "    print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "        config[\"info\"], epoch + 1, config[\"epoch\"], current_time, bit, config[\"dataset\"]), end=\"\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_idx, img in enumerate(train_loader):\n",
    "        image = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "        label = img[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        u = model(image)\n",
    "\n",
    "        loss = criterion(u, label.float(), 0, config)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    if epoch % 50 ==0:\n",
    "        eer = test(model,test_loader)\n",
    "        if eer < Best_eer:\n",
    "            Best_eer = eer\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': Best_eer,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, True, filename='checkpointGCN2wayHashingsimpleprotocol.pth.tar', remark='GCN2wayHashingsimpleprotocolopen')\n",
    "        print(\"%s epoch:%d, bit:%d, dataset:%s,eer:%.5f, Best eer: %.5f\" % (\n",
    "            config[\"info\"], epoch + 1, bit, config[\"dataset\"], eer, Best_eer))\n",
    "    print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))##loss:0.625\n",
    "    scheduler.step()\n",
    "\n",
    "#     if (epoch + 1) % config[\"test_map\"] == 0:\n",
    "#         # print(\"calculating test binary code......\")\n",
    "#         tst_binary, tst_label = compute_result(test_loader, net, device=device)\n",
    "\n",
    "#         # print(\"calculating dataset binary code.......\")\\\n",
    "#         trn_binary, trn_label = compute_result(dataset_loader, net, device=device)\n",
    "\n",
    "#         # print(\"calculating map.......\")\n",
    "#         mAP = CalcTopMap(trn_binary.numpy(), tst_binary.numpy(), trn_label.numpy(), tst_label.numpy(),\n",
    "#                          config[\"topK\"])\n",
    "\n",
    "#         if mAP > Best_mAP:\n",
    "#             Best_mAP = mAP\n",
    "\n",
    "#             if \"save_path\" in config:\n",
    "#                 if not os.path.exists(config[\"save_path\"]):\n",
    "#                     os.makedirs(config[\"save_path\"])\n",
    "#                 print(\"save in \", config[\"save_path\"])\n",
    "#                 np.save(os.path.join(config[\"save_path\"], config[\"dataset\"] + str(mAP) + \"-\" + \"trn_binary.npy\"),\n",
    "#                         trn_binary.numpy())\n",
    "#                 torch.save(net.state_dict(),\n",
    "#                            os.path.join(config[\"save_path\"], config[\"dataset\"] + \"-\" + str(mAP) + \"-model.pt\"))\n",
    "#         print(\"%s epoch:%d, bit:%d, dataset:%s, MAP:%.3f, Best MAP: %.3f\" % (\n",
    "#             config[\"info\"], epoch + 1, bit, config[\"dataset\"], mAP, Best_mAP))\n",
    "#         print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c207d-c655-4440-a022-7b52fb8f09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if epoch % 50 ==0:\n",
    "# save_checkpoint({\n",
    "#     'epoch': epoch + 1,\n",
    "#     'state_dict': net.state_dict(),\n",
    "#     'best_prec1': 0,\n",
    "#     'optimizer' : optimizer.state_dict(),\n",
    "# }, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5106d6-538f-49b0-8727-639819292ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, img in tqdm.tqdm(enumerate(train_loader)):\n",
    "#     print((img[0].size(), img[1].size()))\n",
    "#     break\n",
    "    \n",
    "# for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "#     print((img[0].size(), img[1].size()))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07f76d-3767-4c30-a673-4ade0f99d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img[0].permute(0, 3, 1, 2).size())\n",
    "# print(img[0].permute(0, 3, 1, 2).double().dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac54206-2374-4eca-89a9-9dedd82149ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b584df4-e536-405f-b2a8-aecca260adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = model(img[0].permute(0, 3, 1, 2).float())\n",
    "# print(outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ae5d-1f0e-4125-9030-2d3e7a03eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best_gcn2wayca.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4c16a-6902-4aec-8243-638a3d87d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a085d-7905-4d95-a92b-dca883df4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=False)  # ,prefetch_factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20120f-d310-4dc6-8b6c-3f00a94f442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break\n",
    "print(img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2253928-e81e-4878-b0a0-09cde7302ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### HELPER FUNCTION FOR FEATURE EXTRACTION\n",
    "# # print(model)\n",
    "\n",
    "# def get_features(name):\n",
    "#     def hook(model, input, output):\n",
    "#         features[name] = output.detach()\n",
    "#     return hook\n",
    "# ##### REGISTER HOOK\n",
    "# model.model_print[16].register_forward_hook(get_features('feats'))\n",
    "# # model.fc1.register_forward_hook(get_features('feats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1841c-fd4b-4c4e-b867-77a297778675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(net,test_loader):\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    FEATS = []\n",
    "    GT = []\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            output = net(rbn)\n",
    "            FEATS.append(output.cpu().numpy())\n",
    "#             FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "    GCNFEATS = np.concatenate(FEATS)\n",
    "    GT = np.concatenate(GT)\n",
    "    print('- feats shape:', GCNFEATS.shape)\n",
    "    print('- GT shape:', GT.shape)\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    def cossim(a,b):\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    pred_scores = []\n",
    "    gt_label = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(3000)):\n",
    "        for j in range(i+1,3000):\n",
    "            # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "            a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "            pred_scores.append(a)\n",
    "            gt_label.append(i//6 == j//6)\n",
    "\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    gt_label = np.array(gt_label)\n",
    "\n",
    "    Gen = pred_scores[gt_label]\n",
    "    Imp = pred_scores[gt_label==False]\n",
    "    Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "#     import seaborn as sns\n",
    "#     sns.distplot(Gen,  kde=False, label='Gen')\n",
    "#     # df =gapminder[gapminder.continent == 'Americas']\n",
    "#     sns.distplot(Imp,  kde=False,label='Imp')\n",
    "#     # Plot formatting\n",
    "#     plt.legend(prop={'size': 12})\n",
    "#     plt.title('Life Expectancy of Two Continents')\n",
    "#     plt.xlabel('Life Exp (years)')\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "    from pyeer.eer_info import get_eer_stats\n",
    "    from pyeer.report import generate_eer_report, export_error_rates\n",
    "    from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "    # Calculating stats for classifier A\n",
    "    stats_a = get_eer_stats(Gen, Imp)\n",
    "    print(stats_a.eer)\n",
    "\n",
    "    return stats_a.eer\n",
    "\n",
    "test(net,test_loader)\n",
    "##### INSPECT FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b1964-473b-4444-b003-4fcf3cd1242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# np.save('FEATS_gcn2wayca.npy', GCNFEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8c159-8bc4-4f96-869a-98be084c706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATSRES18 = np.load('FEATS_res18.npy')\n",
    "# print(FEATSRES18.shape)\n",
    "# FEATSRES18 = np.squeeze(FEATSRES18);\n",
    "# print(FEATSRES18.shape)\n",
    "# # # NewFEATS = np.dstack((FEATS,FEATSRES18))\n",
    "# # FEATS = np.concatenate((FEATS,FEATSRES18), axis=1)\n",
    "# # print(NewFEATS.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc60cae-2740-45df-9277-817c10f4fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "pred_scores = []\n",
    "gt_label = []\n",
    "\n",
    "for i in tqdm.tqdm(range(3000)):\n",
    "    for j in range(i+1,3000):\n",
    "        # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "        a = cossim(GCNFEATS[i,:],GCNFEATS[j,:])\n",
    "#         b = cossim(FEATSRES18[i,:],FEATSRES18[j,:])\n",
    "#         pred_scores.append((1.8*a+0.2*b)/2)\n",
    "        pred_scores.append(a)\n",
    "#         pred_scores.append(b)\n",
    "        gt_label.append(i//6 == j//6)\n",
    "        \n",
    "\n",
    "# for i in tqdm.tqdm(range(600)):\n",
    "#         # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "#         pred_scores.append(cossim(FEATS[i,:],FEATS[]))\n",
    "#         # gt_label.append(i//12 == j//12)\n",
    "pred_scores = np.array(pred_scores)\n",
    "gt_label = np.array(gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbcf98-18e0-4fda-8bfd-53925629bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Gen = pred_scores[gt_label]\n",
    "Imp = pred_scores[gt_label==False]\n",
    "Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.distplot(Gen,  kde=False, label='Gen')\n",
    "# df =gapminder[gapminder.continent == 'Americas']\n",
    "sns.distplot(Imp,  kde=False,label='Imp')\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.title('Life Expectancy of Two Continents')\n",
    "plt.xlabel('Life Exp (years)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from pyeer.report import generate_eer_report, export_error_rates\n",
    "from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "# Calculating stats for classifier A\n",
    "stats_a = get_eer_stats(Gen, Imp)\n",
    "print(stats_a.eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673f170-9bc8-499b-a6c2-29209e84ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)#, positive_label\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "\n",
    "eer = compute_eer(gt_label, pred_scores)\n",
    "print('The equal error rate is {:.3f}'.format(eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3bd27-046c-42d5-a20d-07a9a4343902",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Gen \n",
    "del Imp\n",
    "del pred_scores\n",
    "del gt_label\n",
    "del GCNFEATS\n",
    "del GT\n",
    "del test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
