{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c20581d-f5c4-47a1-8b6e-9289655b0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from utils import accuracy, AverageMeter, save_checkpoint, visualize_graph, get_parameters_size\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from net_factory import get_network_fn\n",
    "\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6983cbb4-bc52-4e7d-9ebf-a700dd4ba4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GCN MNIST Training')\n",
    "\n",
    "parser.add_argument('--epochs', default=50, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--pretrained', default='', type=str, metavar='PATH',\n",
    "                    help='path to pretrained checkpoint (default: none)')\n",
    "parser.add_argument('--gpu', default=0, type=int,\n",
    "                    metavar='N', help='GPU device ID (default: -1)')\n",
    "parser.add_argument('--dataset_dir', default='../../MNIST', type=str, metavar='PATH',\n",
    "                    help='path to dataset (default: ../MNIST)')\n",
    "parser.add_argument('--comment', default='', type=str, metavar='INFO',\n",
    "                    help='Extra description for tensorboard')\n",
    "parser.add_argument('--model', default='gcn', type=str, metavar='NETWORK',\n",
    "                    help='Network to train')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "use_cuda = (args.gpu >= 0) and torch.cuda.is_available()\n",
    "best_prec1 = 0\n",
    "writer = SummaryWriter(comment='_'+args.model+'_'+args.comment)\n",
    "iteration = 0\n",
    "\n",
    "# # from loaddataset import load_data\n",
    "# from loaddataset import load_data\n",
    "\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n",
    "# test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cdaa3e-43a8-4a65-b846-dbc1f66065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model\n",
    "model = get_network_fn(name='Resent18')#GCNCNN\n",
    "# print(model)\n",
    "\n",
    "# Try to visulize the model\n",
    "try:\n",
    "\tvisualize_graph(model, writer, input_size=(1, 5, 128, 128))\n",
    "except:\n",
    "\tprint('\\nNetwork Visualization Failed! But the training procedure continue.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010b059e-a572-4e84-b2c8-5f5e17d8f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model.fc = nn.Linear(num_ftrs, 450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b1d92b-24b6-4edb-b68e-fba7bc38aef7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import DataParallel\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# read image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "ms_polyu_path = 'dataset/MS_PolyU/'\n",
    "casia_path = 'dataset/CASIA-Multi-Spectral-PalmprintV1/images/'\n",
    "\n",
    "r_img_path = ms_polyu_path + 'Red_ind/'\n",
    "b_img_path =  ms_polyu_path + 'Blue_ind/'\n",
    "n_img_path =  ms_polyu_path + 'NIR_ind/'\n",
    "g_img_path =  ms_polyu_path + 'Green_ind/'\n",
    "\n",
    "################ DATASET CLASS\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] \n",
    "# one_hot_embedding(1, 10)\n",
    "def part_init(istrain=True):\n",
    "    r_list = []\n",
    "    b_list = []\n",
    "    vein_list = []\n",
    "    prints_list = []\n",
    "    labels = []\n",
    "    \n",
    "        # split all data into train, test data\n",
    "    train_ratio = 0.9\n",
    "    train_num = int(500 * train_ratio)\n",
    "    print(\"split train users:\",train_num)\n",
    "    if istrain:\n",
    "        for i in tqdm.tqdm(range(train_num)):\n",
    "            for j in range(12):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "                g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "                b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "                n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "                rb = r_normed - b_normed * 0.5\n",
    "                rb =  (rb * 128+128).astype(np.uint8)\n",
    "\n",
    "                imgprint = np.dstack((r_img,g_img,b_img))\n",
    "                imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "                vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                # labels.append(one_hot_embedding(i, train_num))\n",
    "                labels.append(i)\n",
    "    else:\n",
    "        for i in tqdm.tqdm(range(train_num,500)):\n",
    "            for j in range(12):\n",
    "                r_img = np.array(Image.open(os.path.join(r_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (r_img - r_img.min()) / (r_img.max()-r_img.min())\n",
    "                \n",
    "                g_img = np.array(Image.open(os.path.join(g_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                g_normed = (g_img - g_img.min()) / (g_img.max()-g_img.min())\n",
    "\n",
    "                b_img = np.array(Image.open(os.path.join(b_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                b_normed = (b_img - b_img.min()) / (b_img.max()-b_img.min())\n",
    "\n",
    "                n_img = np.array(Image.open(os.path.join(n_img_path, \"%04d_\"%(i+1)+\"%04d.jpg\"%(j+1))))\n",
    "                r_normed = (n_img - n_img.min()) / (n_img.max()-n_img.min())\n",
    "                \n",
    "                rb = r_normed - b_normed * 0.5\n",
    "                rb =  (rb * 128+128).astype(np.uint8)\n",
    "                imgprint = np.dstack((r_img,g_img,b_img))\n",
    "                imgvein = np.dstack((rb, n_img))\n",
    "                \n",
    "                vein_list.append(imgvein)\n",
    "                prints_list.append(imgprint)\n",
    "                # labels.append(one_hot_embedding(i, train_num))\n",
    "                labels.append(i)\n",
    "\n",
    "\n",
    "\n",
    "    # return np.array(r_list), np.array(b_list), np.array(n_list), np.array(labels),np.array(r_list_test), np.array(b_list_test), np.array(n_list_test), np.array(labels_test)\n",
    "    return  vein_list,prints_list, labels\n",
    "\n",
    "# r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "class load_data(Dataset):\n",
    "    \"\"\"Loads the Data.\"\"\"\n",
    "    def __init__(self, training=True):\n",
    "\n",
    "        self.training = training\n",
    "#         r_list, b_list, n_list, labels,r_list_test, b_list_test, n_list_test, labels_test = part_init()\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomPerspective(),\n",
    "        transforms.RandomAffine(30),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        self.transform_test = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.Resize((224, 224)),# if resnet\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                              [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "        if self.training:\n",
    "            print('\\n...... Train files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels= part_init(istrain=True)\n",
    "            print('\\nTrain files loaded ......\\n')\n",
    "        else:\n",
    "            print('\\n...... Test files loading\\n')\n",
    "            self.vein_list,self.prints_list, self.labels = part_init(istrain=False)\n",
    "            print('\\nTest files loaded ......\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vein_list)\n",
    "\n",
    "         \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.training:\n",
    "            prints_img = self.transform(self.prints_list[idx])\n",
    "            vein_img = self.transform(self.vein_list[idx])\n",
    "        else:\n",
    "            prints_img = self.transform_test(self.prints_list[idx])\n",
    "            vein_img = self.transform_test(self.vein_list[idx])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        n_img = np.dstack((prints_img[0,:,:],prints_img[1,:,:],prints_img[2,:,:],vein_img[0,:,:],vein_img[1,:,:]))\n",
    "        \n",
    "        return n_img,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33159f71-22b4-4950-9972-b2cfa3d3277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...... Train files loading\n",
      "\n",
      "split train users: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:32<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train files loaded ......\n",
      "\n",
      "\n",
      "...... Test files loading\n",
      "\n",
      "split train users: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 15.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test files loaded ......\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(load_data(training=True), batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,prefetch_factor=8)  # ,prefetch_factor=2\n",
    "test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=True)  # ,prefetch_factor=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5106d6-538f-49b0-8727-639819292ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:10, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([128, 128, 128, 5]), torch.Size([128]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([128, 128, 128, 5]), torch.Size([128]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, img in tqdm.tqdm(enumerate(train_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break\n",
    "    \n",
    "for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b07f76d-3767-4c30-a673-4ade0f99d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img[0].permute(0, 3, 1, 2).size())\n",
    "# print(img[0].permute(0, 3, 1, 2).double().dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac54206-2374-4eca-89a9-9dedd82149ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b584df4-e536-405f-b2a8-aecca260adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 450])\n"
     ]
    }
   ],
   "source": [
    "outs = model(img[0].permute(0, 3, 1, 2).float())\n",
    "print(outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6479d25d-f9eb-440c-9754-2563f963c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=args.lr, rho=0.9, eps=1e-06, weight_decay=3e-05)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=3e-05)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=args.momentum, weight_decay=3e-05)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab66ae5d-1f0e-4125-9030-2d3e7a03eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total parameters of the model\n",
    "# print('Model size: {:0.2f} million float parameters'.format(get_parameters_size(model)/1e6))\n",
    "# args.pretrained = 'model_best_gcn2wayca.pth.tar'\n",
    "# if os.path.isfile(args.pretrained):\n",
    "#     print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
    "#     checkpoint = torch.load(args.pretrained)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "128280a3-f568-480b-b494-76e70bcc17d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Train Epoch: 1 [0/5400 (0%)]\tLoss: 6.365069, Accuracy: 0.00, lr: 0.01000\n",
      "Epoch time:31.76s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 2 [0/5400 (0%)]\tLoss: 5.692993, Accuracy: 3.91, lr: 0.01000\n",
      "Epoch time:31.53s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 3 [0/5400 (0%)]\tLoss: 4.476200, Accuracy: 8.59, lr: 0.01000\n",
      "Epoch time:31.69s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 4 [0/5400 (0%)]\tLoss: 3.293104, Accuracy: 31.25, lr: 0.01000\n",
      "Epoch time:31.91s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 5 [0/5400 (0%)]\tLoss: 1.979097, Accuracy: 60.16, lr: 0.01000\n",
      "Epoch time:32.01s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 6 [0/5400 (0%)]\tLoss: 1.473891, Accuracy: 65.62, lr: 0.01000\n",
      "Epoch time:32.19s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 7 [0/5400 (0%)]\tLoss: 0.800554, Accuracy: 87.50, lr: 0.01000\n",
      "Epoch time:31.79s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 8 [0/5400 (0%)]\tLoss: 0.602678, Accuracy: 92.19, lr: 0.01000\n",
      "Epoch time:31.94s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 9 [0/5400 (0%)]\tLoss: 0.359643, Accuracy: 96.88, lr: 0.01000\n",
      "Epoch time:31.71s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 10 [0/5400 (0%)]\tLoss: 0.315723, Accuracy: 95.31, lr: 0.01000\n",
      "Epoch time:32.07s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 11 [0/5400 (0%)]\tLoss: 0.196768, Accuracy: 96.88, lr: 0.01000\n",
      "Epoch time:31.91s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 12 [0/5400 (0%)]\tLoss: 0.133275, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.81s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 13 [0/5400 (0%)]\tLoss: 0.128069, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:31.80s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 14 [0/5400 (0%)]\tLoss: 0.090990, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:32.16s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 15 [0/5400 (0%)]\tLoss: 0.178177, Accuracy: 96.88, lr: 0.01000\n",
      "Epoch time:32.10s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 16 [0/5400 (0%)]\tLoss: 0.078736, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.72s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 17 [0/5400 (0%)]\tLoss: 0.102553, Accuracy: 98.44, lr: 0.01000\n",
      "Epoch time:31.75s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 18 [0/5400 (0%)]\tLoss: 0.090083, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:32.34s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 19 [0/5400 (0%)]\tLoss: 0.087094, Accuracy: 98.44, lr: 0.01000\n",
      "Epoch time:31.60s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 20 [0/5400 (0%)]\tLoss: 0.094759, Accuracy: 98.44, lr: 0.01000\n",
      "Epoch time:31.97s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 21 [0/5400 (0%)]\tLoss: 0.100256, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:31.74s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 22 [0/5400 (0%)]\tLoss: 0.045296, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.97s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 23 [0/5400 (0%)]\tLoss: 0.034747, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.93s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 24 [0/5400 (0%)]\tLoss: 0.061735, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:32.40s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 25 [0/5400 (0%)]\tLoss: 0.070020, Accuracy: 98.44, lr: 0.01000\n",
      "Epoch time:31.78s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 26 [0/5400 (0%)]\tLoss: 0.048927, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:32.11s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 27 [0/5400 (0%)]\tLoss: 0.040349, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:31.68s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 28 [0/5400 (0%)]\tLoss: 0.047789, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:32.52s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 29 [0/5400 (0%)]\tLoss: 0.026314, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.89s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 30 [0/5400 (0%)]\tLoss: 0.027224, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.79s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 31 [0/5400 (0%)]\tLoss: 0.025476, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.72s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 32 [0/5400 (0%)]\tLoss: 0.031956, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:32.29s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 33 [0/5400 (0%)]\tLoss: 0.020909, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.70s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 34 [0/5400 (0%)]\tLoss: 0.024222, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.77s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 35 [0/5400 (0%)]\tLoss: 0.021396, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.89s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 36 [0/5400 (0%)]\tLoss: 0.013589, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.92s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 37 [0/5400 (0%)]\tLoss: 0.019549, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.89s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 38 [0/5400 (0%)]\tLoss: 0.012104, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.83s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 39 [0/5400 (0%)]\tLoss: 0.038633, Accuracy: 98.44, lr: 0.01000\n",
      "Epoch time:31.88s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 40 [0/5400 (0%)]\tLoss: 0.045594, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:31.91s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 41 [0/5400 (0%)]\tLoss: 0.015579, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.99s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 42 [0/5400 (0%)]\tLoss: 0.021999, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:32.22s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 43 [0/5400 (0%)]\tLoss: 0.018769, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:32.18s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 44 [0/5400 (0%)]\tLoss: 0.030737, Accuracy: 99.22, lr: 0.01000\n",
      "Epoch time:31.96s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 45 [0/5400 (0%)]\tLoss: 0.014560, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.74s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 46 [0/5400 (0%)]\tLoss: 0.012317, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.80s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 47 [0/5400 (0%)]\tLoss: 0.014379, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.78s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 48 [0/5400 (0%)]\tLoss: 0.020179, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.90s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 49 [0/5400 (0%)]\tLoss: 0.022726, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:32.20s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 50 [0/5400 (0%)]\tLoss: 0.014262, Accuracy: 100.00, lr: 0.01000\n",
      "Epoch time:31.71s\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 51 [0/5400 (0%)]\tLoss: 0.008880, Accuracy: 100.00, lr: 0.01000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17372/699278978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m#     prec1 = test(epoch+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17372/699278978.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    global iteration\n",
    "    st = time.time()\n",
    "    for batch_idx, img in enumerate(train_loader):\n",
    "        rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "        label = img[1].to(device)\n",
    "        model.train()\n",
    "#         print(rbn.size())\n",
    "        #  r_diff n is vein  RGB is print\n",
    "#         vein = torch.cat([r_diff,n], dim=1)\n",
    "#         pprint = torch.cat([r,b], dim=1)\n",
    "#         inputs = torch.cat([r,b,n], dim=1)\n",
    "        inputs = rbn\n",
    "        iteration += 1\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        prec1, = accuracy(output, label)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {:.2f}, lr: {:.5f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), prec1.item(),optimizer.param_groups[0]['lr']))\n",
    "            writer.add_scalar('Loss/Train', loss.item(), iteration)\n",
    "            writer.add_scalar('Accuracy/Train', prec1, iteration)\n",
    "    epoch_time = time.time() - st\n",
    "    print('Epoch time:{:0.2f}s'.format(epoch_time))\n",
    "    scheduler.step()\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    " \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%, lr: {:.2f}%\\n'.format(test_loss.avg, acc.avg))\n",
    "    writer.add_scalar('Loss/Test', test_loss.avg, epoch)\n",
    "    writer.add_scalar('Accuracy/Test', acc.avg, epoch)\n",
    "    return acc.avg\n",
    "\n",
    "for epoch in range(args.start_epoch, 1000):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    train(epoch+1)\n",
    "#     prec1 = test(epoch+1)\n",
    "\n",
    "#     # remember best prec@1 and save checkpoint\n",
    "#     is_best = prec1 > best_prec1\n",
    "#     best_prec1 = max(prec1, best_prec1)\n",
    "    if epoch % 50 ==0:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, True)\n",
    "\n",
    "print('Finished!')\n",
    "print('Best Test Precision@top1:{:.2f}'.format(best_prec1))\n",
    "writer.add_scalar('Best TOP1', best_prec1, 0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54c4c16a-6902-4aec-8243-638a3d87d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c9a085d-7905-4d95-a92b-dca883df4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...... Test files loading\n",
      "\n",
      "split train users: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 17.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test files loaded ......\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(load_data(training=False), batch_size=batch_size, shuffle=False)  # ,prefetch_factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c20120f-d310-4dc6-8b6c-3f00a94f442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([128, 128, 128, 5]), torch.Size([128]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, img in tqdm.tqdm(enumerate(test_loader)):\n",
    "    print((img[0].size(), img[1].size()))\n",
    "    break\n",
    "# print(img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2253928-e81e-4878-b0a0-09cde7302ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fdbec71b850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### HELPER FUNCTION FOR FEATURE EXTRACTION\n",
    "# print(model)\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook\n",
    "##### REGISTER HOOK\n",
    "model.model.avgpool.register_forward_hook(get_features('feats'))\n",
    "# model.model.layer4.register_forward_hook(get_features('feats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79f1841c-fd4b-4c4e-b867-77a297778675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATS = []\n",
    "GT = []\n",
    "features = {}\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, img in enumerate(test_loader):\n",
    "            rbn = img[0].permute(0, 3, 1, 2).to(device, dtype=torch.float)\n",
    "            label = img[1].to(device)\n",
    "\n",
    "            clss = model(rbn)\n",
    "#             FEATS.append(output.cpu().numpy())\n",
    "            FEATS.append(features['feats'].cpu().numpy())\n",
    "            GT.append(img[1].numpy())\n",
    "\n",
    "  \n",
    "    return acc.avg\n",
    "\n",
    "test()\n",
    "##### INSPECT FEATURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "021b1964-473b-4444-b003-4fcf3cd1242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- feats shape: (600, 512, 1, 1)\n",
      "- GT shape: (600,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FEATS = np.concatenate(FEATS)\n",
    "GT = np.concatenate(GT)\n",
    "\n",
    "print('- feats shape:', FEATS.shape)\n",
    "print('- GT shape:', GT.shape)\n",
    "np.save('FEATS_res18.npy', FEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbc60cae-2740-45df-9277-817c10f4fbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 115.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "pred_scores = []\n",
    "gt_label = []\n",
    "\n",
    "for i in tqdm.tqdm(range(600)):\n",
    "    for j in range(i+1,600):\n",
    "        # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "        pred_scores.append(cossim(FEATS[i,:,0,0],FEATS[j,:,0,0]))\n",
    "        gt_label.append(i//12 == j//12)\n",
    "        \n",
    "\n",
    "# for i in tqdm.tqdm(range(600)):\n",
    "#         # pred_scores.append(final[i,j].detach().cpu().numpy())\n",
    "#         pred_scores.append(cossim(FEATS[i,:],FEATS[]))\n",
    "#         # gt_label.append(i//12 == j//12)\n",
    "pred_scores = np.array(pred_scores)\n",
    "gt_label = np.array(gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70fbcf98-18e0-4fda-8bfd-53925629bccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03333333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "Gen = pred_scores[gt_label]\n",
    "Imp = pred_scores[gt_label==False]\n",
    "Imp = Imp[np.random.permutation(len(Imp))[:len(Gen)]]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.distplot(Gen,  kde=False, label='Gen')\n",
    "# df =gapminder[gapminder.continent == 'Americas']\n",
    "sns.distplot(Imp,  kde=False,label='Imp')\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.title('Life Expectancy of Two Continents')\n",
    "plt.xlabel('Life Exp (years)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from pyeer.report import generate_eer_report, export_error_rates\n",
    "from pyeer.plot import plot_eer_stats\n",
    "\n",
    "\n",
    "# Calculating stats for classifier A\n",
    "stats_a = get_eer_stats(Gen, Imp)\n",
    "print(stats_a.eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9673f170-9bc8-499b-a6c2-29209e84ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equal error rate is 0.033\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)#, positive_label\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "\n",
    "eer = compute_eer(gt_label, pred_scores)\n",
    "print('The equal error rate is {:.3f}'.format(eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd3bd27-046c-42d5-a20d-07a9a4343902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
